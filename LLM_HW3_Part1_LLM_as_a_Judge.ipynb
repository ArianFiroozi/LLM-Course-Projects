{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CA 3 - Part1, LLMs Spring 2025**\n",
        "\n",
        "- **Name:**\n",
        "- **Student ID:**\n",
        "\n",
        "---\n",
        "#### Your submission should be named using the following format: `CA3 - Part1_LASTNAME_STUDENTID.ipynb`.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
        "\n",
        "- For text-based answers, you should replace the text that says ```Your Answer Here``` with your actual answer.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---\n",
        "\n",
        "If you have any further questions or concerns, contact the TAs via email:"
      ],
      "metadata": {
        "id": "_ze-S8UKMQx-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X4-pPZJ2DwU"
      },
      "source": [
        "# Import libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1_-R9OrdpSG",
        "outputId": "e6c60bdd-641a-453a-ca33-76066645181b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3invC6Fdv3w",
        "outputId": "1c2976d7-f287-4fd6-907d-f65a8c655f90"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZgivWMATc3ZZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token {\"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\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdEP3Cehdiqo",
        "outputId": "5f37adb5-94f3-43f9-c1b7-7f4f960073e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `CA2` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `CA2`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧩Part 1: Judgement Strategies in LLM as a Judge"
      ],
      "metadata": {
        "id": "5PjORxmLhzWZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLJ2f7-91Lrk"
      },
      "source": [
        "## 1.1 Load Dataset\n",
        "\n",
        "In this assignment, you will explore a dataset commonly used for evaluating feedback and alignment in Large Language Models (LLMs). The goal is to help you become familiar with how such datasets are structured and how to extract meaningful information from them.\n",
        "\n",
        " use the 🤗 datasets library to download the following dataset:\n",
        "\n",
        "> `prometheus-eval/Feedback-Bench`\n",
        "\n",
        "> Link: https://huggingface.co/datasets/prometheus-eval/Feedback-Bench\n",
        "\n",
        "> paper: https://arxiv.org/abs/2310.08491\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cJUUmYSh13Ni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4322bc-03c2-41c4-d7b3-ada1079d4d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "  dataset = load_dataset(\"prometheus-eval/Feedback-Bench\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MtBrFXs90umT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d8caa7-7d8a-4466-d277-2c57cddebad4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['orig_instruction', 'orig_score3_description', 'orig_score4_description', 'output', 'orig_response', 'orig_reference_answer', 'orig_feedback', 'orig_score1_description', 'orig_score', 'orig_criteria', 'orig_score2_description', 'instruction', 'orig_score5_description', 'input', 'messages', '__index_level_0__'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Summary and Statistical Analysis of Dataset (3 points)\n",
        "In this section, your task is to explore and analyze the dataset both quantitatively and qualitatively.\n",
        "\n",
        "* Describe what the column represents.\n",
        "\n",
        "* Identify columns with integer or numerical values.\n",
        "\n",
        "* Plot the distribution of these columns using histograms or other appropriate visualizations."
      ],
      "metadata": {
        "id": "lNlsby2ANShY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VemOt-wk1xQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af097e06-d6b9-4a59-e0b8-c959ce8b0c53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['orig_instruction',\n",
              " 'orig_score3_description',\n",
              " 'orig_score4_description',\n",
              " 'output',\n",
              " 'orig_response',\n",
              " 'orig_reference_answer',\n",
              " 'orig_feedback',\n",
              " 'orig_score1_description',\n",
              " 'orig_score',\n",
              " 'orig_criteria',\n",
              " 'orig_score2_description',\n",
              " 'instruction',\n",
              " 'orig_score5_description',\n",
              " 'input',\n",
              " 'messages',\n",
              " '__index_level_0__']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset['train'].column_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"orig_score\", max(dataset['train'][\"orig_score\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWd6EuPpfF5M",
        "outputId": "7858717e-15af-4f9a-8975-239f6e4708d0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orig_score 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "counts = Counter(dataset['train'][\"orig_score\"])\n",
        "\n",
        "x = sorted(counts.keys())\n",
        "y = [counts[num] for num in x]\n",
        "\n",
        "plt.bar(x, y)\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('orig_score')\n",
        "plt.xticks([1, 2, 3, 4, 5])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "VLx-EjaPhXBC",
        "outputId": "a0f6f99f-b520-4b52-9685-49a3ab6ecee9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ],
      "metadata": {
        "id": "Y8-7rT39Ml7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Load Phi-3-3.8B model\n",
        "\n",
        "Use the Hugging Face transformers library to load the model and tokenizer:\n",
        "\n",
        "Model: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n"
      ],
      "metadata": {
        "id": "h9fDj-EsjCGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "fwQfEBdti81S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=DEVICE,\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "BoyLrVsvjBXw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3828aecb468c49e0820cf4810256b3c9",
            "00d08ff34dbc4d1f9bad92de558e7af8",
            "e0ba1b861cba47a2acfc47dade6eaa65",
            "bd02f427e27543a8aa25d2a4aa5c875c",
            "972c80ef6a504be3986ae85f4614307a",
            "a6d7784b50fd46288d270400204fd6be",
            "a4028c076190455bb4b0e1980184482c",
            "87edd34462d5480c928a7370be338882",
            "1ca7b7bd4aef47d78ac4bf251205e830",
            "fdda40b7dfed433badb7232ace953ba0",
            "47a1731fcfb44184909a9fe385352f7e"
          ]
        },
        "outputId": "c4371008-73db-4b09-de0b-b6fd76b9740e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3828aecb468c49e0820cf4810256b3c9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Phi Judgemnt Performance Evaluation (23 points)\n",
        "\n",
        "In this part of the assignment, you will assess the ability of the Phi-3-mini model to generate evaluative judgments based on structured prompts derived from the dataset. Follow the steps below to carry out the inference process and evaluate the model’s performance:\n",
        "\n",
        "**1. Prompt Construction:**\n",
        "\n",
        "\n",
        "Use relevant columns from the dataset (e.g., orig_instruction,orig_criteria, etc.) to construct informative prompts that the model can respond to meaningfully.\n",
        "\n",
        "\n",
        "**2. Model Inference:**\n",
        "\n",
        "Select a random sample of 50 entries from the dataset. For each entry, feed the constructed prompt into the Phi model and generate a corresponding judgment and score.\n",
        "\n",
        "*Don't forget applying chat template 😊*\n",
        "\n",
        "**3. Output Parsing:**\n",
        "\n",
        "After generating model outputs, create a method to extract the predicted score  from the model’s response.\n",
        "\n",
        "\n",
        "**4. Metric Selection and Performance Analysis:**\n",
        "\n",
        "Compare the predicted scores obtained from the model with the original human-annotated scores available in the `orig_score` column of the dataset. This step will help you measure how well the model’s outputs align with refrence judge."
      ],
      "metadata": {
        "id": "ftFFnhmKq9Uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 Prompt Construction (2 points)"
      ],
      "metadata": {
        "id": "L2FjGnKoVzcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "\n",
        "for i in range(50):\n",
        "  messages.append([\n",
        "      {\"role\": \"system\", \"content\": dataset['train'][\"orig_instruction\"][i]},\n",
        "      {\"role\": \"user\", \"content\": dataset['train'][\"orig_response\"][i]},\n",
        "      {\"role\": \"assistant\", \"content\": dataset['train'][\"orig_criteria\"][i]},\n",
        "      {\"role\": \"user\", \"content\": \"rate the response in range 1 to 5\"},\n",
        "  ])"
      ],
      "metadata": {
        "id": "vSfK0miRWF9s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eiJ8kFgjxuG",
        "outputId": "75b37774-f82a-4c62-d54e-e6112d40e627"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'Imagine a scenario where an individual from the UK is in the United States for a vacation. However, they are struggling to understand the local dialects, accents, and expressions used by the people there. They are also finding it hard to convey their intended message as their phrases and expressions, heavily influenced by their regional factors, are often misunderstood. What steps or strategies can this individual employ to improve their understanding and communication in such a scenario?'},\n",
              " {'role': 'user',\n",
              "  'content': \"The individual might find it difficult to adjust to the American dialects and expressions initially, but some strategies might be of slight help. Watching some local American shows can somewhat help in getting accustomed to the local accents. While conversing, try to stick to English that's more generic, it might make communication a little easier. When you don't comprehend what is being said, you might want to ask for explanations, though it might not always help. Sometimes, you might be able to guess the meaning of unfamiliar phrases from the situation or conversation. Active listening might be of little help, but it's worth trying. Using translation apps could be an option, but they might not always translate local expressions accurately.\"},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Is the model proficient in interpreting and responding to various local dialects, accents, and local expressions? Does it have the ability to comprehend and understand the same expression or phrase used in diverse situations influenced by regional factors?'},\n",
              " {'role': 'user', 'content': 'rate the response in range 1 to 5'}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 Model Inference (5 points)"
      ],
      "metadata": {
        "id": "VarMRNQZWG5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 10,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "outputs = []\n",
        "for m in messages:\n",
        "  outputs.append(pipe(m, **generation_args))\n",
        "  print(outputs[-1][0]['generated_text'])"
      ],
      "metadata": {
        "id": "uysvNUNrqbd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79088237-bd69-4764-c670-d9203c0fdb3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I would rate the response as a 2.\n",
            " I would rate the response as a 4.\n",
            " The model's proficiency in multit\n",
            " I would rate the response a 1. The\n",
            " I would rate the response as a 4.\n",
            " 5\n",
            " I would rate the response a 5. The\n",
            " I would rate the response a 4. The\n",
            " I would rate the response a 3. While\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I would rate the response a 1. The\n",
            " I would rate the response a 3. The\n",
            " I would rate the response a 5. The\n",
            " I would rate the response a 5. The\n",
            " The response provided by the speaker in the scenario was\n",
            " I would rate the response a 3. The\n",
            " I would rate the response as a 5.\n",
            " I would rate the response a 3. The\n",
            " I would rate the response a 4. The\n",
            " I would rate the response a 5. The\n",
            " I'm sorry, but I can't\n",
            " I would rate the response as a 5.\n",
            " I would rate the response as a 4.\n",
            " I would rate the response as a 5.\n",
            " I would rate the response as a 3.\n",
            " I would rate the response a 4. The\n",
            " I'm sorry, but it seems there might\n",
            " I would rate the response a 5. The\n",
            " I'm sorry, but I cannot provide a\n",
            " I would rate the response as a 5.\n",
            " I'm sorry, but I cannot rate the\n",
            " I'm sorry, but as an AI\n",
            " I would rate the response a 2. The\n",
            " The response provided demonstrates a clear understanding of the\n",
            " I would rate the response as a 4.\n",
            " I'm sorry, but as an AI\n",
            " I would rate the response as a 4.\n",
            " I would rate the response a 5. The\n",
            " I would rate the response as a 1.\n",
            " I would rate the response a 4. The\n",
            " I'd rate the response a 4.\n",
            " I would rate the response a 5. The\n",
            " I would rate the response a 5. The\n",
            " I would rate the AI model's adapt\n",
            " I would rate the response as a 4.\n",
            " I would rate the response a 5. The\n",
            " I would rate the response a 2. While\n",
            " The model's response is rated as a\n",
            " I'm sorry, but it seems there has\n",
            " I would rate the response as a 4.\n",
            " I'm sorry, but I cannot rate the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][\"orig_score\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "67PRsGrOpePP",
        "outputId": "09ada004-0971-45dc-becb-6205cbcad90f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.3 Extract Score (Output Parsing) (5 points)"
      ],
      "metadata": {
        "id": "PKPkfJHqyzQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_scores=[]\n",
        "for o in outputs:\n",
        "  numbers = re.findall(r'\\d+', o[0]['generated_text'])\n",
        "  if len(numbers):\n",
        "    model_scores.append(numbers[-1])\n",
        "  else:\n",
        "    model_scores.append(None)"
      ],
      "metadata": {
        "id": "3z3INKCWyydu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, score in enumerate(model_scores):\n",
        "  print(score, dataset['train'][\"orig_score\"][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49D8W0zNr79F",
        "outputId": "83b51aad-5839-4771-8b48-60e94bc76b53"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 2\n",
            "4 1\n",
            "None 5\n",
            "1 1\n",
            "4 1\n",
            "5 1\n",
            "5 4\n",
            "4 5\n",
            "3 1\n",
            "1 1\n",
            "3 3\n",
            "5 5\n",
            "5 5\n",
            "None 1\n",
            "3 3\n",
            "5 4\n",
            "3 2\n",
            "4 3\n",
            "5 4\n",
            "None 1\n",
            "5 5\n",
            "4 2\n",
            "5 5\n",
            "3 3\n",
            "4 2\n",
            "None 2\n",
            "5 2\n",
            "None 2\n",
            "5 5\n",
            "None 1\n",
            "None 2\n",
            "2 2\n",
            "None 5\n",
            "4 4\n",
            "None 5\n",
            "4 4\n",
            "5 1\n",
            "1 1\n",
            "4 1\n",
            "4 3\n",
            "5 3\n",
            "5 4\n",
            "None 2\n",
            "4 2\n",
            "5 4\n",
            "2 1\n",
            "None 4\n",
            "None 1\n",
            "4 3\n",
            "None 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.4 Metric Selection and Performance Analysis (11 points)\n",
        "\n",
        "Respond to the following questions to deepen your understanding of evaluation strategies in LLM-based scoring tasks:\n",
        "\n",
        "\n",
        "What is the most appropriate evaluation metric for comparing the model’s predicted scores with the reference value (`orig_score`)? Consider the type of scores (e.g., continuous, ordinal, or categorical) when making your choice. (3 points)\n",
        "\n",
        "Calculate the chosen evaluation metric (any suitable metric) to quantify the relationship between the model's predicted score and `orig_score` (6 points).\n",
        "\n",
        "Is accuracy a suitable metric in this context? Why or why not? (2 points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0GIbqfSO4r4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ],
      "metadata": {
        "id": "M92jh31lNd41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "yKgnROsi56-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ],
      "metadata": {
        "id": "e55bgsxHNed8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Alternative Evaluation Strategies (15 points)\n",
        "\n",
        "In addition to the default scoring approach, you are encouraged to explore alternative judgment strategies to evaluate the model’s performance on the judgment task.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Examples of Alternative Approaches\n",
        "\n",
        "#### Quantetive Prompt Design\n",
        "- Reformulate the prompts to request a **score on a different scale**, such as from **1 to 100** instead of 1 to 5.\n",
        "- After model inference, **normalize** or **map** the predicted score back to the **1–5 range** for comparison (e.g., using simple scaling or binning).\n",
        "\n",
        "#### Qualitative Scoring (Likert-style)\n",
        "- Design prompts to elicit **descriptive judgments**, such as:  \n",
        "  `\"Poor\"`, `\"Fair\"`, `\"Good\"`, `\"Very Good\"`, `\"Excellent\"`\n",
        "- Then **map these qualitative outputs** to **numerical values** (e.g., 1 to 5) to enable metric-based evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "aFnLvKIR8mmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "KI7bUlm-UO0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "P7qDS0cSqRgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "5OQgN6N0qRUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "v0vc3MJFqUMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 🧩 Part 2: Creating Preference Data Using LLM as Judge\n",
        "\n",
        "In this part, you will explore how to use large language models (LLMs) to generate **preference data** for optimization tasks.\n",
        "\n",
        "We will compare two models:\n",
        "\n",
        "- `Qwen/Qwen1.5-1.8B-Chat`\n",
        "- `stabilityai/stablelm-2-zephyr-1_6b`\n",
        "\n",
        "The goal is to evaluate how well these models can **distinguish preferred answers (\"chosen\") from less favorable ones (\"rejected\")** in a human-like manner.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eE91AL2PqZfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Download the Models and Dataset\n",
        "\n",
        "- Load the following two models from Hugging Face:\n",
        "  - `Qwen/Qwen1.5-1.8B-Chat`\n",
        "  - `stabilityai/stablelm-2-zephyr-1_6b`\n",
        "\n",
        "- Download the dataset:  \n",
        "  [`HumanLLMs/Human-Like-DPO-Dataset`](https://huggingface.co/datasets/HumanLLMs/Human-Like-DPO-Dataset)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-Trlh0ydZjhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "InUGnZdfJL8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "1S_traP8I-Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "_4S11PbRZ0Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Dataset Exploration (1 point)\n",
        "\n",
        "\n",
        "- Analyze the `HumanLLMs/Human-Like-DPO-Dataset`.\n",
        "  - Describe the dataset structure and columns.\n",
        "\n",
        "- **Optional**: Read the paper for additional context and insights:  \n",
        "   [Human-Like DPO (arXiv:2501.05032)](https://arxiv.org/pdf/2501.05032)\n",
        "\n"
      ],
      "metadata": {
        "id": "2kohpa-vOHUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ],
      "metadata": {
        "id": "l0EkXgwiNz3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Judging Setup (3 points)\n",
        "\n",
        "- Create a **prompting framework** that presents both the **chosen** and **rejected** answers to the model and asks it to **select the better one**.\n",
        "\n",
        "\n",
        "Example prompt structure:\n",
        "> \"Here is a prompt and two responses. Please choose the better response based on helpfulness, relevance, and coherence.  \n",
        ">  \n",
        "> Prompt: {prompt}  \n",
        ">  \n",
        "> Response 1: {chosen or rejected}  \n",
        "> Response 2: {rejected or chosen}  \n",
        ">  \n",
        "> Which response is better? Reply with 'Answer 1' or 'Answer 2'.\"\n",
        "---"
      ],
      "metadata": {
        "id": "N4e0fakr9Orq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Write Your Code Here"
      ],
      "metadata": {
        "id": "fykTRxZzcFeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Model Comparison (10 points)\n",
        "\n",
        "- Run inference using both models on a **sample of the dataset** (e.g., 200–500 instances from dataset). (2 points)\n",
        "- Compare each model's judgments to the **ground truth** (i.e., whether it preferred the \"chosen\" response). (4 points)\n",
        "- Compute the **accuracy** and plot **confusion matrix** for each model to evaluate performance. (4 points)\n",
        "- Make sure to properly handle cases where the model's output is unclear or the preference cannot be extracted (e.g., skip or categorize as \"unkowned\")."
      ],
      "metadata": {
        "id": "N0lpkq0NPbaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "AyK2-BsiJtnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "mnZM43PMqwD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "qrisL91j9OEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "aKvtiLs3nrrl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "2X4-pPZJ2DwU"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3828aecb468c49e0820cf4810256b3c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00d08ff34dbc4d1f9bad92de558e7af8",
              "IPY_MODEL_e0ba1b861cba47a2acfc47dade6eaa65",
              "IPY_MODEL_bd02f427e27543a8aa25d2a4aa5c875c"
            ],
            "layout": "IPY_MODEL_972c80ef6a504be3986ae85f4614307a"
          }
        },
        "00d08ff34dbc4d1f9bad92de558e7af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6d7784b50fd46288d270400204fd6be",
            "placeholder": "​",
            "style": "IPY_MODEL_a4028c076190455bb4b0e1980184482c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e0ba1b861cba47a2acfc47dade6eaa65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87edd34462d5480c928a7370be338882",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ca7b7bd4aef47d78ac4bf251205e830",
            "value": 2
          }
        },
        "bd02f427e27543a8aa25d2a4aa5c875c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdda40b7dfed433badb7232ace953ba0",
            "placeholder": "​",
            "style": "IPY_MODEL_47a1731fcfb44184909a9fe385352f7e",
            "value": " 2/2 [00:23&lt;00:00, 10.76s/it]"
          }
        },
        "972c80ef6a504be3986ae85f4614307a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6d7784b50fd46288d270400204fd6be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4028c076190455bb4b0e1980184482c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87edd34462d5480c928a7370be338882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ca7b7bd4aef47d78ac4bf251205e830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdda40b7dfed433badb7232ace953ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47a1731fcfb44184909a9fe385352f7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}