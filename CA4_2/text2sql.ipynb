{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ca5ad4",
   "metadata": {},
   "source": [
    "## CA 4 - Part 2, LLMs Spring 2025\n",
    "\n",
    "- **Name:**\n",
    "- **Student ID:**\n",
    "\n",
    "---\n",
    "#### Your submission should be named using the following format: `CA4_LASTNAME_STUDENTID.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "TA Email: miladmohammadi@ut.ac.ir\n",
    "\n",
    "##### *How to do this problem set:*\n",
    "\n",
    "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
    "\n",
    "- For text-based answers, you should replace the text that says ```Your Answer Here``` with your actual answer.\n",
    "\n",
    "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
    "\n",
    "---\n",
    "\n",
    "##### *Academic honesty*\n",
    "\n",
    "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
    "\n",
    "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86cccf5",
   "metadata": {},
   "source": [
    "## Text2SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c97a10",
   "metadata": {},
   "source": [
    "In this section, you will progressively build and evaluate multiple Text-to-SQL pipelines. You’ll start with a simple prompting-based baseline, then design a graph-based routing system using chain-of-thought and schema reasoning, and finally construct a ReAct agent that interacts with the schema via tools. Each stage demonstrates a different strategy for generating SQL from natural language using LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86892463",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367a33b",
   "metadata": {},
   "source": [
    "This section prepares the environment and initializes the LLM model (Gemini) to be used in later parts of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "079d57ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: langchain in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 2)) (0.3.25)\n",
      "Collecting langgraph (from -r requirements.txt (line 3))\n",
      "  Using cached langgraph-0.4.8-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 4)) (0.3.64)\n",
      "Collecting langchain-google-genai (from -r requirements.txt (line 5))\n",
      "  Using cached langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 6)) (4.67.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 7)) (2.2.3)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (2.10.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core->-r requirements.txt (line 4)) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core->-r requirements.txt (line 4)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core->-r requirements.txt (line 4)) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (0.28.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 2)) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 2)) (3.2.3)\n",
      "Collecting langgraph-checkpoint>=2.0.26 (from langgraph->-r requirements.txt (line 3))\n",
      "  Using cached langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-prebuilt>=0.2.0 (from langgraph->-r requirements.txt (line 3))\n",
      "  Using cached langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langgraph-sdk>=0.1.42 (from langgraph->-r requirements.txt (line 3))\n",
      "  Using cached langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph->-r requirements.txt (line 3))\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached google_api_core-2.25.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5)) (5.29.0)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5)) (1.67.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached protobuf-6.31.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached grpcio-1.72.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->-r requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->-r requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->-r requirements.txt (line 7)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->-r requirements.txt (line 7)) (2024.2)\n",
      "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint>=2.0.26->langgraph->-r requirements.txt (line 3))\n",
      "  Using cached ormsgpack-1.10.0-cp311-cp311-win_amd64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (1.3.1)\n",
      "Using cached langgraph-0.4.8-py3-none-any.whl (152 kB)\n",
      "Using cached langchain_google_genai-2.1.5-py3-none-any.whl (44 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
      "Using cached google_api_core-2.25.0-py3-none-any.whl (160 kB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached grpcio_status-1.72.1-py3-none-any.whl (14 kB)\n",
      "Using cached grpcio-1.72.1-cp311-cp311-win_amd64.whl (4.2 MB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached protobuf-6.31.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n",
      "Using cached ormsgpack-1.10.0-cp311-cp311-win_amd64.whl (121 kB)\n",
      "Using cached langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
      "Using cached langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Installing collected packages: filetype, xxhash, pyasn1, protobuf, ormsgpack, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, langgraph-sdk, grpcio-status, google-auth, google-api-core, langgraph-checkpoint, langgraph-prebuilt, google-ai-generativelanguage, langgraph, langchain-google-genai\n",
      "\n",
      "   ----------------------------------------  0/20 [filetype]\n",
      "   ----------------------------------------  0/20 [filetype]\n",
      "   ----------------------------------------  0/20 [filetype]\n",
      "   ----------------------------------------  0/20 [filetype]\n",
      "   ----------------------------------------  0/20 [filetype]\n",
      "   ----------------------------------------  0/20 [filetype]\n",
      "   ----------------------------------------  0/20 [filetype]\n",
      "   ----------------------------------------  0/20 [filetype]\n",
      "   -- -------------------------------------  1/20 [xxhash]\n",
      "   ---- -----------------------------------  2/20 [pyasn1]\n",
      "   ---- -----------------------------------  2/20 [pyasn1]\n",
      "  Attempting uninstall: protobuf\n",
      "   ---- -----------------------------------  2/20 [pyasn1]\n",
      "    Found existing installation: protobuf 5.29.0\n",
      "   ---- -----------------------------------  2/20 [pyasn1]\n",
      "    Uninstalling protobuf-5.29.0:\n",
      "   ---- -----------------------------------  2/20 [pyasn1]\n",
      "      Successfully uninstalled protobuf-5.29.0\n",
      "   ---- -----------------------------------  2/20 [pyasn1]\n",
      "   ------ ---------------------------------  3/20 [protobuf]\n",
      "   ------ ---------------------------------  3/20 [protobuf]\n",
      "   ------ ---------------------------------  3/20 [protobuf]\n",
      "   ------ ---------------------------------  3/20 [protobuf]\n",
      "   ------ ---------------------------------  3/20 [protobuf]\n",
      "   ------ ---------------------------------  3/20 [protobuf]\n",
      "   ------ ---------------------------------  3/20 [protobuf]\n",
      "  Attempting uninstall: grpcio\n",
      "   ------ ---------------------------------  3/20 [protobuf]\n",
      "    Found existing installation: grpcio 1.67.0\n",
      "   ------ ---------------------------------  3/20 [protobuf]\n",
      "    Uninstalling grpcio-1.67.0:\n",
      "   ------ ---------------------------------  3/20 [protobuf]\n",
      "      Successfully uninstalled grpcio-1.67.0\n",
      "   ------ ---------------------------------  3/20 [protobuf]\n",
      "   ---------- -----------------------------  5/20 [grpcio]\n",
      "   ---------- -----------------------------  5/20 [grpcio]\n",
      "   ---------- -----------------------------  5/20 [grpcio]\n",
      "   ---------- -----------------------------  5/20 [grpcio]\n",
      "   ---------- -----------------------------  5/20 [grpcio]\n",
      "   -------------- -------------------------  7/20 [rsa]\n",
      "   -------------- -------------------------  7/20 [rsa]\n",
      "   -------------- -------------------------  7/20 [rsa]\n",
      "   -------------- -------------------------  7/20 [rsa]\n",
      "   ---------------- -----------------------  8/20 [pyasn1-modules]\n",
      "   ---------------- -----------------------  8/20 [pyasn1-modules]\n",
      "   ---------------- -----------------------  8/20 [pyasn1-modules]\n",
      "   ---------------- -----------------------  8/20 [pyasn1-modules]\n",
      "   ---------------- -----------------------  8/20 [pyasn1-modules]\n",
      "   ---------------- -----------------------  8/20 [pyasn1-modules]\n",
      "   ---------------- -----------------------  8/20 [pyasn1-modules]\n",
      "   ---------------- -----------------------  8/20 [pyasn1-modules]\n",
      "   ---------------- -----------------------  8/20 [pyasn1-modules]\n",
      "   ---------------- -----------------------  8/20 [pyasn1-modules]\n",
      "   ---------------- -----------------------  8/20 [pyasn1-modules]\n",
      "   ------------------ ---------------------  9/20 [proto-plus]\n",
      "   ------------------ ---------------------  9/20 [proto-plus]\n",
      "   -------------------- ------------------- 10/20 [googleapis-common-protos]\n",
      "   -------------------- ------------------- 10/20 [googleapis-common-protos]\n",
      "   -------------------- ------------------- 10/20 [googleapis-common-protos]\n",
      "   -------------------- ------------------- 10/20 [googleapis-common-protos]\n",
      "   -------------------- ------------------- 10/20 [googleapis-common-protos]\n",
      "   -------------------- ------------------- 10/20 [googleapis-common-protos]\n",
      "   -------------------- ------------------- 10/20 [googleapis-common-protos]\n",
      "   -------------------- ------------------- 10/20 [googleapis-common-protos]\n",
      "   ---------------------- ----------------- 11/20 [langgraph-sdk]\n",
      "   -------------------------- ------------- 13/20 [google-auth]\n",
      "   -------------------------- ------------- 13/20 [google-auth]\n",
      "   -------------------------- ------------- 13/20 [google-auth]\n",
      "   -------------------------- ------------- 13/20 [google-auth]\n",
      "   -------------------------- ------------- 13/20 [google-auth]\n",
      "   -------------------------- ------------- 13/20 [google-auth]\n",
      "   -------------------------- ------------- 13/20 [google-auth]\n",
      "   ---------------------------- ----------- 14/20 [google-api-core]\n",
      "   ---------------------------- ----------- 14/20 [google-api-core]\n",
      "   ---------------------------- ----------- 14/20 [google-api-core]\n",
      "   ---------------------------- ----------- 14/20 [google-api-core]\n",
      "   ---------------------------- ----------- 14/20 [google-api-core]\n",
      "   ------------------------------ --------- 15/20 [langgraph-checkpoint]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 17/20 [google-ai-generativelanguage]\n",
      "   ------------------------------------ --- 18/20 [langgraph]\n",
      "   ------------------------------------ --- 18/20 [langgraph]\n",
      "   ------------------------------------ --- 18/20 [langgraph]\n",
      "   ------------------------------------ --- 18/20 [langgraph]\n",
      "   ------------------------------------ --- 18/20 [langgraph]\n",
      "   -------------------------------------- - 19/20 [langchain-google-genai]\n",
      "   ---------------------------------------- 20/20 [langchain-google-genai]\n",
      "\n",
      "Successfully installed cachetools-5.5.2 filetype-1.2.0 google-ai-generativelanguage-0.6.18 google-api-core-2.25.0 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.72.1 grpcio-status-1.72.1 langchain-google-genai-2.1.5 langgraph-0.4.8 langgraph-checkpoint-2.0.26 langgraph-prebuilt-0.2.2 langgraph-sdk-0.1.70 ormsgpack-1.10.0 proto-plus-1.26.1 protobuf-6.31.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rotobuf (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rotobuf (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rotobuf (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install --use-pep517 --no-build-isolation -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f4a2c",
   "metadata": {},
   "source": [
    "#### Load API Key (2 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e23a1",
   "metadata": {},
   "source": [
    "**Task:** Load the Gemini API key stored in the `.env` file and set it as an environment variable so it can be used to authenticate API requests later.\n",
    "\n",
    "* Use `dotenv` to load the file.\n",
    "* Extract the API key with `os.getenv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd477695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce1714",
   "metadata": {},
   "source": [
    "#### Create ChatModel (3 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50a6f5",
   "metadata": {},
   "source": [
    "**Task:** Create an instance of the Gemini LLM using LangChain. You should configure the model with proper parameters for our task.\n",
    "\n",
    "Note: You may use any model that supports Structured Output and Tool Use. We recommend using gemini-2.5-flash-preview-05-20 from Google AI Studio, as it offers a generous free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c117040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-preview-05-20\",\n",
    "    temperature=0,\n",
    "    max_tokens=5000,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81412ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there!\\n\\nJust wanted to send a little note of appreciation your way. Thank you so much for taking the time and effort to grade my homework! I truly appreciate all the hard work you put into it.\\n\\nHope you have a wonderful day! 😊', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'models/gemini-2.5-flash-preview-05-20', 'safety_ratings': []}, id='run--da950528-2ad4-4241-8c77-3ef6b4472be6-0', usage_metadata={'input_tokens': 29, 'output_tokens': 52, 'total_tokens': 265, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 184}})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that spreads positivity and encouragement. \",\n",
    "    ),\n",
    "    (\"human\", \"Say something to the person who is grading my homework. (say something nice)\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8440112",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c1c0b",
   "metadata": {},
   "source": [
    "In this section, you'll build a simple baseline pipeline that directly converts a question and schema into a SQL query using a single prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef3ecf",
   "metadata": {},
   "source": [
    "#### Baseline Function (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452b396",
   "metadata": {},
   "source": [
    "**Task:** Implement a function that sends a system message defining the task, and a user message containing the input question and schema. The LLM should return the SQL query formatted as: \"```sql\\n[query]```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0fd0eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(question: str, schema: str):\n",
    "    #YOUR CODE HERE\n",
    "    messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a SQL query generator that generates SQL queries based on the provided question and schema. Format your response like this example: sql\\n[query]\",\n",
    "        ),\n",
    "        (\"human\", \"In this schema: \" + schema + \", write a SQL query to answer the question: \" + question),\n",
    "    ]\n",
    "    sql_query = llm.invoke(messages).content.strip()\n",
    "    \n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f0caf5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sql\\nSELECT T1.element, T2.label FROM atom AS T1 JOIN molecule AS T2 ON T1.molecule_id = T2.molecule_id WHERE T1.molecule_id = 'TR060'\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_baseline(\n",
    "    question=\"What are the elements of the toxicology and label of molecule TR060? (Evidence: TR060 is the molecule id; label = '+' mean molecules are carcinogenic; label = '-' means molecules are non-carcinogenic; element = 'cl' means Chlorine; element = 'c' means Carbon; element = 'h' means Hydrogen; element = 'o' means Oxygen, element = 's' means Sulfur; element = 'n' means Nitrogen, element = 'p' means Phosphorus, element = 'na' means Sodium, element = 'br' means Bromine, element = 'f' means Fluorine; element = 'i' means Iodine; element = 'sn' means Tin; element = 'pb' means Lead; element = 'te' means Tellurium; element = 'ca' means Calcium)\",\n",
    "    schema=\"atom (atom_id, molecule_id, element)\\nbond (bond_id, molecule_id, bond_type)\\nconnected (atom_id, atom_id2, bond_id)\\nmolecule (molecule_id, label)\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335cbd8",
   "metadata": {},
   "source": [
    "#### Run and Evaluate (Estimated Run Time 5-10min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f328a0c2",
   "metadata": {},
   "source": [
    "Run your baseline function over the dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "deb29e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wrapt_timeout_decorator\n",
      "  Downloading wrapt_timeout_decorator-1.5.1-py3-none-any.whl.metadata (50 kB)\n",
      "Collecting cli-exit-tools (from wrapt_timeout_decorator)\n",
      "  Downloading cli_exit_tools-1.2.7-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lib-detect-testenv (from wrapt_timeout_decorator)\n",
      "  Downloading lib_detect_testenv-2.0.8-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from wrapt_timeout_decorator)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wrapt_timeout_decorator) (6.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wrapt_timeout_decorator) (1.16.0)\n",
      "Collecting dill!=0.3.5,!=0.3.5.1,>0.3.0 (from wrapt_timeout_decorator)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting click (from cli-exit-tools->wrapt_timeout_decorator)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\arian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->cli-exit-tools->wrapt_timeout_decorator) (0.4.6)\n",
      "Downloading wrapt_timeout_decorator-1.5.1-py3-none-any.whl (30 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading cli_exit_tools-1.2.7-py3-none-any.whl (11 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading lib_detect_testenv-2.0.8-py3-none-any.whl (8.4 kB)\n",
      "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Installing collected packages: lib-detect-testenv, dill, click, multiprocess, cli-exit-tools, wrapt_timeout_decorator\n",
      "\n",
      "   ------ --------------------------------- 1/6 [dill]\n",
      "   ------ --------------------------------- 1/6 [dill]\n",
      "   ------ --------------------------------- 1/6 [dill]\n",
      "   ------ --------------------------------- 1/6 [dill]\n",
      "   ------ --------------------------------- 1/6 [dill]\n",
      "   ------ --------------------------------- 1/6 [dill]\n",
      "   ------------- -------------------------- 2/6 [click]\n",
      "   ------------- -------------------------- 2/6 [click]\n",
      "   -------------------- ------------------- 3/6 [multiprocess]\n",
      "   -------------------- ------------------- 3/6 [multiprocess]\n",
      "   -------------------- ------------------- 3/6 [multiprocess]\n",
      "   -------------------- ------------------- 3/6 [multiprocess]\n",
      "   -------------------- ------------------- 3/6 [multiprocess]\n",
      "   -------------------------- ------------- 4/6 [cli-exit-tools]\n",
      "   -------------------------- ------------- 4/6 [cli-exit-tools]\n",
      "   -------------------------- ------------- 4/6 [cli-exit-tools]\n",
      "   -------------------------- ------------- 4/6 [cli-exit-tools]\n",
      "   -------------------------- ------------- 4/6 [cli-exit-tools]\n",
      "   --------------------------------- ------ 5/6 [wrapt_timeout_decorator]\n",
      "   ---------------------------------------- 6/6 [wrapt_timeout_decorator]\n",
      "\n",
      "Successfully installed cli-exit-tools-1.2.7 click-8.2.1 dill-0.4.0 lib-detect-testenv-2.0.8 multiprocess-0.70.18 wrapt_timeout_decorator-1.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rotobuf (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rotobuf (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rotobuf (C:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install wrapt_timeout_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "538878ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Find the percentage of atoms with single bond. (Evidence: single bond refers to bond_type = '-'; percentage = DIVIDE(SUM(bond_type = '-'), COUNT(bond_id)) as percentage)\n",
      "Schema: atom (atom_id, molecule_id, element)\n",
      "bond (bond_id, molecule_id, bond_type)\n",
      "connected (atom_id, atom_id2, bond_id)\n",
      "molecule (molecule_id, label)\n",
      "\n",
      "Generated SQL: SELECT CAST(SUM(CASE WHEN bond_type = '-' THEN 1 ELSE 0 END) AS REAL) * 100.0 / COUNT(bond_id) FROM bond\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                              | 1/18 [00:18<05:17, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Indicate which atoms are connected in non-carcinogenic type molecules. (Evidence: label = '-' means molecules are non-carcinogenic)\n",
      "Schema: atom (atom_id, molecule_id, element)\n",
      "bond (bond_id, molecule_id, bond_type)\n",
      "connected (atom_id, atom_id2, bond_id)\n",
      "molecule (molecule_id, label)\n",
      "\n",
      "Generated SQL: sql\n",
      "SELECT\n",
      "  T1.atom_id,\n",
      "  T1.atom_id2\n",
      "FROM connected AS T1\n",
      "INNER JOIN bond AS T2\n",
      "  ON T1.bond_id = T2.bond_id\n",
      "INNER JOIN molecule AS T3\n",
      "  ON T2.molecule_id = T3.molecule_id\n",
      "WHERE\n",
      "  T3.label = '-';\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████████▏                                                                         | 2/18 [00:30<03:53, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the average number of bonds the atoms with the element iodine have? (Evidence: atoms with the element iodine refers to element = 'i'; average = DIVIDE(COUND(bond_id), COUNT(atom_id)) where element = 'i')\n",
      "Schema: atom (atom_id, molecule_id, element)\n",
      "bond (bond_id, molecule_id, bond_type)\n",
      "connected (atom_id, atom_id2, bond_id)\n",
      "molecule (molecule_id, label)\n",
      "\n",
      "Generated SQL: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▊                                                                     | 3/18 [01:04<05:55, 23.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List down two molecule id of triple bond non carcinogenic molecules with element carbon. (Evidence: carbon refers to element = 'c'; triple bond refers to bond_type = '#'; label = '-' means molecules are non-carcinogenic)\n",
      "Schema: atom (atom_id, molecule_id, element)\n",
      "bond (bond_id, molecule_id, bond_type)\n",
      "connected (atom_id, atom_id2, bond_id)\n",
      "molecule (molecule_id, label)\n",
      "\n",
      "Generated SQL: sql\n",
      "SELECT T1.molecule_id\n",
      "FROM molecule AS T1\n",
      "INNER JOIN bond AS T2\n",
      "  ON T1.molecule_id = T2.molecule_id\n",
      "INNER JOIN atom AS T3\n",
      "  ON T1.molecule_id = T3.molecule_id\n",
      "WHERE\n",
      "  T2.bond_type = '#' AND T3.element = 'c' AND T1.label = '-'\n",
      "LIMIT 2;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████▍                                                                | 4/18 [01:17<04:28, 19.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the elements of the toxicology and label of molecule TR060? (Evidence: TR060 is the molecule id; label = '+' mean molecules are carcinogenic; label = '-' means molecules are non-carcinogenic; element = 'cl' means Chlorine; element = 'c' means Carbon; element = 'h' means Hydrogen; element = 'o' means Oxygen, element = 's' means Sulfur; element = 'n' means Nitrogen, element = 'p' means Phosphorus, element = 'na' means Sodium, element = 'br' means Bromine, element = 'f' means Fluorine; element = 'i' means Iodine; element = 'sn' means Tin; element = 'pb' means Lead; element = 'te' means Tellurium; element = 'ca' means Calcium)\n",
      "Schema: atom (atom_id, molecule_id, element)\n",
      "bond (bond_id, molecule_id, bond_type)\n",
      "connected (atom_id, atom_id2, bond_id)\n",
      "molecule (molecule_id, label)\n",
      "\n",
      "Generated SQL: sql\n",
      "SELECT T1.element, T2.label FROM atom AS T1 JOIN molecule AS T2 ON T1.molecule_id = T2.molecule_id WHERE T1.molecule_id = 'TR060'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████                                                            | 5/18 [01:28<03:31, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the elements for bond id TR001_10_11? (Evidence: element = 'cl' means Chlorine; element = 'c' means Carbon; element = 'h' means Hydrogen; element = 'o' means Oxygen, element = 's' means Sulfur; element = 'n' means Nitrogen, element = 'p' means Phosphorus, element = 'na' means Sodium, element = 'br' means Bromine, element = 'f' means Fluorine; element = 'i' means Iodine; element = 'sn' means Tin; element = 'pb' means Lead; element = 'te' means Tellurium; element = 'ca' means Calcium)\n",
      "Schema: atom (atom_id, molecule_id, element)\n",
      "bond (bond_id, molecule_id, bond_type)\n",
      "connected (atom_id, atom_id2, bond_id)\n",
      "molecule (molecule_id, label)\n",
      "\n",
      "Generated SQL: sql\n",
      "SELECT T3.element FROM bond AS T1 JOIN connected AS T2 ON T1.bond_id = T2.bond_id JOIN atom AS T3 ON T2.atom_id = T3.atom_id WHERE T1.bond_id = 'TR001_10_11' UNION SELECT T3.element FROM bond AS T1 JOIN connected AS T2 ON T1.bond_id = T2.bond_id JOIN atom AS T3 ON T2.atom_id2 = T3.atom_id WHERE T1.bond_id = 'TR001_10_11'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▋                                                       | 6/18 [01:40<02:57, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many superheroes were published by Dark Horse Comics? (Evidence: published by Dark Horse Comics refers to publisher_name = 'Dark Horse Comics';)\n",
      "Schema: alignment (id, alignment)\n",
      "attribute (id, attribute_name)\n",
      "colour (id, colour)\n",
      "gender (id, gender)\n",
      "publisher (id, publisher_name)\n",
      "race (id, race)\n",
      "superhero (id, superhero_name, full_name, gender_id, eye_colour_id, hair_colour_id, skin_colour_id, race_id, publisher_id, alignment_id, height_cm, weight_kg)\n",
      "hero_attribute (hero_id, attribute_id, attribute_value)\n",
      "superpower (id, power_name)\n",
      "hero_power (hero_id, power_id)\n",
      "\n",
      "Generated SQL: SELECT\n",
      "  COUNT(s.id)\n",
      "FROM superhero AS s\n",
      "JOIN publisher AS p\n",
      "  ON s.publisher_id = p.id\n",
      "WHERE\n",
      "  p.publisher_name = 'Dark Horse Comics';\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████▎                                                  | 7/18 [01:51<02:29, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the race and alignment of Cameron Hicks? (Evidence: Cameron Hicks refers to superhero_name = 'Cameron Hicks';)\n",
      "Schema: alignment (id, alignment)\n",
      "attribute (id, attribute_name)\n",
      "colour (id, colour)\n",
      "gender (id, gender)\n",
      "publisher (id, publisher_name)\n",
      "race (id, race)\n",
      "superhero (id, superhero_name, full_name, gender_id, eye_colour_id, hair_colour_id, skin_colour_id, race_id, publisher_id, alignment_id, height_cm, weight_kg)\n",
      "hero_attribute (hero_id, attribute_id, attribute_value)\n",
      "superpower (id, power_name)\n",
      "hero_power (hero_id, power_id)\n",
      "\n",
      "Generated SQL: SELECT\n",
      "  T2.race,\n",
      "  T3.alignment\n",
      "FROM superhero AS T1\n",
      "INNER JOIN race AS T2\n",
      "  ON T1.race_id = T2.id\n",
      "INNER JOIN alignment AS T3\n",
      "  ON T1.alignment_id = T3.id\n",
      "WHERE\n",
      "  T1.superhero_name = 'Cameron Hicks';\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████████▉                                              | 8/18 [02:02<02:08, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Among the superheroes with height from 170 to 190, list the names of the superheroes with no eye color. (Evidence: height from 170 to 190 refers to height_cm BETWEEN 170 AND 190; no eye color refers to eye_colour_id = 1)\n",
      "Schema: alignment (id, alignment)\n",
      "attribute (id, attribute_name)\n",
      "colour (id, colour)\n",
      "gender (id, gender)\n",
      "publisher (id, publisher_name)\n",
      "race (id, race)\n",
      "superhero (id, superhero_name, full_name, gender_id, eye_colour_id, hair_colour_id, skin_colour_id, race_id, publisher_id, alignment_id, height_cm, weight_kg)\n",
      "hero_attribute (hero_id, attribute_id, attribute_value)\n",
      "superpower (id, power_name)\n",
      "hero_power (hero_id, power_id)\n",
      "\n",
      "Generated SQL: SELECT superhero_name\n",
      "FROM superhero\n",
      "WHERE height_cm BETWEEN 170 AND 190 AND eye_colour_id = 1;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 9/18 [02:13<01:51, 12.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List down at least five superpowers of male superheroes. (Evidence: male refers to gender = 'Male'; superpowers refers to power_name;)\n",
      "Schema: alignment (id, alignment)\n",
      "attribute (id, attribute_name)\n",
      "colour (id, colour)\n",
      "gender (id, gender)\n",
      "publisher (id, publisher_name)\n",
      "race (id, race)\n",
      "superhero (id, superhero_name, full_name, gender_id, eye_colour_id, hair_colour_id, skin_colour_id, race_id, publisher_id, alignment_id, height_cm, weight_kg)\n",
      "hero_attribute (hero_id, attribute_id, attribute_value)\n",
      "superpower (id, power_name)\n",
      "hero_power (hero_id, power_id)\n",
      "\n",
      "Generated SQL: sql\n",
      "SELECT T3.power_name\n",
      "FROM superhero AS T1\n",
      "INNER JOIN gender AS T2\n",
      "  ON T1.gender_id = T2.id\n",
      "INNER JOIN hero_power AS T4\n",
      "  ON T1.id = T4.hero_id\n",
      "INNER JOIN superpower AS T3\n",
      "  ON T4.power_id = T3.id\n",
      "WHERE\n",
      "  T2.gender = 'Male'\n",
      "LIMIT 5;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████▌                                    | 10/18 [02:27<01:40, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the percentage of superheroes who act in their own self-interest or make decisions based on their own moral code? Indicate how many of the said superheroes were published by Marvel Comics. (Evidence: published by Marvel Comics refers to publisher_name = 'Marvel Comics'; superheroes who act in their own self-interest or make decisions based on their own moral code refers to alignment = 'Bad'; calculation = MULTIPLY(DIVIDE(SUM(alignment = 'Bad); count(id)), 100))\n",
      "Schema: alignment (id, alignment)\n",
      "attribute (id, attribute_name)\n",
      "colour (id, colour)\n",
      "gender (id, gender)\n",
      "publisher (id, publisher_name)\n",
      "race (id, race)\n",
      "superhero (id, superhero_name, full_name, gender_id, eye_colour_id, hair_colour_id, skin_colour_id, race_id, publisher_id, alignment_id, height_cm, weight_kg)\n",
      "hero_attribute (hero_id, attribute_id, attribute_value)\n",
      "superpower (id, power_name)\n",
      "hero_power (hero_id, power_id)\n",
      "\n",
      "Generated SQL: SELECT\n",
      "  CAST(SUM(CASE WHEN T2.alignment = 'Bad' THEN 1 ELSE 0 END) AS REAL) * 100 / COUNT(T1.id) AS percentage_bad_alignment,\n",
      "  SUM(CASE WHEN T2.alignment = 'Bad' AND T3.publisher_name = 'Marvel Comics' THEN 1 ELSE 0 END) AS marvel_bad_alignment_count\n",
      "FROM superhero AS T1\n",
      "INNER JOIN alignment AS T2\n",
      "  ON T1.alignment_id = T2.id\n",
      "INNER JOIN publisher AS T3\n",
      "  ON T1.publisher_id = T3.id;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████████████████████████████████████████████████                                | 11/18 [02:39<01:27, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which publisher created more superheroes: DC or Marvel Comics? Find the difference in the number of superheroes. (Evidence: DC refers to publisher_name = 'DC Comics'; Marvel Comics refers to publisher_name = 'Marvel Comics'; if SUM(publisher_name = 'DC Comics') > SUM(publisher_name = 'Marvel Comics'), it means DC Comics published more superheroes than Marvel Comics; if SUM(publisher_name = 'Marvel Comics') > SUM(publisher_name = 'Marvel Comics'), it means Marvel Comics published more heroes than DC Comics; difference = SUBTRACT(SUM(publisher_name = 'DC Comics'), SUM(publisher_name = 'Marvel Comics'));)\n",
      "Schema: alignment (id, alignment)\n",
      "attribute (id, attribute_name)\n",
      "colour (id, colour)\n",
      "gender (id, gender)\n",
      "publisher (id, publisher_name)\n",
      "race (id, race)\n",
      "superhero (id, superhero_name, full_name, gender_id, eye_colour_id, hair_colour_id, skin_colour_id, race_id, publisher_id, alignment_id, height_cm, weight_kg)\n",
      "hero_attribute (hero_id, attribute_id, attribute_value)\n",
      "superpower (id, power_name)\n",
      "hero_power (hero_id, power_id)\n",
      "\n",
      "Generated SQL: sql\n",
      "SELECT\n",
      "  CASE\n",
      "    WHEN SUM(CASE WHEN T2.publisher_name = 'DC Comics' THEN 1 ELSE 0 END) > SUM(CASE WHEN T2.publisher_name = 'Marvel Comics' THEN 1 ELSE 0 END)\n",
      "    THEN 'DC Comics'\n",
      "    ELSE 'Marvel Comics'\n",
      "  END AS publisher_with_more_superheroes,\n",
      "  ABS(SUM(CASE WHEN T2.publisher_name = 'DC Comics' THEN 1 ELSE 0 END) - SUM(CASE WHEN T2.publisher_name = 'Marvel Comics' THEN 1 ELSE 0 END)) AS difference_in_superheroes\n",
      "FROM superhero AS T1\n",
      "INNER JOIN publisher AS T2\n",
      "  ON T1.publisher_id = T2.id\n",
      "WHERE\n",
      "  T2.publisher_name IN ('DC Comics', 'Marvel Comics');\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████▋                           | 12/18 [02:51<01:14, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was the first one paid his/her dues? Tell the full name. (Evidence: full name refers to first_name, last_name; first paid dues refers to MIN(received_date) where source = 'Dues')\n",
      "Schema: event (event_id, event_name, event_date, type, notes, location, status)\n",
      "major (major_id, major_name, department, college)\n",
      "zip_code (zip_code, type, city, county, state, short_state)\n",
      "attendance (link_to_event, link_to_member)\n",
      "budget (budget_id, category, spent, remaining, amount, event_status, link_to_event)\n",
      "expense (expense_id, expense_description, expense_date, cost, approved, link_to_member, link_to_budget)\n",
      "income (income_id, date_received, amount, source, notes, link_to_member)\n",
      "member (member_id, first_name, last_name, email, position, t_shirt_size, phone, zip, link_to_major)\n",
      "\n",
      "Generated SQL: sql\n",
      "SELECT T1.first_name, T1.last_name\n",
      "FROM member AS T1\n",
      "INNER JOIN income AS T2\n",
      "  ON T1.member_id = T2.link_to_member\n",
      "WHERE\n",
      "  T2.source = 'Dues'\n",
      "ORDER BY\n",
      "  T2.date_received ASC\n",
      "LIMIT 1;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████▏                      | 13/18 [03:03<01:00, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many income are received with an amount of 50? (Evidence: amount of 50 refers to amount = 50)\n",
      "Schema: event (event_id, event_name, event_date, type, notes, location, status)\n",
      "major (major_id, major_name, department, college)\n",
      "zip_code (zip_code, type, city, county, state, short_state)\n",
      "attendance (link_to_event, link_to_member)\n",
      "budget (budget_id, category, spent, remaining, amount, event_status, link_to_event)\n",
      "expense (expense_id, expense_description, expense_date, cost, approved, link_to_member, link_to_budget)\n",
      "income (income_id, date_received, amount, source, notes, link_to_member)\n",
      "member (member_id, first_name, last_name, email, position, t_shirt_size, phone, zip, link_to_major)\n",
      "\n",
      "Generated SQL: SELECT\n",
      "  COUNT(income_id)\n",
      "FROM income\n",
      "WHERE\n",
      "  amount = 50;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████▊                  | 14/18 [03:14<00:47, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Name the event with the highest amount spent on advertisement. (Evidence: event refers to event_name; highest amount spent on advertisement refers to MAX(spent) where category = 'Advertisement')\n",
      "Schema: event (event_id, event_name, event_date, type, notes, location, status)\n",
      "major (major_id, major_name, department, college)\n",
      "zip_code (zip_code, type, city, county, state, short_state)\n",
      "attendance (link_to_event, link_to_member)\n",
      "budget (budget_id, category, spent, remaining, amount, event_status, link_to_event)\n",
      "expense (expense_id, expense_description, expense_date, cost, approved, link_to_member, link_to_budget)\n",
      "income (income_id, date_received, amount, source, notes, link_to_member)\n",
      "member (member_id, first_name, last_name, email, position, t_shirt_size, phone, zip, link_to_major)\n",
      "\n",
      "Generated SQL: sql\n",
      "SELECT T1.event_name\n",
      "FROM event AS T1\n",
      "INNER JOIN budget AS T2\n",
      "  ON T1.event_id = T2.link_to_event\n",
      "WHERE\n",
      "  T2.category = 'Advertisement'\n",
      "ORDER BY\n",
      "  T2.spent DESC\n",
      "LIMIT 1;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████▎             | 15/18 [03:25<00:35, 11.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Based on the total cost for all event, what is the percentage of cost for Yearly Kickoff event? (Evidence: DIVIDE(SUM(cost where event_name = 'Yearly Kickoff'), SUM(cost)) * 100)\n",
      "Schema: event (event_id, event_name, event_date, type, notes, location, status)\n",
      "major (major_id, major_name, department, college)\n",
      "zip_code (zip_code, type, city, county, state, short_state)\n",
      "attendance (link_to_event, link_to_member)\n",
      "budget (budget_id, category, spent, remaining, amount, event_status, link_to_event)\n",
      "expense (expense_id, expense_description, expense_date, cost, approved, link_to_member, link_to_budget)\n",
      "income (income_id, date_received, amount, source, notes, link_to_member)\n",
      "member (member_id, first_name, last_name, email, position, t_shirt_size, phone, zip, link_to_major)\n",
      "\n",
      "Generated SQL: SELECT\n",
      "  SUM(CASE WHEN T3.event_name = 'Yearly Kickoff' THEN T1.cost ELSE 0 END) * 100.0 / SUM(T1.cost)\n",
      "FROM expense AS T1\n",
      "INNER JOIN budget AS T2\n",
      "  ON T1.link_to_budget = T2.budget_id\n",
      "INNER JOIN event AS T3\n",
      "  ON T2.link_to_event = T3.event_id;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████████████████████████████████▉         | 16/18 [03:37<00:23, 11.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Calculate the total average cost that Elijah Allen spent in the events on September and October. (Evidence: events in September and October refers to month(expense_date) = 9 AND MONTH(expense_date) = 10)\n",
      "Schema: event (event_id, event_name, event_date, type, notes, location, status)\n",
      "major (major_id, major_name, department, college)\n",
      "zip_code (zip_code, type, city, county, state, short_state)\n",
      "attendance (link_to_event, link_to_member)\n",
      "budget (budget_id, category, spent, remaining, amount, event_status, link_to_event)\n",
      "expense (expense_id, expense_description, expense_date, cost, approved, link_to_member, link_to_budget)\n",
      "income (income_id, date_received, amount, source, notes, link_to_member)\n",
      "member (member_id, first_name, last_name, email, position, t_shirt_size, phone, zip, link_to_major)\n",
      "\n",
      "Generated SQL: sql\n",
      "SELECT\n",
      "  AVG(T1.cost)\n",
      "FROM expense AS T1\n",
      "INNER JOIN member AS T2\n",
      "  ON T1.link_to_member = T2.member_id\n",
      "WHERE\n",
      "  T2.first_name = 'Elijah' AND T2.last_name = 'Allen' AND (\n",
      "    STRFTIME('%m', T1.expense_date) = '09' OR STRFTIME('%m', T1.expense_date) = '10'\n",
      "  );\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████▍    | 17/18 [03:49<00:11, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Find the name and date of events with expenses for pizza that were more than fifty dollars but less than a hundred dollars. (Evidence: name of event refers to event_name; date of event refers to event_date; expenses for pizza refers to expense_description = 'Pizza' where cost > 50 and cost < 100)\n",
      "Schema: event (event_id, event_name, event_date, type, notes, location, status)\n",
      "major (major_id, major_name, department, college)\n",
      "zip_code (zip_code, type, city, county, state, short_state)\n",
      "attendance (link_to_event, link_to_member)\n",
      "budget (budget_id, category, spent, remaining, amount, event_status, link_to_event)\n",
      "expense (expense_id, expense_description, expense_date, cost, approved, link_to_member, link_to_budget)\n",
      "income (income_id, date_received, amount, source, notes, link_to_member)\n",
      "member (member_id, first_name, last_name, email, position, t_shirt_size, phone, zip, link_to_major)\n",
      "\n",
      "Generated SQL: sql\n",
      "SELECT T1.event_name, T1.event_date FROM event AS T1 INNER JOIN budget AS T2 ON T1.event_id = T2.link_to_event INNER JOIN expense AS T3 ON T2.budget_id = T3.link_to_budget WHERE T3.expense_description = 'Pizza' AND T3.cost > 50 AND T3.cost < 100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [04:00<00:00, 13.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to compare without knowledge for ex\n",
      "Process finished successfully\n",
      "start calculate\n",
      "                     simple               moderate             challenging          total               \n",
      "count                6                    6                    6                    18                  \n",
      "======================================    ACCURACY    =====================================\n",
      "accuracy             0.00                 50.00                50.00                33.33               \n",
      "===========================================================================================\n",
      "Finished evaluation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from method_run import run_method\n",
    "import re\n",
    "\n",
    "def function_template(item):\n",
    "    result = run_baseline(item['question'], item['schema'])\n",
    "    # First try to extract query from markdown SQL block\n",
    "    match = re.search(r'```sql\\n(.*?)```', result, re.DOTALL)\n",
    "    if match:\n",
    "        query = match.group(1).strip()\n",
    "    else:\n",
    "        # If no markdown block found, try to extract just SQL query\n",
    "        query = result.strip()\n",
    "        # Remove any ```sql or ``` if present without proper formatting\n",
    "        query = re.sub(r'```sql|```', '', query).strip()\n",
    "    \n",
    "    print(f\"Question: {item['question']}\")\n",
    "    print(f\"Schema: {item['schema']}\")\n",
    "    print(f\"Generated SQL: {query}\\n\")\n",
    "    \n",
    "    return {**item, 'sql': query}\n",
    "\n",
    "run_method(function_template, SLEEP_TIME=10)\n",
    "\n",
    "#Run on mode=nano if you want to test it on a smaller dataset\n",
    "# run_method(function_template, SLEEP_TIME=10, mode=\"nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd06f9",
   "metadata": {},
   "source": [
    "### Chain/Router"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46affa4",
   "metadata": {},
   "source": [
    "Here, you will build a more advanced system that routes the query through different paths based on question difficulty. Easier questions go straight to query generation; harder ones go through schema path extraction first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa254aa",
   "metadata": {},
   "source": [
    "#### Define State (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7db5cc",
   "metadata": {},
   "source": [
    "**Task:** Define a `RouterGraphState` using `MessagesState` and `pydantic` that contains:\n",
    "* The input question and schema\n",
    "* The predicted difficulty level\n",
    "* The extracted schema path\n",
    "* The final query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from typing import Literal\n",
    "\n",
    "class RouterGraphState(MessagesState):\n",
    "    #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696dc1c9",
   "metadata": {},
   "source": [
    "#### Node: Analyser (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ace53",
   "metadata": {},
   "source": [
    "**Task:** Build a node that:\n",
    "* Accepts a question and schema\n",
    "* Analyzes the difficulty (simple/moderate/challanging)\n",
    "* Uses the LLM’s structured output feature to return the difficulty\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. Define a Pydantic class to hold the expected structured output.\n",
    "2. Use structure output mode of LLM to bind it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1969dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class QuestionDifficaultyAnalysis(BaseModel):\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "def analyser_node(state: RouterGraphState):\n",
    "    #YOUR CODE HERE\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f78d38c",
   "metadata": {},
   "source": [
    "#### Conditional Edge (2 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d17e0",
   "metadata": {},
   "source": [
    "**Task:** Implement a branching function that decides whether to proceed to direct query generation or schema path extraction based on the difficulty label returned by the analyser.\n",
    "\n",
    "* If the difficulty is “easy”, go directly to query generation.\n",
    "* Otherwise, extract the schema path first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908afa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_schema_extraction_needed(state: RouterGraphState)->Literal[\"schema_path_extractor\", \"query_generator\"]:\n",
    "  #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c9d25",
   "metadata": {},
   "source": [
    "#### Node: Schema Extractor (3 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e159a0f9",
   "metadata": {},
   "source": [
    "**Task:** Implement a node that takes the question and schema and extracts a join path or sequence of relevant tables from the schema based on the question.\n",
    "\n",
    "* Use a simple prompt for this.\n",
    "* Store the result in the `schema_path` field of the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e82812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_path_extractor_node(state: RouterGraphState):\n",
    "    #YOUR CODE HERE\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091dc790",
   "metadata": {},
   "source": [
    "#### Node: Generator (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f374e09",
   "metadata": {},
   "source": [
    "**Task:** Generate the SQL query based on the question and schema.\n",
    "\n",
    "* If a schema path is available, include it in the prompt.\n",
    "* Save the output query in the `query` field of the state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a600328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_generator_node(state: RouterGraphState):\n",
    "    #YOUR CODE HERE\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b20d96c",
   "metadata": {},
   "source": [
    "#### Build Graph (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416b89b",
   "metadata": {},
   "source": [
    "**Task:** Assemble the full routing graph using the nodes and edges you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f58c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "router_graph_builder = StateGraph(RouterGraphState)\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "router_graph = router_graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204fab8e",
   "metadata": {},
   "source": [
    "#### Run and Evaluate (Estimated Run Time 10-15min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f0bc7",
   "metadata": {},
   "source": [
    "**Task:** Run your compiled routing graph on a dataset. For each question:\n",
    "\n",
    "* Instantiate the `RouterGraphState` with the question and schema.\n",
    "* Run the graph to completion.\n",
    "* Extract and clean the query from the result.\n",
    "\n",
    "Use the `run_method` function to handle iteration and timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from method_run import run_method\n",
    "def run_router_graph(item):\n",
    "    response = router_graph.invoke(\n",
    "        RouterGraphState(\n",
    "            question=item['question'],\n",
    "            schema=item['schema'],\n",
    "            schema_path=None,\n",
    "            question_difficulty=None,\n",
    "            query=None\n",
    "        )\n",
    "    )\n",
    "    result = response[\"query\"]\n",
    "    # First try to extract query from markdown SQL block\n",
    "    match = re.search(r'```sql\\n(.*?)```', result, re.DOTALL)\n",
    "    if match:\n",
    "        query = match.group(1).strip()\n",
    "    else:\n",
    "        # If no markdown block found, try to extract just SQL query\n",
    "        query = result.strip()\n",
    "        # Remove any ```sql or ``` if present without proper formatting\n",
    "        query = re.sub(r'```sql|```', '', query).strip()\n",
    "    print(f\"Question: {item['question']}\")\n",
    "    print(f\"Schema: {item['schema']}\")\n",
    "    print(f\"Question Difficulty: {response['question_difficulty']}\")\n",
    "    if response[\"schema_path\"]:\n",
    "        print(f\"Schema Path: {response['schema_path']}\")\n",
    "    print(f\"Generated SQL: {query}\\n\")\n",
    "    return {**item, 'sql': query}\n",
    "\n",
    "\n",
    "run_method(run_router_graph, SLEEP_TIME=30)\n",
    "\n",
    "#Run on mode=nano if you want to test it on a smaller dataset\n",
    "#run_method(run_router_graph, SLEEP_TIME=10, mode=\"nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4666dff4",
   "metadata": {},
   "source": [
    "### Agent (ReAct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc99580",
   "metadata": {},
   "source": [
    "Now you will implement a full ReAct agent that incrementally solves the Text-to-SQL task using tools. The agent can explore tables and columns before finalizing the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df0a65",
   "metadata": {},
   "source": [
    "**You are not allowed to use 'Prebuilt Agent' of LangGraph. You have to build your own graph.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505b9f8",
   "metadata": {},
   "source": [
    "#### Define Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3582a",
   "metadata": {},
   "source": [
    "**Task:** Define three tools for the agent to interact with the schema:\n",
    "1. `get_samples_from_table`: Returns the first few rows of a table.\n",
    "2. `get_column_description`: Provides a human-readable description of a specific column.\n",
    "3. `execute`: Executes a SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab00e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from db_manager import DBManager\n",
    "db_manager = DBManager()\n",
    "\n",
    "@tool\n",
    "def get_samples_from_table(table_name: str, config: RunnableConfig):\n",
    "  \"\"\"Gets the first few rows (samples) from a specified table.\n",
    "\n",
    "  Args:\n",
    "    table_name: The name of the table from which to fetch samples.\n",
    "\n",
    "  Returns:\n",
    "    The first few rows from the specified table.\n",
    "  \"\"\"\n",
    "  db_name = config[\"configurable\"].get(\"database_name\")\n",
    "  result = db_manager.get_table_head(table_name, db_name=db_name)\n",
    "  return result\n",
    "\n",
    "@tool\n",
    "def get_column_description(table_name: str, column_name: str, config: RunnableConfig):\n",
    "  \"\"\"Provides a description for a specific column within a given table.\n",
    "\n",
    "  Args:\n",
    "    table_name: The name of the table containing the column.\n",
    "    column_name: The name of the column for which to get the description.\n",
    "\n",
    "  Returns:\n",
    "    A string containing the description of the specified column.\n",
    "  \"\"\"\n",
    "  db_name = config[\"configurable\"].get(\"database_name\")\n",
    "  result = db_manager.get_column_description(db_name, table_name, column_name)\n",
    "  return result\n",
    "\n",
    "@tool\n",
    "def execute(query: str, config: RunnableConfig):\n",
    "  \"\"\"Executes a given SQL query against the database.\n",
    "\n",
    "  Args:\n",
    "    query: The SQL query string to be executed.\n",
    "\n",
    "  Returns:\n",
    "    The result of the executed query. This could be a set of rows,\n",
    "    a confirmation message, or an error.\n",
    "  \"\"\"\n",
    "  db_name = config[\"configurable\"].get(\"database_name\")\n",
    "  result = db_manager.query(query, db_name)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66680244",
   "metadata": {},
   "source": [
    "#### Extra Tool (5+5 Bonus Points):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80baae9",
   "metadata": {},
   "source": [
    "**Task**: Create and integrate a new custom tool into the ReAct agent. To receive credit for this part, your tool must be meaningfully different from the existing three tools and provide practical value in helping the agent generate more accurate or efficient SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0308d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe11d0",
   "metadata": {},
   "source": [
    "#### Create Tool Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_samples_from_table, get_column_description, execute]\n",
    "tools_node = ToolNode(tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348623d",
   "metadata": {},
   "source": [
    "#### ReAct Agent Prompt (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d0f151",
   "metadata": {},
   "source": [
    "**Task:** Set up the agent node with planning, tool use, and final SQL generation prompts. For writing efficient prompt you can read this link.\n",
    "https://cookbook.openai.com/examples/gpt4-1_prompting_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "REACT_SYS_PROMPT = \"\"\"\n",
    "#YOUR PROMPT HERE\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee385fd",
   "metadata": {},
   "source": [
    "#### Agent Node (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549f4b1",
   "metadata": {},
   "source": [
    "**Task:** Set up the agent node with models that have binded with tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def agent_node(state: MessagesState) -> MessagesState:\n",
    "    #For rate-limiting purposes, we will sleep for 10 seconds before invoking the LLM\n",
    "    time.sleep(10)\n",
    "    #YOUR CODE HERE\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4d541",
   "metadata": {},
   "source": [
    "#### Build Graph (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbf177",
   "metadata": {},
   "source": [
    "**Task:** Assemble the ReAct agent graph, connecting the agent node and tool node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import tools_condition\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class ConfigSchema(TypedDict):\n",
    "    database_name: str\n",
    "\n",
    "react_builder = StateGraph(MessagesState, config_schema=ConfigSchema)\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "react_graph = react_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7aee0f",
   "metadata": {},
   "source": [
    "#### Run and Evaluate (Estimated Run Time 20min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a2020",
   "metadata": {},
   "source": [
    "**Task:** Execute the ReAct agent pipeline on the dataset and collect SQL outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9184c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from method_run import run_method\n",
    "import re\n",
    "def run_react_agent_with_config(item):\n",
    "    question = item['question']\n",
    "    schema = item['schema']\n",
    "    user_prompt = f\"Question: {question}\\nSchema: {schema}\"\n",
    "    input_msg = HumanMessage(content=user_prompt)\n",
    "    input_config = {\"configurable\": {\"database_name\": item['db_id']}}\n",
    "    response = react_graph.invoke(MessagesState(messages=[input_msg]), config=input_config)\n",
    "\n",
    "    for msg in response[\"messages\"]:\n",
    "        msg.pretty_print()\n",
    "        \n",
    "    # If last AI Message is a list of messages, we need to extract the last one\n",
    "    last_msg = response[\"messages\"][-1].content\n",
    "    if isinstance(last_msg, list):\n",
    "        last_msg = last_msg[-1]\n",
    "\n",
    "    # First try to extract query from markdown SQL block\n",
    "    match = re.search(r'```sql\\n(.*?)```', last_msg, re.DOTALL)\n",
    "    if match:\n",
    "        query = match.group(1).strip()\n",
    "    else:\n",
    "        # If no markdown block found, try to extract just SQL query\n",
    "        query = last_msg.strip()\n",
    "        # Remove any ```sql or ``` if present without proper formatting\n",
    "        query = re.sub(r'```sql|```', '', query).strip()\n",
    "\n",
    "    return {**item, 'sql': query}\n",
    "\n",
    "#Run agent on mode=nano, it's not needed to run on full dataset\n",
    "run_method(run_react_agent_with_config, SLEEP_TIME=20, mode=\"nano\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
