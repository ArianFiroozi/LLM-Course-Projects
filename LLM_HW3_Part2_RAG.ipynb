{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFsETkP_6ZoL"
      },
      "source": [
        "# **CA3-Part2, LLMs Spring 2025**\n",
        "\n",
        "- **Name:**\n",
        "- **Student ID:**\n",
        "\n",
        "---\n",
        "#### Your submission should be named using the following format: `CA3-Part2_LASTNAME_STUDENTID.ipynb`.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks under each `Completion` section.\n",
        "\n",
        "- For text-based answers, you should replace the text that says `WRITE YOUR ANSWER HERE` with your actual answer, or you can look for `Report` and `Question` blocks.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---\n",
        "\n",
        "If you have any further questions or concerns, contact the TAs via email or Telegram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zivgc23P8r0f"
      },
      "source": [
        "# RAG (50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7koJfgPBRlNz"
      },
      "source": [
        "If you have any further questions or concerns, contact the TA via email (pouya.sadeghi@ut.ac.ir) or telegram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNHplrzc8r0f"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iX-Lm3M58r0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76a40f0-8b99-4b1e-9625-94e0d9a8ef6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m141.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_community langchain_huggingface huggingface_hub\n",
        "!pip install -q sentence_transformers tiktoken lark datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SalrEJJrutW",
        "outputId": "6f58ed09-0627-420e-f94c-9d9eba62aede"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May  6 11:53:41 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3MhNsfsNVAVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37d70aa7-efc6-41a1-8d26-9f91cc89e20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# To determine your system's CUDA version, run the following command:\n",
        "# !nvidia-smi\n",
        "\n",
        "# Based on your CUDA version, install the appropriate FAISS-GPU package:\n",
        "\n",
        "# For CUDA 12.x:\n",
        "!pip install -q faiss-gpu-cu12\n",
        "\n",
        "# For CUDA 11.x:\n",
        "# !pip install faiss-gpu-cu11\n",
        "\n",
        "# If you prefer the CPU-only version of FAISS:\n",
        "# !pip install -q faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuwfOPincl-1"
      },
      "source": [
        "## 1. An Overview of Information Retrieval (IR) and RAG (2 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHWLyGsKc59P"
      },
      "source": [
        "- **Information Retrieval (IR)**: The process of obtaining information system resources relevant to a specific information need from a collection of those resources. Each IR system consists of a collection of documents, a set of queries, and a retrieval function that ranks the documents based on their relevance to the query.\n",
        "- **Retrieval-Augmented Generation (RAG)**: A model that combines the strengths of retrieval-based and generation-based approaches. It retrieves relevant documents from a large corpus and uses them to generate a response to a query. RAG is particularly useful for tasks where the answer is not explicitly present in the training data but can be inferred from related documents.\n",
        "- **RAG Architecture**: The RAG architecture consists of two main components:\n",
        "  - **Retriever**: This component retrieves relevant documents from a large corpus based on the input query. It can be implemented using various retrieval methods, such as BM25 or dense retrieval.\n",
        "  - **Generator**: This component generates a response based on the retrieved documents and the input query. It can be implemented using transformer-based models.\n",
        "  \n",
        "In this computer assignment, you will implement a RAG pipeline using the LangChain framework. You will use two different retrievers: TF-IDF and dense retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0wV_2SCcl-2"
      },
      "source": [
        "#### Question 1: (2 points)\n",
        "1. Why do we need to use RAG?\n",
        "2. What is LangChain and how does it help in building RAG pipelines?\n",
        "<!-- 2. What are the advantages of RAG over generation methods? -->\n",
        "<!-- 3. What is the difference between dense and sparse retrievers? -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0zBJI6O6ZoN"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiB9LEjb8r0f"
      },
      "source": [
        "## 2. An Overview of LangChain (12 points + 2)\n",
        "\n",
        "In this overview, we will provide a step-by-step guide on how to construct a basic application using LangChain. To learn more about this framework, check its [tutorial](https://python.langchain.com/docs/tutorials/) which is available for different releases!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPaEdN5E8r0g"
      },
      "source": [
        "### 2.1 Lets load our model (4 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRahQhR_cl-2"
      },
      "source": [
        "#### Question 2: (2 points)\n",
        "\n",
        "1. Explain how different parameters, such as `temperature`, `max_length`, `top_p`, `top_k`, and `repetition_penalty`, affect the generation process in a language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8uW1zlt6ZoO"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKqpgVSmcl-2"
      },
      "source": [
        "#### Completion 1: (2 points)\n",
        "\n",
        "Load the `microsoft/Phi-4-mini-instruct` model and its tokenizer, and create a `text-generation` pipeline. Use the LangChain framework to integrate the model into your application. You should configure the pipeline with appropriate parameters, such as *max_new_tokens*, *temperature*, *top_p*, *top_k*, and *repetition_penalty*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "64-XOX3M8r0g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3c64ffd50c78429aa7da2ec51dd9797a",
            "9697c587102c44cfbcc60ee59f223a6f",
            "0b8ef95054cf4732bc2f2d1f1b659207",
            "96c957caddb34d3e99ecf95ffcdac8e4",
            "9e61680b7c194192a5b0b0ab793b6f25",
            "6b2318a1b1444640969a4ef289d6fa5f",
            "23c54afe53d54e8ab104db39611ee5b9",
            "04e4c8a5fbfc484ab737365f57784c64",
            "fa54c6c70d7d41148e86799e9bd833f1",
            "546ed3fa25314ee8aa5aaee532c57ab9",
            "32c6e2932428430883580809138126e1"
          ]
        },
        "outputId": "57546c5d-f8b1-4573-ab0b-8586ee2f5be3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c64ffd50c78429aa7da2ec51dd9797a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
        "DEVICE = \"cuda:0\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=DEVICE,\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens= 50)\n",
        "\n",
        "# Load the pipeline into LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCphndr0uyDf",
        "outputId": "b6101874-d6be-4ddd-bfdc-862f1ac7f45d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hAxNYgBGcl-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87bd6e92-5017-4b1e-a27b-6865337113b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is the president of the United States? The president of the United States is Joe Biden. He assumed office on January 20, 2021, after winning the 2020 presidential election.\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Who is the president of the United States?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joCaX8tY8r0g"
      },
      "source": [
        "### 2.2 Simple Chain (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdkMm3Mz8r0h"
      },
      "source": [
        "#### Completion 2: (2 points)\n",
        "\n",
        "Complete the next cell to create a simple chain that takes the name of a football (soccer) player as input and outputs some information about that person. To do so:\n",
        "\n",
        "1. Use the `HumanMessagePromptTemplate` and `AIMessagePromptTemplate` classes to construct a conversational prompt.\n",
        "2. Use `ChatPromptTemplate` to organize the messages.\n",
        "3. Pass the prompt into the model you have loaded before.\n",
        "4. Use `StrOutputParser` to return a plain string.\n",
        "\n",
        "Your final chain should take a dictionary with a **person_name** key and return a brief description about that player."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gMEB4D4z8r0h"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# Create a simple prompt template with a human message and an AI message\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "  HumanMessagePromptTemplate.from_template(\"Give me a brief explanation of {person_name} in soccer.\"),\n",
        "  AIMessagePromptTemplate.from_template(\"{person_name} is\"),\n",
        "])\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Create a simple chain with the prompt, LLM, and output parser. The goal is to generate a response to the prompt and parse the output as a string.\n",
        "simple_chain = (\n",
        "    prompt |\n",
        "    llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tGGAq5Atcl-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33bb4114-fb1a-445c-cc24-e1f5cfef36aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Give me a brief explanation of Kylian Mbappé in soccer.\n",
            "AI: Kylian Mbappé is a professional French footballer who plays as a forward. He is known for his speed, agility, and goal-scoring ability. Mbappé has been a key player for Paris Saint-Germain (PSG) in France and has also represented the French\n"
          ]
        }
      ],
      "source": [
        "answer = simple_chain.invoke({\"person_name\": \"Kylian Mbappé\"})\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq8wDeKK8r0h"
      },
      "source": [
        "#### Question 3: (2 points)\n",
        "\n",
        "1. Write about the objectives behind the creation of `HumanMessagePromptTemplate` and `AIMessagePromptTemplate` classes. What they actually do and how should we use them? Write a brief description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2syj8rxJ6ZoP"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoN6LFaW8r0h"
      },
      "source": [
        "### 2.3 JSON Chain (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F56RIBhk8r0i"
      },
      "source": [
        "#### Completion 3: (1 point)\n",
        "\n",
        "Now we want to improve the chain to extract data from the model response. Modify the existing prompt to request information about a football player, such as:\n",
        "- full name\n",
        "- nationality\n",
        "- age\n",
        "- current club\n",
        "- position\n",
        "\n",
        "In this chain, you can use `SystemMessagePromptTemplate` as well.\n",
        "At the end, use `JsonOutputParser` to parse the model's output and return a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ztIkrpmk8r0i"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Create the prompt template.\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"You are an expert in football, and you give statistics about players like full name, nationality, age, current club and position. format your answers as json.\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"Give me a brief explanation of {player_name} in soccer.\"),\n",
        "    AIMessagePromptTemplate.from_template(\"{{\")\n",
        "])\n",
        "\n",
        "# Use JsonOutputParser to parse the response as a dictionary\n",
        "output_parser = JsonOutputParser()\n",
        "\n",
        "get_ai_resp = lambda x: x.split(\"AI:\")[-1]\n",
        "\n",
        "# Define the chain. This time, we want output to be parsed as JSON, using the JsonOutputParser.\n",
        "json_chain =prompt|llm|get_ai_resp|output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "TVy5yaAJi3NU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25486c2d-6d54-40f8-ad71-b156c7d59c00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'full_name': 'Lionel Andrés Messi',\n",
              " 'nationality': 'Argentine',\n",
              " 'age': 35,\n",
              " 'current_club': 'Paris Saint-Germain',\n",
              " 'position': 'Forward'}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "json_chain.invoke({\"player_name\": \"Lionel Messi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "8xMWli_Ocl-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5da4d7-6271-4f34-e73d-74fc2a4697bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lionel Messi:\n",
            "  full_name: Lionel Andrés Messi\n",
            "  nationality: Argentine\n",
            "  age: 35\n",
            "  current_club: Paris Saint-Germain\n",
            "  position: Forward\n",
            "\n",
            "Cristiano Ronaldo:\n",
            "  full_name: Cristiano Ronaldo dos Santos Aveiro\n",
            "  nationality: Portuguese\n",
            "  age: 37\n",
            "  current_club: Al Nassr\n",
            "  position: Forward\n",
            "\n",
            "Kylian Mbappé:\n",
            "  full_name: Kylian Mbappé\n",
            "  nationality: French\n",
            "  age: 22\n",
            "  current_club: Paris Saint-Germain\n",
            "  position: Forward\n",
            "\n",
            "Neymar:\n",
            "  full_name: Neymar Jr.\n",
            "  nationality: Brazilian\n",
            "  age: 32\n",
            "  current_club: Paris Saint-Germain\n",
            "  position: Forward\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Batch the requests for multiple\n",
        "batch_questions = [\n",
        "  {\"player_name\": \"Lionel Messi\"},\n",
        "  {\"player_name\": \"Cristiano Ronaldo\"},\n",
        "  {\"player_name\": \"Kylian Mbappé\"},\n",
        "  {\"player_name\": \"Neymar\"}\n",
        "]\n",
        "answers = json_chain.batch(batch_questions)\n",
        "\n",
        "# Print the extracted information\n",
        "for (q, a) in zip(batch_questions, answers):\n",
        "  print(f\"{q['player_name']}:\")\n",
        "  for key, value in a.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ0ogQiMcl-4"
      },
      "source": [
        "#### Report 1: (1 point)\n",
        "\n",
        "Explain the challenges you faced in this step. How did you manage to solve them? How could the parameters you used in the text generation pipeline affect the model’s output?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uowWBQc6ZoP"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_3x5rCT6ZoP"
      },
      "source": [
        "#### Question 4: (2 points)\n",
        "\n",
        "1. How sampling parameters such as *temperature*, *top_p*, and *top_k* can affect our JSON pipeline? Answer the question with respect to the format and content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mQ6qjFu6ZoP"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RF_aXt6dC9g"
      },
      "source": [
        "### 2.4 (Optional) Tool use: Web search (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjkcoqElcl-4"
      },
      "source": [
        "#### Completion 4: (2 points)\n",
        "\n",
        "Add context about each player by using web search. For this purpose, you need to use a web search tool, and write a function to check the output of the search tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE-1-fzpcl-4"
      },
      "outputs": [],
      "source": [
        "# Code here. Note that you may need to install additional packages for web search tool.\n",
        "\n",
        "\n",
        "\n",
        "# Define the chain here\n",
        "footballer_chain = (\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UhjgFJTcl-4"
      },
      "outputs": [],
      "source": [
        "# Batch the requests for multiple\n",
        "batch_questions = [\n",
        "  {\"player_name\": \"Lionel Messi\"},\n",
        "  {\"player_name\": \"Cristiano Ronaldo\"},\n",
        "  {\"player_name\": \"Kylian Mbappé\"},\n",
        "  {\"player_name\": \"Neymar\"},\n",
        "  {\"player_name\": \"Declan Rice\"},\n",
        "  {\"player_name\": \"Trent Alexander-Arnold\"},\n",
        "  {\"player_name\": \"John Stones\"},\n",
        "  {\"player_name\": \"Alphonso Davies\"}\n",
        "]\n",
        "answers = footballer_chain.batch(batch_questions)\n",
        "\n",
        "# Print the extracted information\n",
        "for (q, a) in zip(batch_questions, answers):\n",
        "  print(f\"{q['player_name']}:\")\n",
        "  for key, value in a.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS8as30J8r0i"
      },
      "source": [
        "## 3. Build a RAG pipeline (26 points + 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aM-prY48r0i"
      },
      "source": [
        "In this section, We use a subset of [RecipeNLG](https://recipenlg.cs.put.poznan.pl) dataset to build our RAG pipelines. The dataset contains recipes and their corresponding instructions.\n",
        "\n",
        "You can download the subset from [this google drive link](https://drive.google.com/file/d/1mgPcQKc7-SaWVyxaJ404L6dGkQvODca5/view?usp=sharing) or from the course website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW2JuoTzNNek"
      },
      "source": [
        "### 3.1 Load and prepare the dataset (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGWVvgLqNNek"
      },
      "source": [
        "#### Completion 5: (4 point)\n",
        "\n",
        "First, you should load the dataset, which is stored in a CSV file. and converting it to a `datasets.Dataset` object.\n",
        "\n",
        "The dataset contains the following columns:\n",
        "- **title**: The name of the recipe\n",
        "- **ingredients**: A list of ingredients used in the recipe, including quantities and preparation methods\n",
        "- **directions**: The instructions for preparing the recipe, presented as a list of sequential steps\n",
        "- **NER**: A list of named entities representing the core food items and cooking components extracted from each recipe, without quantities or preparation instructions.\n",
        "\n",
        "**Attention**: You should carefully process list objects (ingredients, directions, and NER) and convert them to a string document.\n",
        "\n",
        "**Attention 2**: The provided dataset, has 5k recipes. You can use a smaller subset of the dataset for your experiments. For example, you can use the first 100 recipes for your experiments or more, based on your resource limitation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIkZv62ZNNek"
      },
      "outputs": [],
      "source": [
        "!pip install -q gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cVk_djLNNek"
      },
      "outputs": [],
      "source": [
        "!gdown 1mgPcQKc7-SaWVyxaJ404L6dGkQvODca5 -O data_5000.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmjlX1Go8r0i"
      },
      "outputs": [],
      "source": [
        "# Code here to load and process the dataset\n",
        "\n",
        "\n",
        "# Store the datasets.Dataset object in the variable `dataset`\n",
        "dataset =\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF2MjTYelRfB"
      },
      "outputs": [],
      "source": [
        "# In this cell, you should store the dataset, as a list of `langchain_core.documents.Document` objects, which can simplify your future steps.\n",
        "# You should decide how to convert the dataset to documents\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents: list[Document] = []\n",
        "\n",
        "\n",
        "print(f\"Number of documents: {len(documents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bHNwp3sNNel"
      },
      "outputs": [],
      "source": [
        "# Now, you should use a splitter to divide long texts into smaller, manageable chunks so they can fit within the context window of language models or retrievers.\n",
        "# Use `RecursiveCharacterTextSplitter` to split the documents into smaller chunks, ans set the `chunk_size` and `chunk_overlap` parameters accordingly.\n",
        "\n",
        "\n",
        "\n",
        "chunks = []\n",
        "\n",
        "print(f\"Number of chunks: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN7dSqY-NNel"
      },
      "source": [
        "### 3.2 Sparse Retriever (3 points)\n",
        "\n",
        "In this section, we would create a sparse retriever for our RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elzh6QSSNNem"
      },
      "source": [
        "#### Question 5: (2 points)\n",
        "\n",
        "1. Explain how a sparse retriever like TF-IDF represents documents and queries. How does this representation influence which documents are retrieved?\n",
        "2. Sparse retrievers rely on exact token matches between queries and documents. What are the strengths and weaknesses of this approach, especially compared to dense retrievers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrP8Kz1K6Zoa"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvmpQ-BtNNem"
      },
      "source": [
        "#### Completion 6: (1 point)\n",
        "\n",
        "Complete the code cells below to create a sparse retriever, which would be later used in our RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rq6DS9wNNem"
      },
      "outputs": [],
      "source": [
        "# Prepare your retriever. For this section, you should use a sparse retriever such as `TFIDF` or `BM25`.\n",
        "# We want our retriever to retrieve the first 3 chunks that are most relevant to the query.\n",
        "\n",
        "\n",
        "sparse_retriever ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7IoZQioNNem"
      },
      "outputs": [],
      "source": [
        "# Query below is related to `Zucchini Nut Bread` recipe.\n",
        "\n",
        "Sample_query = \"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\"\n",
        "\n",
        "# Use the sparse retriever to get the most relevant chunks for the query\n",
        "retrieved_chunks =\n",
        "\n",
        "# Now, see what chunks were retrieved\n",
        "for i, chunk in enumerate(retrieved_chunks, start=1):\n",
        "    print(f\"Chunk {i}:\")\n",
        "    # print(chunk.page_content)\n",
        "    print(\"Metadata:\", chunk.metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qnbeJnz8r0k"
      },
      "source": [
        "### 3.3 Semantic Retriever (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgEphnmn8r0k"
      },
      "source": [
        "#### Question 6: (2 point)\n",
        "\n",
        "1. How does a semantic retriever represent documents and queries differently from a sparse retriever? Explain why this helps in cases where there are no exact word overlaps.\n",
        "2. What role do embedding models (e.g., sentence-transformers) play in semantic retrieval? How does the choice of embedding model affect the retriever’s performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6u_rLg-6Zob"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1GCzoOXUf8c"
      },
      "source": [
        "#### Completion 7: (2 point)\n",
        "\n",
        "Let's create a semantic retriever. We would use `BAAI/bge-small-en` as our embedding model, and `FAISS` as our vector store. Complete the code cells below to create a semantic retriever, which would be later used in our RAG pipeline.\n",
        "\n",
        "As explained before, we want our retriever to retrieve the first 3 most relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6NzUEAs8r0k"
      },
      "outputs": [],
      "source": [
        "# Code here\n",
        "\n",
        "embedding_model =\n",
        "semantic_retriever ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5Uc59HkUf8c"
      },
      "outputs": [],
      "source": [
        "# Query below is related to `Zucchini Nut Bread` recipe.\n",
        "\n",
        "Sample_query = \"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\"\n",
        "\n",
        "# Use the semantic retriever to get the most relevant chunks for the query\n",
        "retrieved_chunks =\n",
        "\n",
        "# Now, see what chunks were retrieved\n",
        "for i, chunk in enumerate(retrieved_chunks, start=1):\n",
        "    print(f\"Chunk {i}:\")\n",
        "    # print(chunk.page_content)\n",
        "    print(\"Metadata:\", chunk.metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYbF75HP2bBc"
      },
      "source": [
        "### 3.4 Create RAG pipelines (6 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_KnISF32bBc"
      },
      "source": [
        "#### Question 7: (2 points)\n",
        "\n",
        "1. What are the main components of a RAG system, and how do they interact during inference? Describe the flow from user input to model output.\n",
        "2. Describe two different strategies for integrating retrieved context into the prompt. What are the trade-offs between these approaches?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwNj8aph6Zob"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avtuGMuT2bBc"
      },
      "source": [
        "#### Completion 8: (4 points)\n",
        "\n",
        "Follow the instructions below to build a RAG pipeline using the retrievers you created in the previous sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqo5m1La2bBc"
      },
      "outputs": [],
      "source": [
        "Sample_query = \"\"\"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\n",
        "\n",
        "What is your best guess about what am I cooking?\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxybNSeT2bBc"
      },
      "outputs": [],
      "source": [
        "# We are going to use \"microsoft/Phi-4-mini-instruct\" as our LLM again. If you need, load it again here and as before.\n",
        "\n",
        "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
        "tokenizer =\n",
        "model =\n",
        "pipe =\n",
        "llm ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-_gpU7u2bBc"
      },
      "outputs": [],
      "source": [
        "# First, we need to define a new chat template, that provide the retrieved documents as context to the LLM.\n",
        "from langchain_core.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YU0sdZa2bBc"
      },
      "outputs": [],
      "source": [
        "# Now, let's create a simple RAG pipeline, using the sparse retriever. Note that we need the retrieved context as part of the output, so that we can later use it for evaluation.\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "sparse_rag = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : (input)  populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : (output) chunks retrieved by the sparse retriever, based on the \"question\" value\n",
        "    # \"response\" : (output) the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #                       into the LLM and stored in a key called \"response\"\n",
        "\n",
        ")\n",
        "\n",
        "# Now, let's test the sparse RAG pipeline with a sample query.\n",
        "response =\n",
        "print(response[\"response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRy88ZMi2bBc"
      },
      "outputs": [],
      "source": [
        "# For this cell, everything is the same as the previous cell, except that we are using the semantic retriever instead of the sparse retriever.\n",
        "\n",
        "semantic_rag = (\n",
        "    # The same as previous cell, but using the semantic retriever instead of the sparse retriever\n",
        ")\n",
        "\n",
        "# Now, let's test the semantic RAG pipeline with a sample query.\n",
        "response =\n",
        "print(response[\"response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvYodfcg2bBd"
      },
      "outputs": [],
      "source": [
        "# Finally, let's try the same query with the LLM directly, without any retrieval.\n",
        "response =\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mCOh9_i2bBd"
      },
      "source": [
        "### 3.5 Evaluate our pipelines (9 points)\n",
        "\n",
        "In this section, we are going to evaluate our RAG pipelines. First, we would design 5 queries to evaluate our RAG pipelines and our LLM alone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDgQ1uNE2bBd"
      },
      "source": [
        "#### Completion 9: (1 point)\n",
        "\n",
        "Add 4 more queries, similar to the example. The examples would be based on the first 100 recipes of our dataset.\n",
        "We would keep the title of the recipe that we used to create the query, for future reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2pV6lw_2bBd"
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    {\n",
        "        \"title\": \"Balsamic Chicken Pasta with Fresh Cheese\",\n",
        "        \"query\": \"I am cooking dinner. Here is what my kitchen looks like:\\nThe linguine is cooked and set aside. The red bell peppers are soft and slightly caramelized. The balsamic dressing is mixed with garlic, salt, pepper, and fresh basil. Each component is ready in its bowl, colorful and aromatic.\\n\\nWhat should I do as my next step?\"\n",
        "    },\n",
        "    # Add 4 more\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Gglt-XH2bBd"
      },
      "outputs": [],
      "source": [
        "from textwrap import fill\n",
        "questions = [{\"question\": q[\"query\"]} for q in queries]\n",
        "\n",
        "llm_responses = llm.batch([q[\"query\"] for q in queries])\n",
        "sparse_rag_responses = sparse_rag.batch(questions)\n",
        "semantic_rag_responses = semantic_rag.batch(questions)\n",
        "\n",
        "for query, r1, r2, r3 in zip(queries, llm_responses, sparse_rag_responses, semantic_rag_responses):\n",
        "    print(f'{query[\"title\"]}:')\n",
        "    print(f'  - Without  RAG: {fill(r1, width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print(f'  - Sparse   RAG: {fill(r2[\"response\"], width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print(f'  - Semantic RAG: {fill(r3[\"response\"], width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOJLRpnD2bBd"
      },
      "source": [
        "#### Report 2: (2 points)\n",
        "\n",
        "Write a report about the experiments above. Your report should address the following:\n",
        "1. Compare the quality of the answers. In which cases did Sparse or Semantic RAG help improve the response? Was there any example where it hurt the performance?\n",
        "2. Discuss the differences between Sparse and Semantic RAG. Based on your examples, which one seems more effective and why?\n",
        "3. Any surprising findings or patterns? Did anything behave differently than you expected?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtIsiDjA6Zoc"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TJIR0sJ2bBd"
      },
      "source": [
        "#### Completion 10: (3 points)\n",
        "\n",
        "Now we want to automate the evaluation process. For this purpose, we are going to use the `ragas`. Follow the instructions of each cell to create the evaluation pipeline. To learn more about this framework, please refer to its [get started](https://docs.ragas.io/en/stable/getstarted/) or [how-to](https://docs.ragas.io/en/stable/howtos/) pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxjSZO7V2bBd"
      },
      "outputs": [],
      "source": [
        "!pip install -q ragas rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG_Q2JCf2bBd"
      },
      "outputs": [],
      "source": [
        "# Load the LLM as ragas llm. For this, we can use the provided wrapper for our existing LLM.\n",
        "\n",
        "ragas_llm ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuL-IP9L2bBd"
      },
      "outputs": [],
      "source": [
        "# Generate 10 test cases, using ragas, based on your documents. You can use a subset of your documents for faster runtime.\n",
        "test_set =\n",
        "\n",
        "\n",
        "test_set.test_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O-XwiVs2bBd"
      },
      "outputs": [],
      "source": [
        "test_df = test_set.to_pandas()\n",
        "test_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU3QIdVW2bBd"
      },
      "outputs": [],
      "source": [
        "test_questions = test_df[\"question\"].values.tolist()\n",
        "test_ground_truths = test_df[\"ground_truth\"].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAyWxf1u2bBd"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"sparse\": {\n",
        "        \"answers\": [],\n",
        "        \"contexts\": []\n",
        "    },\n",
        "    \"dense\": {\n",
        "        \"answers\": [],\n",
        "        \"contexts\": []\n",
        "    },\n",
        "}\n",
        "\n",
        "for question in test_questions:\n",
        "    q = {\"question\": question}\n",
        "    s_response = sparse_rag.invoke(q)\n",
        "    d_response = semantic_rag.invoke(q)\n",
        "\n",
        "    results[\"sparse\"][\"answers\"].append(s_response[\"response\"])\n",
        "    results[\"sparse\"][\"contexts\"].append([context.page_content for context in s_response[\"context\"]])\n",
        "    results[\"dense\"][\"answers\"].append(d_response[\"response\"])\n",
        "    results[\"dense\"][\"contexts\"].append([context.page_content for context in d_response[\"context\"]])\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "sparse_response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : results[\"sparse\"][\"answers\"],\n",
        "    \"contexts\" : results[\"sparse\"][\"contexts\"],\n",
        "    \"ground_truth\" : test_ground_truths\n",
        "})\n",
        "dense_response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : results[\"dense\"][\"answers\"],\n",
        "    \"contexts\" : results[\"dense\"][\"contexts\"],\n",
        "    \"ground_truth\" : test_ground_truths\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHHmIKiV2bBd"
      },
      "outputs": [],
      "source": [
        "# Load ragas evaluation metrics. We would use all possible metrics, including:\n",
        "# - Faithfulness\n",
        "# - Answer relevancy\n",
        "# - Answer correctness\n",
        "# - retrieved context related metrics\n",
        "\n",
        "metrics = [\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gn_stOM2bBd"
      },
      "outputs": [],
      "source": [
        "# Use ragas evaluator to report the score of each pipeline, using the metrics defined above.\n",
        "\n",
        "sparse_scores =\n",
        "dense_scores =\n",
        "\n",
        "print(f\"Sparse RAG Score: {sparse_scores}\")\n",
        "print(f\"Dense RAG Score: {dense_scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p1Zsbbv2bBe"
      },
      "outputs": [],
      "source": [
        "sparse_scores.to_pandas().head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkrM-3XC2bBe"
      },
      "outputs": [],
      "source": [
        "dense_scores.to_pandas().head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGTkCkXR2bBe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_sparse = pd.DataFrame(list(sparse_scores.items()), columns=['Metric', 'Sparse Retriever'])\n",
        "df_dense = pd.DataFrame(list(dense_scores.items()), columns=['Metric', 'Dense Retriever'])\n",
        "\n",
        "df_merged = pd.merge(df_sparse, df_dense, on='Metric')\n",
        "\n",
        "df_merged['Delta'] = df_merged['Dense Retriever'] - df_merged['Sparse Retriever']\n",
        "\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgCuLLiP2bBe"
      },
      "source": [
        "#### Report 3: (1 point)\n",
        "\n",
        "Compare the automated evaluation (using ragas) with your manual evaluation from the previous step. In your report, make sure to address the following:\n",
        "1. How are the two evaluation methods different? Briefly describe what makes the automated evaluation distinct from your manual judgment process (e.g., consistency, objectivity, criteria used).\n",
        "2.\tDo both evaluations show the same results? Were the rankings or judgments about the quality of responses consistent between your analysis and the automated scores?\n",
        "3.\tIf there were differences, why might that be? Reflect on what factors could lead to different results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOwTO_Xn6Zoe"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSIS7xwI2bBe"
      },
      "source": [
        "#### Question 8: (2 points)\n",
        "\n",
        "1. Explain the underlying mechanism and techniques used by `ragas` to evaluate the performance of RAG pipelines. How does it ensure that the evaluation is both comprehensive and relevant to the task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh_mRj8-6Zoe"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrgGWbOG2bBe"
      },
      "source": [
        "### 3.6 (Optional) Other strategy: (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12LXj99X2bBe"
      },
      "source": [
        "#### Question 9: (3 points)\n",
        "\n",
        "There are other retriever strategies you can use to improve the performance of your RAG pipeline. In this task:\n",
        "1. Explain what the `MultiQueryRetriever` does and how it can help improve retrieval quality in your pipeline.\n",
        "2. Implement the `MultiQueryRetriever` in your RAG pipeline and evaluate its performance using both manual and automated methods.\n",
        "3. You may also make additional improvements to your pipeline. If you do so, briefly explain what changes you made, how they affect the system, and why they might improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHg0DwLO6Zoe"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUy0-3vC6Zof"
      },
      "source": [
        "## 4. Read more: (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPUsHZh56Zof"
      },
      "source": [
        "#### Cache-Augmented Generation (CAG): (4 points)\n",
        "\n",
        "1. What is Cache-Augmented Generation (CAG)? How does it improve efficiency or performance during generation?\n",
        "2. What are the similarities and differences between Cache-Augmented Generation (CAG) and Retrieval-Augmented Generation (RAG)? In what scenarios might you prefer one over the other?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efeiIAsc6Zof"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuZxBibl6Zof"
      },
      "source": [
        "#### Multi-modal RAG: (6 points)\n",
        "\n",
        "1. How do models like CLIP enable the embedding of both text and images into a shared vector space? What are the advantages and disadvantages of using a unified embedding space for cross-modal retrieval in RAG systems.\n",
        "2. In systems like [Colpali](https://arxiv.org/pdf/2407.01449), how does dividing document images into patches enhance the retrieval process in multimodal RAG? Explore how patch-based processing preserves structural information and its impact on retrieval accuracy.\n",
        "3. What are the implications of converting non-text modalities (e.g., images) into textual representations for retrieval purposes? Discuss the benefits and drawbacks of grounding all modalities into a primary modality, such as text, in the context of RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiGRfYMs6Zof"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TuwfOPincl-1",
        "OiB9LEjb8r0f",
        "FN7dSqY-NNel",
        "QvmpQ-BtNNem"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c64ffd50c78429aa7da2ec51dd9797a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9697c587102c44cfbcc60ee59f223a6f",
              "IPY_MODEL_0b8ef95054cf4732bc2f2d1f1b659207",
              "IPY_MODEL_96c957caddb34d3e99ecf95ffcdac8e4"
            ],
            "layout": "IPY_MODEL_9e61680b7c194192a5b0b0ab793b6f25"
          }
        },
        "9697c587102c44cfbcc60ee59f223a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b2318a1b1444640969a4ef289d6fa5f",
            "placeholder": "​",
            "style": "IPY_MODEL_23c54afe53d54e8ab104db39611ee5b9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0b8ef95054cf4732bc2f2d1f1b659207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04e4c8a5fbfc484ab737365f57784c64",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa54c6c70d7d41148e86799e9bd833f1",
            "value": 2
          }
        },
        "96c957caddb34d3e99ecf95ffcdac8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_546ed3fa25314ee8aa5aaee532c57ab9",
            "placeholder": "​",
            "style": "IPY_MODEL_32c6e2932428430883580809138126e1",
            "value": " 2/2 [00:41&lt;00:00, 19.76s/it]"
          }
        },
        "9e61680b7c194192a5b0b0ab793b6f25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2318a1b1444640969a4ef289d6fa5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c54afe53d54e8ab104db39611ee5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04e4c8a5fbfc484ab737365f57784c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa54c6c70d7d41148e86799e9bd833f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "546ed3fa25314ee8aa5aaee532c57ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c6e2932428430883580809138126e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}