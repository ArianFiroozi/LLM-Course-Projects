{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFsETkP_6ZoL"
      },
      "source": [
        "# **CA3-Part2, LLMs Spring 2025**\n",
        "\n",
        "- **Name:**\n",
        "- **Student ID:**\n",
        "\n",
        "---\n",
        "#### Your submission should be named using the following format: `CA3-Part2_LASTNAME_STUDENTID.ipynb`.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks under each `Completion` section.\n",
        "\n",
        "- For text-based answers, you should replace the text that says `WRITE YOUR ANSWER HERE` with your actual answer, or you can look for `Report` and `Question` blocks.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---\n",
        "\n",
        "If you have any further questions or concerns, contact the TAs via email or Telegram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zivgc23P8r0f"
      },
      "source": [
        "# RAG (50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7koJfgPBRlNz"
      },
      "source": [
        "If you have any further questions or concerns, contact the TA via email (pouya.sadeghi@ut.ac.ir) or telegram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNHplrzc8r0f"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iX-Lm3M58r0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05904b13-8601-4515-e0a1-a413f6552510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_community langchain_huggingface huggingface_hub\n",
        "!pip install -q sentence_transformers tiktoken lark datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SalrEJJrutW",
        "outputId": "6f58ed09-0627-420e-f94c-9d9eba62aede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May  6 11:53:41 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3MhNsfsNVAVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f3a64db-95d9-478b-8632-fc1053dc8939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# To determine your system's CUDA version, run the following command:\n",
        "# !nvidia-smi\n",
        "\n",
        "# Based on your CUDA version, install the appropriate FAISS-GPU package:\n",
        "\n",
        "# For CUDA 12.x:\n",
        "!pip install -q faiss-gpu-cu12\n",
        "\n",
        "# For CUDA 11.x:\n",
        "# !pip install faiss-gpu-cu11\n",
        "\n",
        "# If you prefer the CPU-only version of FAISS:\n",
        "# !pip install -q faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuwfOPincl-1"
      },
      "source": [
        "## 1. An Overview of Information Retrieval (IR) and RAG (2 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHWLyGsKc59P"
      },
      "source": [
        "- **Information Retrieval (IR)**: The process of obtaining information system resources relevant to a specific information need from a collection of those resources. Each IR system consists of a collection of documents, a set of queries, and a retrieval function that ranks the documents based on their relevance to the query.\n",
        "- **Retrieval-Augmented Generation (RAG)**: A model that combines the strengths of retrieval-based and generation-based approaches. It retrieves relevant documents from a large corpus and uses them to generate a response to a query. RAG is particularly useful for tasks where the answer is not explicitly present in the training data but can be inferred from related documents.\n",
        "- **RAG Architecture**: The RAG architecture consists of two main components:\n",
        "  - **Retriever**: This component retrieves relevant documents from a large corpus based on the input query. It can be implemented using various retrieval methods, such as BM25 or dense retrieval.\n",
        "  - **Generator**: This component generates a response based on the retrieved documents and the input query. It can be implemented using transformer-based models.\n",
        "  \n",
        "In this computer assignment, you will implement a RAG pipeline using the LangChain framework. You will use two different retrievers: TF-IDF and dense retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0wV_2SCcl-2"
      },
      "source": [
        "#### Question 1: (2 points)\n",
        "1. Why do we need to use RAG?\n",
        "2. What is LangChain and how does it help in building RAG pipelines?\n",
        "<!-- 2. What are the advantages of RAG over generation methods? -->\n",
        "<!-- 3. What is the difference between dense and sparse retrievers? -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0zBJI6O6ZoN"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiB9LEjb8r0f"
      },
      "source": [
        "## 2. An Overview of LangChain (12 points + 2)\n",
        "\n",
        "In this overview, we will provide a step-by-step guide on how to construct a basic application using LangChain. To learn more about this framework, check its [tutorial](https://python.langchain.com/docs/tutorials/) which is available for different releases!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPaEdN5E8r0g"
      },
      "source": [
        "### 2.1 Lets load our model (4 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRahQhR_cl-2"
      },
      "source": [
        "#### Question 2: (2 points)\n",
        "\n",
        "1. Explain how different parameters, such as `temperature`, `max_length`, `top_p`, `top_k`, and `repetition_penalty`, affect the generation process in a language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8uW1zlt6ZoO"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKqpgVSmcl-2"
      },
      "source": [
        "#### Completion 1: (2 points)\n",
        "\n",
        "Load the `microsoft/Phi-4-mini-instruct` model and its tokenizer, and create a `text-generation` pipeline. Use the LangChain framework to integrate the model into your application. You should configure the pipeline with appropriate parameters, such as *max_new_tokens*, *temperature*, *top_p*, *top_k*, and *repetition_penalty*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64-XOX3M8r0g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3c64ffd50c78429aa7da2ec51dd9797a",
            "9697c587102c44cfbcc60ee59f223a6f",
            "0b8ef95054cf4732bc2f2d1f1b659207",
            "96c957caddb34d3e99ecf95ffcdac8e4",
            "9e61680b7c194192a5b0b0ab793b6f25",
            "6b2318a1b1444640969a4ef289d6fa5f",
            "23c54afe53d54e8ab104db39611ee5b9",
            "04e4c8a5fbfc484ab737365f57784c64",
            "fa54c6c70d7d41148e86799e9bd833f1",
            "546ed3fa25314ee8aa5aaee532c57ab9",
            "32c6e2932428430883580809138126e1"
          ]
        },
        "outputId": "57546c5d-f8b1-4573-ab0b-8586ee2f5be3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c64ffd50c78429aa7da2ec51dd9797a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
        "DEVICE = \"cuda:0\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=DEVICE,\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens= 50)\n",
        "\n",
        "# Load the pipeline into LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCphndr0uyDf",
        "outputId": "b6101874-d6be-4ddd-bfdc-862f1ac7f45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAxNYgBGcl-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87bd6e92-5017-4b1e-a27b-6865337113b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is the president of the United States? The president of the United States is Joe Biden. He assumed office on January 20, 2021, after winning the 2020 presidential election.\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Who is the president of the United States?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joCaX8tY8r0g"
      },
      "source": [
        "### 2.2 Simple Chain (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdkMm3Mz8r0h"
      },
      "source": [
        "#### Completion 2: (2 points)\n",
        "\n",
        "Complete the next cell to create a simple chain that takes the name of a football (soccer) player as input and outputs some information about that person. To do so:\n",
        "\n",
        "1. Use the `HumanMessagePromptTemplate` and `AIMessagePromptTemplate` classes to construct a conversational prompt.\n",
        "2. Use `ChatPromptTemplate` to organize the messages.\n",
        "3. Pass the prompt into the model you have loaded before.\n",
        "4. Use `StrOutputParser` to return a plain string.\n",
        "\n",
        "Your final chain should take a dictionary with a **person_name** key and return a brief description about that player."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMEB4D4z8r0h"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# Create a simple prompt template with a human message and an AI message\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "  HumanMessagePromptTemplate.from_template(\"Give me a brief explanation of {person_name} in soccer.\"),\n",
        "  AIMessagePromptTemplate.from_template(\"{person_name} is\"),\n",
        "])\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Create a simple chain with the prompt, LLM, and output parser. The goal is to generate a response to the prompt and parse the output as a string.\n",
        "simple_chain = (\n",
        "    prompt |\n",
        "    llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGGAq5Atcl-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33bb4114-fb1a-445c-cc24-e1f5cfef36aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Give me a brief explanation of Kylian Mbappé in soccer.\n",
            "AI: Kylian Mbappé is a professional French footballer who plays as a forward. He is known for his speed, agility, and goal-scoring ability. Mbappé has been a key player for Paris Saint-Germain (PSG) in France and has also represented the French\n"
          ]
        }
      ],
      "source": [
        "answer = simple_chain.invoke({\"person_name\": \"Kylian Mbappé\"})\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq8wDeKK8r0h"
      },
      "source": [
        "#### Question 3: (2 points)\n",
        "\n",
        "1. Write about the objectives behind the creation of `HumanMessagePromptTemplate` and `AIMessagePromptTemplate` classes. What they actually do and how should we use them? Write a brief description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2syj8rxJ6ZoP"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoN6LFaW8r0h"
      },
      "source": [
        "### 2.3 JSON Chain (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F56RIBhk8r0i"
      },
      "source": [
        "#### Completion 3: (1 point)\n",
        "\n",
        "Now we want to improve the chain to extract data from the model response. Modify the existing prompt to request information about a football player, such as:\n",
        "- full name\n",
        "- nationality\n",
        "- age\n",
        "- current club\n",
        "- position\n",
        "\n",
        "In this chain, you can use `SystemMessagePromptTemplate` as well.\n",
        "At the end, use `JsonOutputParser` to parse the model's output and return a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztIkrpmk8r0i"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Create the prompt template.\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"You are an expert in football, and you give statistics about players like full name, nationality, age, current club and position. format your answers as json.\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"Give me a brief explanation of {player_name} in soccer.\"),\n",
        "    AIMessagePromptTemplate.from_template(\"{{\")\n",
        "])\n",
        "\n",
        "# Use JsonOutputParser to parse the response as a dictionary\n",
        "output_parser = JsonOutputParser()\n",
        "\n",
        "get_ai_resp = lambda x: x.split(\"AI:\")[-1]\n",
        "\n",
        "# Define the chain. This time, we want output to be parsed as JSON, using the JsonOutputParser.\n",
        "json_chain =prompt|llm|get_ai_resp|output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVy5yaAJi3NU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25486c2d-6d54-40f8-ad71-b156c7d59c00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'full_name': 'Lionel Andrés Messi',\n",
              " 'nationality': 'Argentine',\n",
              " 'age': 35,\n",
              " 'current_club': 'Paris Saint-Germain',\n",
              " 'position': 'Forward'}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "json_chain.invoke({\"player_name\": \"Lionel Messi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xMWli_Ocl-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5da4d7-6271-4f34-e73d-74fc2a4697bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lionel Messi:\n",
            "  full_name: Lionel Andrés Messi\n",
            "  nationality: Argentine\n",
            "  age: 35\n",
            "  current_club: Paris Saint-Germain\n",
            "  position: Forward\n",
            "\n",
            "Cristiano Ronaldo:\n",
            "  full_name: Cristiano Ronaldo dos Santos Aveiro\n",
            "  nationality: Portuguese\n",
            "  age: 37\n",
            "  current_club: Al Nassr\n",
            "  position: Forward\n",
            "\n",
            "Kylian Mbappé:\n",
            "  full_name: Kylian Mbappé\n",
            "  nationality: French\n",
            "  age: 22\n",
            "  current_club: Paris Saint-Germain\n",
            "  position: Forward\n",
            "\n",
            "Neymar:\n",
            "  full_name: Neymar Jr.\n",
            "  nationality: Brazilian\n",
            "  age: 32\n",
            "  current_club: Paris Saint-Germain\n",
            "  position: Forward\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Batch the requests for multiple\n",
        "batch_questions = [\n",
        "  {\"player_name\": \"Lionel Messi\"},\n",
        "  {\"player_name\": \"Cristiano Ronaldo\"},\n",
        "  {\"player_name\": \"Kylian Mbappé\"},\n",
        "  {\"player_name\": \"Neymar\"}\n",
        "]\n",
        "answers = json_chain.batch(batch_questions)\n",
        "\n",
        "# Print the extracted information\n",
        "for (q, a) in zip(batch_questions, answers):\n",
        "  print(f\"{q['player_name']}:\")\n",
        "  for key, value in a.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ0ogQiMcl-4"
      },
      "source": [
        "#### Report 1: (1 point)\n",
        "\n",
        "Explain the challenges you faced in this step. How did you manage to solve them? How could the parameters you used in the text generation pipeline affect the model’s output?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uowWBQc6ZoP"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_3x5rCT6ZoP"
      },
      "source": [
        "#### Question 4: (2 points)\n",
        "\n",
        "1. How sampling parameters such as *temperature*, *top_p*, and *top_k* can affect our JSON pipeline? Answer the question with respect to the format and content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mQ6qjFu6ZoP"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RF_aXt6dC9g"
      },
      "source": [
        "### 2.4 (Optional) Tool use: Web search (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjkcoqElcl-4"
      },
      "source": [
        "#### Completion 4: (2 points)\n",
        "\n",
        "Add context about each player by using web search. For this purpose, you need to use a web search tool, and write a function to check the output of the search tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE-1-fzpcl-4"
      },
      "outputs": [],
      "source": [
        "# Code here. Note that you may need to install additional packages for web search tool.\n",
        "\n",
        "\n",
        "\n",
        "# Define the chain here\n",
        "footballer_chain = (\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UhjgFJTcl-4"
      },
      "outputs": [],
      "source": [
        "# Batch the requests for multiple\n",
        "batch_questions = [\n",
        "  {\"player_name\": \"Lionel Messi\"},\n",
        "  {\"player_name\": \"Cristiano Ronaldo\"},\n",
        "  {\"player_name\": \"Kylian Mbappé\"},\n",
        "  {\"player_name\": \"Neymar\"},\n",
        "  {\"player_name\": \"Declan Rice\"},\n",
        "  {\"player_name\": \"Trent Alexander-Arnold\"},\n",
        "  {\"player_name\": \"John Stones\"},\n",
        "  {\"player_name\": \"Alphonso Davies\"}\n",
        "]\n",
        "answers = footballer_chain.batch(batch_questions)\n",
        "\n",
        "# Print the extracted information\n",
        "for (q, a) in zip(batch_questions, answers):\n",
        "  print(f\"{q['player_name']}:\")\n",
        "  for key, value in a.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS8as30J8r0i"
      },
      "source": [
        "## 3. Build a RAG pipeline (26 points + 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aM-prY48r0i"
      },
      "source": [
        "In this section, We use a subset of [RecipeNLG](https://recipenlg.cs.put.poznan.pl) dataset to build our RAG pipelines. The dataset contains recipes and their corresponding instructions.\n",
        "\n",
        "You can download the subset from [this google drive link](https://drive.google.com/file/d/1mgPcQKc7-SaWVyxaJ404L6dGkQvODca5/view?usp=sharing) or from the course website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW2JuoTzNNek"
      },
      "source": [
        "### 3.1 Load and prepare the dataset (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGWVvgLqNNek"
      },
      "source": [
        "#### Completion 5: (4 point)\n",
        "\n",
        "First, you should load the dataset, which is stored in a CSV file. and converting it to a `datasets.Dataset` object.\n",
        "\n",
        "The dataset contains the following columns:\n",
        "- **title**: The name of the recipe\n",
        "- **ingredients**: A list of ingredients used in the recipe, including quantities and preparation methods\n",
        "- **directions**: The instructions for preparing the recipe, presented as a list of sequential steps\n",
        "- **NER**: A list of named entities representing the core food items and cooking components extracted from each recipe, without quantities or preparation instructions.\n",
        "\n",
        "**Attention**: You should carefully process list objects (ingredients, directions, and NER) and convert them to a string document.\n",
        "\n",
        "**Attention 2**: The provided dataset, has 5k recipes. You can use a smaller subset of the dataset for your experiments. For example, you can use the first 100 recipes for your experiments or more, based on your resource limitation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YIkZv62ZNNek"
      },
      "outputs": [],
      "source": [
        "!pip install -q gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6cVk_djLNNek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc2313a-5fd9-4e36-ad0b-785e49b3ddd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mgPcQKc7-SaWVyxaJ404L6dGkQvODca5\n",
            "To: /content/data_5000.csv\n",
            "\r  0% 0.00/5.92M [00:00<?, ?B/s]\r100% 5.92M/5.92M [00:00<00:00, 65.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1mgPcQKc7-SaWVyxaJ404L6dGkQvODca5 -O data_5000.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dmjlX1Go8r0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e9ffd0-b2ba-40d5-ced8-ef2bca6f99c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Unnamed: 0', 'title', 'ingredients', 'directions', 'NER'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Code here to load and process the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Store the datasets.Dataset object in the variable `dataset`\n",
        "dataset = load_dataset('csv', data_files=\"/content/data_5000.csv\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WnclCb3WZOA",
        "outputId": "2828b233-e3ee-4aa6-f1f1-d72e8bc4dd18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Unnamed: 0': 0,\n",
              " 'title': 'Worlds Best Mac and Cheese',\n",
              " 'ingredients': '[\"6 ounces penne\", \"2 cups Beechers Flagship Cheese Sauce (recipe follows)\", \"1 ounce Cheddar, grated (1/4 cup)\", \"1 ounce Gruyere cheese, grated (1/4 cup)\", \"1/4 to 1/2 teaspoon chipotle chili powder (see Note)\", \"1/4 cup (1/2 stick) unsalted butter\", \"1/3 cup all-purpose flour\", \"3 cups milk\", \"14 ounces semihard cheese (page 23), grated (about 3 1/2 cups)\", \"2 ounces semisoft cheese (page 23), grated (1/2 cup)\", \"1/2 teaspoon kosher salt\", \"1/4 to 1/2 teaspoon chipotle chili powder\", \"1/8 teaspoon garlic powder\", \"(makes about 4 cups)\"]',\n",
              " 'directions': '[\"Preheat the oven to 350 F. Butter or oil an 8-inch baking dish.\", \"Cook the penne 2 minutes less than package directions.\", \"(It will finish cooking in the oven.)\", \"Rinse the pasta in cold water and set aside.\", \"Combine the cooked pasta and the sauce in a medium bowl and mix carefully but thoroughly.\", \"Scrape the pasta into the prepared baking dish.\", \"Sprinkle the top with the cheeses and then the chili powder.\", \"Bake, uncovered, for 20 minutes.\", \"Let the mac and cheese sit for 5 minutes before serving.\", \"Melt the butter in a heavy-bottomed saucepan over medium heat and whisk in the flour.\", \"Continue whisking and cooking for 2 minutes.\", \"Slowly add the milk, whisking constantly.\", \"Cook until the sauce thickens, about 10 minutes, stirring frequently.\", \"Remove from the heat.\", \"Add the cheeses, salt, chili powder, and garlic powder.\", \"Stir until the cheese is melted and all ingredients are incorporated, about 3 minutes.\", \"Use immediately, or refrigerate for up to 3 days.\", \"This sauce reheats nicely on the stove in a saucepan over low heat.\", \"Stir frequently so the sauce doesnt scorch.\", \"This recipe can be assembled before baking and frozen for up to 3 monthsjust be sure to use a freezer-to-oven pan and increase the baking time to 50 minutes.\", \"One-half teaspoon of chipotle chili powder makes a spicy mac, so make sure your family and friends can handle it!\", \"The proportion of pasta to cheese sauce is crucial to the success of the dish.\", \"It will look like a lot of sauce for the pasta, but some of the liquid will be absorbed.\"]',\n",
              " 'NER': '[\"penne\", \"Beechers Flagship Cheese Sauce\", \"Cheddar\", \"Gruyere cheese\", \"chipotle chili powder\", \"butter\", \"flour\", \"milk\", \"semihard cheese\", \"semisoft cheese\", \"kosher salt\", \"chipotle chili powder\", \"garlic\"]'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WF2MjTYelRfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9df709f-f5e9-41f7-8a73-0c8ad91138b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 5000\n"
          ]
        }
      ],
      "source": [
        "# In this cell, you should store the dataset, as a list of `langchain_core.documents.Document` objects, which can simplify your future steps.\n",
        "# You should decide how to convert the dataset to documents\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents: list[Document] = [Document(\n",
        "        page_content=str(row),\n",
        "        metadata=dict(row)\n",
        "    )\n",
        "    for row in dataset['train']]\n",
        "\n",
        "\n",
        "print(f\"Number of documents: {len(documents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2bHNwp3sNNel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a00426f-d989-4bb8-9639-a834a817f1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 15194\n"
          ]
        }
      ],
      "source": [
        "# Now, you should use a splitter to divide long texts into smaller, manageable chunks so they can fit within the context window of language models or retrievers.\n",
        "# Use `RecursiveCharacterTextSplitter` to split the documents into smaller chunks, ans set the `chunk_size` and `chunk_overlap` parameters accordingly.\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,         # adjust based on your needs and model token limits\n",
        "    chunk_overlap=50        # overlap helps preserve context between chunks\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Number of chunks: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN7dSqY-NNel"
      },
      "source": [
        "### 3.2 Sparse Retriever (3 points)\n",
        "\n",
        "In this section, we would create a sparse retriever for our RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elzh6QSSNNem"
      },
      "source": [
        "#### Question 5: (2 points)\n",
        "\n",
        "1. Explain how a sparse retriever like TF-IDF represents documents and queries. How does this representation influence which documents are retrieved?\n",
        "2. Sparse retrievers rely on exact token matches between queries and documents. What are the strengths and weaknesses of this approach, especially compared to dense retrievers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrP8Kz1K6Zoa"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvmpQ-BtNNem"
      },
      "source": [
        "#### Completion 6: (1 point)\n",
        "\n",
        "Complete the code cells below to create a sparse retriever, which would be later used in our RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6Rq6DS9wNNem"
      },
      "outputs": [],
      "source": [
        "# Prepare your retriever. For this section, you should use a sparse retriever such as `TFIDF` or `BM25`.\n",
        "# We want our retriever to retrieve the first 3 chunks that are most relevant to the query.\n",
        "from langchain_community.retrievers import TFIDFRetriever\n",
        "\n",
        "sparse_retriever =TFIDFRetriever.from_documents(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7IoZQioNNem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced446c4-60af-4f65-d514-0eba141e82c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-16b24db22bc3>:11: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_chunks = sparse_retriever.get_relevant_documents(Sample_query)[:3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "Metadata: {'Unnamed: 0': 728, 'title': 'Pumpkin Cider Bread', 'ingredients': '[\"1 cup apple cider\", \"1 cup canned pumpkin puree\", \"2 large eggs\", \"1/4 cup vegetable oil\", \"3/4 cup firmly packed light brown sugar\", \"2 tablespoons freshly grated orange zest\", \"2 cups all-purpose flour\", \"2 teaspoons double-acting baking powder\", \"1/2 teaspoon salt\", \"1/4 teaspoon baking soda\", \"1/4 teaspoon ground mace\", \"1/4 teaspoon cinnamon\", \"1/8 teaspoon ground cloves\", \"1/2 cup chopped walnuts\"]', 'directions': '[\"In a saucepan boil the cider until it is reduced to about 1/4 cup and let it cool.\", \"In a bowl whisk together well the pumpkin puree, the eggs, the oil, the brown sugar, the zest, and the reduced cider.\", \"Into the bowl sift together the flour, the baking powder, the salt, the baking soda, the mace, the cinnamon, and the cloves, add the walnuts, and stir the batter until it is just combined.\", \"Transfer the batter to a well-buttered 8 1/2-by 4 1/2-inch loaf pan and bake the bread in the middle of a preheated 350F.\", \"oven for 1 hour, or until a tester comes out clean.\", \"Let the bread cool in the pan.\"]', 'NER': '[\"apple cider\", \"pumpkin puree\", \"eggs\", \"vegetable oil\", \"brown sugar\", \"orange zest\", \"flour\", \"double-acting baking powder\", \"salt\", \"baking soda\", \"ground mace\", \"cinnamon\", \"ground cloves\", \"walnuts\"]'}\n",
            "\n",
            "Chunk 2:\n",
            "Metadata: {'Unnamed: 0': 1339, 'title': 'Rich Flourless Chocolate Torte', 'ingredients': '[\"1 cup granulated sugar\", \"1 1/2 cups light corn syrup\", \"20 ounces goodquality semisweet chocolate, preferably French or Belgian cut into small pieces plus, 8 ounces additional\", \"chocolate\", \"2 sticks (16 tablespoons) unsalted butter\", \"10 whole eggs\", \"1/2 cup heavy cream\"]', 'directions': '[\"Preheat the oven to 350 degrees.\", \"In a heavy bottomed pot, combine the sugar and corn syrup and bring to a boil.\", \"Reduce the heat to low and cook for 5 minutes.\", \"Remove from the heat.\", \"Melt the 20 ounces chopped chocolate in the top of a double boiler over simmering water.\", \"Stir the chocolate occasionally as it melts.\", \"Remove the pan from the heat and stir in the butter.\", \"Set aside to cool slightly.\", \"With an electric mixer, beat the eggs at high speed until frothy.\", \"Lower the speed to medium and carefully pour the hot syrup into the beaten eggs.\", \"Add the melted chocolate and butter mixture and blend to a smooth consistency.\", \"Butter a 10 inch to 12inch round cake pan lightly and pour in the chocolate mixture.\", \"Set the cake pan into a larger pan (such as a roasting pan) and add hot water to rise halfway up the sides of the cake pan.\", \"Place in the preheated oven and bake about 45 to 50 minutes.\", \"Being very careful not to scald yourself or get water into the torte, remove the pans from the oven, and remove the cake pan from the water bath.\", \"Cool the torte completely to room temperature before unmolding.\", \"Run a sharp knife around the inner circumference of the pan.\", \"Place a plate over the pan, invert, and, if necessary, tap the bottom of the pan with the knife handle to encourage the torte to release.\", \"The torte should then be refrigerated for at least 3 hours before serving.\", \"While the torte is cooling, place the heavy cream in a thick bottomed pot and begin to heat gently.\", \"Add the remaining 8 ounces of chocolate, stir together while the chocolate begins to melt in the hot cream.\", \"Being careful not to burn, boil or scorch the chocolate, when the chocolate has melted almost entirely, remove from the heat and allow to cool for several minutes before pouring over the torte for a glossy \\\\\"finish\\\\\".\", \"Place the unmolded torte on a baking rack with a cookie sheet underneath, and pour slightly cooled chocolate ganache directly onto the center of the torte in one motion so that it will flow out in one smooth sheet over the top and sides of the torte.\", \"You can gently help this along with the aid of a spatula but is important to use still warmed ganache.\", \"Allow to dry a few moments at room temperature.\", \"Then place the torte, baking rack, and cookie sheet into the refrigerator to chill completely and set firm.\", \"If you haven\\'t overheated the chocolate or cream, this \\\\\"ganache\\\\\" will set and harden into a shiny \\\\\"couverture\\\\\" during the hour the torte is chilling in the refrigerator.\"]', 'NER': '[\"sugar\", \"light corn syrup\", \"goodquality semisweet chocolate\", \"chocolate\", \"butter\", \"eggs\", \"heavy cream\"]'}\n",
            "\n",
            "Chunk 3:\n",
            "Metadata: {'Unnamed: 0': 1592, 'title': \"Kunkhen's Torn Noodle Soup\", 'ingredients': '[\"2 cups all-purpose flour\", \"1/4 teaspoon sea salt\", \"3/4 cup water\", \"1 tablespoon peanut oil\", \"2 medium onions, thinly sliced\", \"2 cloves garlic, coarsely chopped\", \"1 2-inch length fresh ginger, peeled and coarsely chopped\", \"1 carrot, cut in half lengthwise, then cut in thin half-moon shapes\", \"2 ounces black radish, peeled and diced\", \"4 tomatoes, cored and coarsely chopped\", \"5 shiitake mushrooms, soaked in 1 cup hot water\", \"About 5 teaspoons fermented black beans, or to taste, (or 1 heaping tablespoon prepared black bean garlic sauce - see head note)\", \"8 cups water\", \"1 1/2 pounds fresh spinach, trimmed and rinsed\", \"2 heads butter lettuce, rinsed, leaves scissor cut in wide strips\", \"Sea salt to taste\", \"1 cup fresh cilantro leaves, firmly packed\", \"3 scallions, trimmed and minced\"]', 'directions': '[\"1.\", \"Place the flour in a medium bowl and make a well.\", \"Add the water and the salt and mix, then gradually mix in the flour.\", \"Turn out the dough onto a lightly floured surface and knead it until it is slightly elastic and smooth, about 5 minutes.\", \"Cover it with a bowl and let it sit.\", \"The noodle dough can be prepared up to 2 hours before making the soup.\", \"2.\", \"To make the soup, place the oil, onions and garlic in a large stockpot over medium heat and cook, stirring occasionally, until the onions are translucent, about 8 minutes.\", \"Add the ginger, the carrots, and the radish and stir, then add the tomatoes and stir.\", \"Cook, stirring, until the tomatoes have lost their shape, about 8 minutes.\", \"3.\", \"Drain the mushrooms, saving the liquid they soaked in, and cut them in quarters.\", \"Add the black beans, or the black bean puree to the vegetables in the stock pot, stir, then add the water and the mushroom soaking liquid.\", \"Stir, cover, and bring to a boil.\", \"Reduce the heat so the liquid is simmering and cook until the vegetables are tender, about 45 minutes.\", \"Add the spinach and the lettuces.\", \"You may need to add the greens gradually, pushing them down into the liquid as they wilt.\", \"Cook until all the greens are wilted.\", \"4.\", \"While the greens are cooking, divide the noodle dough in quarters.\", \"Roll the quarters into 1-inch thick ropes, then take one rope and holding it at the end between your thumbs and forefingers, gradually move along the rope of dough as you continue to pinch the dough with your fingers, so that you end up with a band of dough that is about 2-inches wide.\", \"Repeat the process on the same band of dough, then repeat with all the ropes of dough so that you have four, flat bands.\", \"They will be irregular, but that is fine.\", \"When the greens are thoroughly cooked, tear 1-inch pieces off the first band of dough and throw them into the soup.\", \"Stir, then continue with the remaining band of dough.\", \"The dough is fairly soft, but it tears quite well, resulting in noodles that are irregularly shaped, just as Kunkhen\\'s were.\", \"5.\", \"Cook the noodles just until they are tender, 2 to 3 minutes, stirring the soup so they cook evenly.\", \"Serve the soup immediately, with the garnishes alongside.\"]', 'NER': '[\"flour\", \"salt\", \"water\", \"peanut oil\", \"onions\", \"garlic\", \"ginger\", \"carrot\", \"black radish\", \"tomatoes\", \"shiitake mushrooms\", \"black beans\", \"water\", \"fresh spinach\", \"butter\", \"salt\", \"fresh cilantro\", \"scallions\"]'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query below is related to `Zucchini Nut Bread` recipe.\n",
        "\n",
        "Sample_query = \"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\"\n",
        "\n",
        "# Use the sparse retriever to get the most relevant chunks for the query\n",
        "retrieved_chunks = sparse_retriever.get_relevant_documents(Sample_query)[:3]\n",
        "\n",
        "# Now, see what chunks were retrieved\n",
        "for i, chunk in enumerate(retrieved_chunks, start=1):\n",
        "    print(f\"Chunk {i}:\")\n",
        "    # print(chunk.page_content)\n",
        "    print(\"Metadata:\", chunk.metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qnbeJnz8r0k"
      },
      "source": [
        "### 3.3 Semantic Retriever (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgEphnmn8r0k"
      },
      "source": [
        "#### Question 6: (2 point)\n",
        "\n",
        "1. How does a semantic retriever represent documents and queries differently from a sparse retriever? Explain why this helps in cases where there are no exact word overlaps.\n",
        "2. What role do embedding models (e.g., sentence-transformers) play in semantic retrieval? How does the choice of embedding model affect the retriever’s performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6u_rLg-6Zob"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1GCzoOXUf8c"
      },
      "source": [
        "#### Completion 7: (2 point)\n",
        "\n",
        "Let's create a semantic retriever. We would use `BAAI/bge-small-en` as our embedding model, and `FAISS` as our vector store. Complete the code cells below to create a semantic retriever, which would be later used in our RAG pipeline.\n",
        "\n",
        "As explained before, we want our retriever to retrieve the first 3 most relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V6NzUEAs8r0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33967ef2-72dd-4842-fede-774eba787c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-eb02adf2272e>:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Code here\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
        "semantic_retriever =vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5Uc59HkUf8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff62cdc7-8546-42e5-f548-c10902b68513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "Metadata: {'Unnamed: 0': 2363, 'title': 'Basil Zucchini', 'ingredients': '[\"1 teaspoon olive oil\", \"3 small zucchini, thinly sliced\", \"2 tablespoons chopped fresh basil\", \"1 garlic clove, minced\"]', 'directions': '[\"Heat oil in heavy medium nonstick skillet.\", \"Add zucchini, 1 tablespoon basil and garlic and stir-fry until zucchini is just tender, about 5 minutes.\", \"Season to taste with salt and pepper.\", \"Remove from heat.\", \"Sprinkle with remaining basil.\"]', 'NER': '[\"olive oil\", \"zucchini\", \"fresh basil\", \"garlic\"]'}\n",
            "\n",
            "Chunk 2:\n",
            "Metadata: {'Unnamed: 0': 229, 'title': 'Baked Zucchini', 'ingredients': '[\"1 teaspoon olive oil\", \"1 pound zucchini, sliced 1/4-inch thick, at an angle\", \"Kosher salt and freshly ground black pepper\", \"Pinch of Hungarian paprika\", \"1/2 cup panko breadcrumbs (Japanese)\", \"1/4 cup grated Parmesan cheese\", \"8 sprigs fresh thyme, leaves stripped from the stem, lightly chopped\", \"1 tablespoon olive oil\"]', 'directions': '[\"Heat the oven to 350 degrees F.\", \"Brush 1 teaspoon olive oil on the bottom of an 8 by 8-inch baking dish.\", \"Arrange the slices of zucchini in the dish with an overlapping pattern in rows or a spiral in a pie dish.\", \"Sprinkle with salt, pepper and paprika.\", \"To make the topping: In a bowl, stir together the panko breadcrumbs, thyme, Parmesan cheese and season with a sprinkle of salt and a few grinds of pepper.\", \"Add 1 tablespoon olive oil and stir until all the breadcrumbs are soaked with the yellow tint of the oil.\", \"Sprinkle the topping evenly over the dish and bake until the top is golden brown, 30 to 35 minutes.\"]', 'NER': '[\"olive oil\", \"zucchini\", \"Kosher salt\", \"paprika\", \"breadcrumbs\", \"Parmesan cheese\", \"thyme\", \"olive oil\"]'}\n",
            "\n",
            "Chunk 3:\n",
            "Metadata: {'Unnamed: 0': 1891, 'title': 'Zucchini Bread', 'ingredients': '[\"4 eggs, beaten\", \"2 cups sugar\", \"1 cup oil\", \"3 12 cups flour\", \"34 teaspoon baking powder\", \"1 12 teaspoons baking soda\", \"1 12 teaspoons salt\", \"2 cups grated zucchini\", \"3 teaspoons vanilla\", \"2 teaspoons cinnamon\", \"1 cup raisins (optional)\", \"1 cup nuts (optional)\"]', 'directions': '[\"Grease and flour 2 loaf pans.\", \"Mix eggs, sugar and oil together.\", \"Add grated zucchini and vanilla to egg mixture.\", \"Sift dry ingredients and add to wet ingredients.\", \"Add raisins and nuts.\", \"Mix well.\", \"Bake at 350 degrees for 1 hour or until toothpick inserted in center comes out clean.\"]', 'NER': '[\"eggs\", \"sugar\", \"oil\", \"flour\", \"baking powder\", \"baking soda\", \"salt\", \"zucchini\", \"vanilla\", \"cinnamon\", \"raisins\", \"nuts\"]'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query below is related to `Zucchini Nut Bread` recipe.\n",
        "\n",
        "Sample_query = \"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\"\n",
        "\n",
        "# Use the semantic retriever to get the most relevant chunks for the query\n",
        "retrieved_chunks = semantic_retriever.get_relevant_documents(Sample_query)[:3]\n",
        "\n",
        "# Now, see what chunks were retrieved\n",
        "for i, chunk in enumerate(retrieved_chunks, start=1):\n",
        "    print(f\"Chunk {i}:\")\n",
        "    # print(chunk.page_content)\n",
        "    print(\"Metadata:\", chunk.metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYbF75HP2bBc"
      },
      "source": [
        "### 3.4 Create RAG pipelines (6 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_KnISF32bBc"
      },
      "source": [
        "#### Question 7: (2 points)\n",
        "\n",
        "1. What are the main components of a RAG system, and how do they interact during inference? Describe the flow from user input to model output.\n",
        "2. Describe two different strategies for integrating retrieved context into the prompt. What are the trade-offs between these approaches?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwNj8aph6Zob"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avtuGMuT2bBc"
      },
      "source": [
        "#### Completion 8: (4 points)\n",
        "\n",
        "Follow the instructions below to build a RAG pipeline using the retrievers you created in the previous sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Dqo5m1La2bBc"
      },
      "outputs": [],
      "source": [
        "Sample_query = \"\"\"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\n",
        "\n",
        "What is your best guess about what am I cooking?\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xxybNSeT2bBc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "00e1a268d18d4720944e30acb39a30c6",
            "62352ebe50654885ad6a2b54bfe5d384",
            "526a317f0ffd4e29b9c11d5ba86567de",
            "61373864472a436bb919f1cd6dab74ba",
            "d7a48e886e854d28a9e6ef832e6af7d1",
            "e801ad2c2ee54e07a22ac0e73bb5ff57",
            "409bc8f7608349589dfb7930b5ddff16",
            "09c01e8b6e2249878b5f961552aa404b",
            "4d5bd78b7c6542748786ad3ec3fe2eb9",
            "761a4452048041039ffcd1a5fd7c672e",
            "280b9a3a032a40b9a6fccb238390570d"
          ]
        },
        "outputId": "2d923d89-982d-4004-cb4d-7fdc3b45f6c6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00e1a268d18d4720944e30acb39a30c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# We are going to use \"microsoft/Phi-4-mini-instruct\" as our LLM again. If you need, load it again here and as before.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "DEVICE = \"cuda:0\"\n",
        "\n",
        "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=DEVICE,\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256\n",
        ")\n",
        "\n",
        "llm =HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j-_gpU7u2bBc"
      },
      "outputs": [],
      "source": [
        "# First, we need to define a new chat template, that provide the retrieved documents as context to the LLM.\n",
        "from langchain_core.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "  SystemMessagePromptTemplate.from_template(\"You are an assistent that uses context information to answer questions. Answer the questions according to the following context.\\n\\nContext: {rag}\"),\n",
        "  HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
        "  AIMessagePromptTemplate.from_template(\"{response}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2YU0sdZa2bBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2f2e99-b4d4-4629-e4d5-f5ebd1a8f110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0b7fbf49fe44>:13: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  RunnableLambda(lambda x: {\"question\": x['question'], \"rag\": sparse_retriever.get_relevant_documents(x['question'])[:3], \"response\": \"\\n\"})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: You are an assistent that uses context information to answer questions. Answer the questions according to the following context.\n",
            "\n",
            "Context: [Document(metadata={'Unnamed: 0': 728, 'title': 'Pumpkin Cider Bread', 'ingredients': '[\"1 cup apple cider\", \"1 cup canned pumpkin puree\", \"2 large eggs\", \"1/4 cup vegetable oil\", \"3/4 cup firmly packed light brown sugar\", \"2 tablespoons freshly grated orange zest\", \"2 cups all-purpose flour\", \"2 teaspoons double-acting baking powder\", \"1/2 teaspoon salt\", \"1/4 teaspoon baking soda\", \"1/4 teaspoon ground mace\", \"1/4 teaspoon cinnamon\", \"1/8 teaspoon ground cloves\", \"1/2 cup chopped walnuts\"]', 'directions': '[\"In a saucepan boil the cider until it is reduced to about 1/4 cup and let it cool.\", \"In a bowl whisk together well the pumpkin puree, the eggs, the oil, the brown sugar, the zest, and the reduced cider.\", \"Into the bowl sift together the flour, the baking powder, the salt, the baking soda, the mace, the cinnamon, and the cloves, add the walnuts, and stir the batter until it is just combined.\", \"Transfer the batter to a well-buttered 8 1/2-by 4 1/2-inch loaf pan and bake the bread in the middle of a preheated 350F.\", \"oven for 1 hour, or until a tester comes out clean.\", \"Let the bread cool in the pan.\"]', 'NER': '[\"apple cider\", \"pumpkin puree\", \"eggs\", \"vegetable oil\", \"brown sugar\", \"orange zest\", \"flour\", \"double-acting baking powder\", \"salt\", \"baking soda\", \"ground mace\", \"cinnamon\", \"ground cloves\", \"walnuts\"]'}, page_content='\"1/2 cup chopped walnuts\"]\\', \\'directions\\': \\'[\"In a saucepan boil the cider until it is reduced to about 1/4 cup and let it cool.\", \"In a bowl whisk together well the pumpkin puree, the eggs, the oil, the brown sugar, the zest, and the reduced cider.\", \"Into the bowl sift together the flour, the baking powder, the salt, the baking soda, the mace, the cinnamon, and the cloves, add the walnuts, and stir the batter until it is just combined.\", \"Transfer the batter to a well-buttered 8 1/2-by 4'), Document(metadata={'Unnamed: 0': 1592, 'title': \"Kunkhen's Torn Noodle Soup\", 'ingredients': '[\"2 cups all-purpose flour\", \"1/4 teaspoon sea salt\", \"3/4 cup water\", \"1 tablespoon peanut oil\", \"2 medium onions, thinly sliced\", \"2 cloves garlic, coarsely chopped\", \"1 2-inch length fresh ginger, peeled and coarsely chopped\", \"1 carrot, cut in half lengthwise, then cut in thin half-moon shapes\", \"2 ounces black radish, peeled and diced\", \"4 tomatoes, cored and coarsely chopped\", \"5 shiitake mushrooms, soaked in 1 cup hot water\", \"About 5 teaspoons fermented black beans, or to taste, (or 1 heaping tablespoon prepared black bean garlic sauce - see head note)\", \"8 cups water\", \"1 1/2 pounds fresh spinach, trimmed and rinsed\", \"2 heads butter lettuce, rinsed, leaves scissor cut in wide strips\", \"Sea salt to taste\", \"1 cup fresh cilantro leaves, firmly packed\", \"3 scallions, trimmed and minced\"]', 'directions': '[\"1.\", \"Place the flour in a medium bowl and make a well.\", \"Add the water and the salt and mix, then gradually mix in the flour.\", \"Turn out the dough onto a lightly floured surface and knead it until it is slightly elastic and smooth, about 5 minutes.\", \"Cover it with a bowl and let it sit.\", \"The noodle dough can be prepared up to 2 hours before making the soup.\", \"2.\", \"To make the soup, place the oil, onions and garlic in a large stockpot over medium heat and cook, stirring occasionally, until the onions are translucent, about 8 minutes.\", \"Add the ginger, the carrots, and the radish and stir, then add the tomatoes and stir.\", \"Cook, stirring, until the tomatoes have lost their shape, about 8 minutes.\", \"3.\", \"Drain the mushrooms, saving the liquid they soaked in, and cut them in quarters.\", \"Add the black beans, or the black bean puree to the vegetables in the stock pot, stir, then add the water and the mushroom soaking liquid.\", \"Stir, cover, and bring to a boil.\", \"Reduce the heat so the liquid is simmering and cook until the vegetables are tender, about 45 minutes.\", \"Add the spinach and the lettuces.\", \"You may need to add the greens gradually, pushing them down into the liquid as they wilt.\", \"Cook until all the greens are wilted.\", \"4.\", \"While the greens are cooking, divide the noodle dough in quarters.\", \"Roll the quarters into 1-inch thick ropes, then take one rope and holding it at the end between your thumbs and forefingers, gradually move along the rope of dough as you continue to pinch the dough with your fingers, so that you end up with a band of dough that is about 2-inches wide.\", \"Repeat the process on the same band of dough, then repeat with all the ropes of dough so that you have four, flat bands.\", \"They will be irregular, but that is fine.\", \"When the greens are thoroughly cooked, tear 1-inch pieces off the first band of dough and throw them into the soup.\", \"Stir, then continue with the remaining band of dough.\", \"The dough is fairly soft, but it tears quite well, resulting in noodles that are irregularly shaped, just as Kunkhen\\'s were.\", \"5.\", \"Cook the noodles just until they are tender, 2 to 3 minutes, stirring the soup so they cook evenly.\", \"Serve the soup immediately, with the garnishes alongside.\"]', 'NER': '[\"flour\", \"salt\", \"water\", \"peanut oil\", \"onions\", \"garlic\", \"ginger\", \"carrot\", \"black radish\", \"tomatoes\", \"shiitake mushrooms\", \"black beans\", \"water\", \"fresh spinach\", \"butter\", \"salt\", \"fresh cilantro\", \"scallions\"]'}, page_content='\"Place the flour in a medium bowl and make a well.\", \"Add the water and the salt and mix, then gradually mix in the flour.\", \"Turn out the dough onto a lightly floured surface and knead it until it is slightly elastic and smooth, about 5 minutes.\", \"Cover it with a bowl and let it sit.\", \"The noodle dough can be prepared up to 2 hours before making the soup.\", \"2.\", \"To make the soup, place the oil, onions and garlic in a large stockpot over medium heat and cook, stirring occasionally, until'), Document(metadata={'Unnamed: 0': 3129, 'title': 'Brioche Bread with Butter & Egg Yolk', 'ingredients': '[\"125 grams Bread flour\", \"125 grams All purpose flour\", \"30 grams Sugar\", \"20 grams Trehalose\", \"3 Egg yolk\", \"80 grams Unsalted butter\", \"30 ml Heavy cream\", \"1 enough to make 160 ml when combined with egg yolk and cream Water\", \"4 grams Salt\", \"4 grams Instant dry yeast\", \"1 to glaze Beaten egg\"]', 'directions': '[\"Beat the egg yolk well and mix with the cream and water.\", \"Put all of the ingredients except for the butter in the bread maker and set it to the bread kneading course.\", \"Place the yeast according to the manufacturers instructions.\", \"Add 1/3 of the butter 3 minutes after starting the course, then another 1/3 after a further 3 minutes and then the last 1/3 after another 3 minutes.\", \"Leave everything in the bread maker until it\\'s finished proofing.\", \"Once proofed, take out the dough, and punch it down to get rid of the gas.\", \"Use a scraper to divide the dough into 12, moulding them into rounds.\", \"Each round should weigh around 45 g.\", \"The butter can cause things to get a bit sticky so feel free to give the dough a dusting of flour if it needs it.\", \"Place the dough seam-side down and cover with a slightly damp cloth for 10 minutes.\", \"Once the 10 minutes is up, turn each piece of dough seam up, press down and re-form into balls.\", \"Make a lump of the top by pressing down the top 1/4 of the dough with your little finger and rolling.\", \"Make the lump quite narrow and put the dough into aluminium cups.\", \"Push the lump down gently to flatten a little.\", \"The butter will begin to melt here, so do this step as quickly as possible.\", \"Proof for a second time in the oven for 35 - 45 minutes at 35C.\", \"When the dough has expanded 1.5x the original size it\\'s ready.\", \"The picture shows the dough after the second proofing.\", \"Once proved, preheat the oven to 200C.\", \"whilst the oven is heating up, glaze the dough with a mixture of water and beaten egg.\", \"Bake for 12 minutes at 200C.\", \"Please adjust the cooking time according to your oven.\", \"Once a nice golden colour, they\\'re ready.\", \"This is how they look inside when baked.\", \"The inside is really chewy, and a nice yellow color, and the smell is fantastic.\"]', 'NER': '[\"Bread flour\", \"flour\", \"Sugar\", \"Trehalose\", \"Egg yolk\", \"butter\", \"cream\", \"enough\", \"Salt\", \"yeast\", \"egg\"]'}, page_content='\"The picture shows the dough after the second proofing.\", \"Once proved, preheat the oven to 200C.\", \"whilst the oven is heating up, glaze the dough with a mixture of water and beaten egg.\", \"Bake for 12 minutes at 200C.\", \"Please adjust the cooking time according to your oven.\", \"Once a nice golden colour, they\\\\\\'re ready.\", \"This is how they look inside when baked.\", \"The inside is really chewy, and a nice yellow color, and the smell is fantastic.\"]\\', \\'NER\\': \\'[\"Bread flour\", \"flour\", \"Sugar\",')]\n",
            "Human: The kitchen smells warm and sweet already. I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. it’s turned into a thick, glossy batter, smooth and golden. I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. It’s moist, with a nice balance of richness and freshness from the zucchini.\n",
            "\n",
            "What is your best guess about what am I cooking?\n",
            "AI: \n",
            "Based on the context provided, it seems you are preparing a type of bread or cake that incorporates zucchini into the batter. The description of the batter being thick, glossy, and with a green-flecked look from the grated zucchini suggests that you might be making a zucchini bread or a zucchini cake. The addition of sugar, vegetable oil, and vanilla also points towards a sweet baked good, which is common in recipes for zucchini bread or cake. The mention of a \"richness and freshness\" from the zucchini further supports the idea that this is a moist, flavorful baked dish.\n"
          ]
        }
      ],
      "source": [
        "# Now, let's create a simple RAG pipeline, using the sparse retriever. Note that we need the retrieved context as part of the output, so that we can later use it for evaluation.\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "sparse_rag = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : (input)  populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : (output) chunks retrieved by the sparse retriever, based on the \"question\" value\n",
        "    # \"response\" : (output) the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #                       into the LLM and stored in a key called \"response\"\n",
        "    RunnableLambda(lambda x: {\"question\": x['question'], \"rag\": sparse_retriever.get_relevant_documents(x['question'])[:3], \"response\": \"\\n\"})\n",
        "    | prompt\n",
        "    | llm\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "# Now, let's test the sparse RAG pipeline with a sample query.\n",
        "response = sparse_rag.invoke({\"question\": Sample_query})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xRy88ZMi2bBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f46f6d-854a-4c79-e12d-5a4b5b2ffc89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: You are an assistent that uses context information to answer questions. Answer the questions according to the following context.\n",
            "\n",
            "Context: [Document(id='e7454554-bc03-444a-87aa-765a75c07a77', metadata={'Unnamed: 0': 229, 'title': 'Baked Zucchini', 'ingredients': '[\"1 teaspoon olive oil\", \"1 pound zucchini, sliced 1/4-inch thick, at an angle\", \"Kosher salt and freshly ground black pepper\", \"Pinch of Hungarian paprika\", \"1/2 cup panko breadcrumbs (Japanese)\", \"1/4 cup grated Parmesan cheese\", \"8 sprigs fresh thyme, leaves stripped from the stem, lightly chopped\", \"1 tablespoon olive oil\"]', 'directions': '[\"Heat the oven to 350 degrees F.\", \"Brush 1 teaspoon olive oil on the bottom of an 8 by 8-inch baking dish.\", \"Arrange the slices of zucchini in the dish with an overlapping pattern in rows or a spiral in a pie dish.\", \"Sprinkle with salt, pepper and paprika.\", \"To make the topping: In a bowl, stir together the panko breadcrumbs, thyme, Parmesan cheese and season with a sprinkle of salt and a few grinds of pepper.\", \"Add 1 tablespoon olive oil and stir until all the breadcrumbs are soaked with the yellow tint of the oil.\", \"Sprinkle the topping evenly over the dish and bake until the top is golden brown, 30 to 35 minutes.\"]', 'NER': '[\"olive oil\", \"zucchini\", \"Kosher salt\", \"paprika\", \"breadcrumbs\", \"Parmesan cheese\", \"thyme\", \"olive oil\"]'}, page_content='{\\'Unnamed: 0\\': 229, \\'title\\': \\'Baked Zucchini\\', \\'ingredients\\': \\'[\"1 teaspoon olive oil\", \"1 pound zucchini, sliced 1/4-inch thick, at an angle\", \"Kosher salt and freshly ground black pepper\", \"Pinch of Hungarian paprika\", \"1/2 cup panko breadcrumbs (Japanese)\", \"1/4 cup grated Parmesan cheese\", \"8 sprigs fresh thyme, leaves stripped from the stem, lightly chopped\", \"1 tablespoon olive oil\"]\\', \\'directions\\': \\'[\"Heat the oven to 350 degrees F.\", \"Brush 1 teaspoon olive oil on the bottom of an 8 by'), Document(id='42cfd1f1-d3ff-464e-830d-961ac8cd6513', metadata={'Unnamed: 0': 3030, 'title': 'Golly Gee This is Good Zucchini Bread', 'ingredients': '[\"3 eggs\", \"2 cups sugar\", \"1 cup vegetable oil\", \"2 cups grated zucchini\", \"3 teaspoons vanilla\", \"3 cups flour\", \"12 teaspoon salt\", \"34 teaspoon baking soda\", \"1 teaspoon baking powder\", \"1 teaspoon cinnamon\", \"14 teaspoon nutmeg\", \"12 cup chopped nuts, preferably walnuts, if desired\", \"1 12 cups raisins, if desired (golden or red raisins)\"]', 'directions': '[\"Beat eggs and sugar until well blended.\", \"Add oil, zucchini, and vanilla.\", \"In a separate bowl, mix dry ingredients together and add gradually to zucchini mixture.\", \"Bake for one hour in a greased loaf pans, at 325 degrees.\"]', 'NER': '[\"eggs\", \"sugar\", \"vegetable oil\", \"zucchini\", \"vanilla\", \"flour\", \"salt\", \"baking soda\", \"baking powder\", \"cinnamon\", \"nutmeg\", \"nuts\", \"raisins\"]'}, page_content='{\\'Unnamed: 0\\': 3030, \\'title\\': \\'Golly Gee This is Good Zucchini Bread\\', \\'ingredients\\': \\'[\"3 eggs\", \"2 cups sugar\", \"1 cup vegetable oil\", \"2 cups grated zucchini\", \"3 teaspoons vanilla\", \"3 cups flour\", \"12 teaspoon salt\", \"34 teaspoon baking soda\", \"1 teaspoon baking powder\", \"1 teaspoon cinnamon\", \"14 teaspoon nutmeg\", \"12 cup chopped nuts, preferably walnuts, if desired\", \"1 12 cups raisins, if desired (golden or red raisins)\"]\\', \\'directions\\': \\'[\"Beat eggs and sugar until well blended.\", \"Add'), Document(id='1f4e4b11-4f46-4eda-804a-3307986fed54', metadata={'Unnamed: 0': 2363, 'title': 'Basil Zucchini', 'ingredients': '[\"1 teaspoon olive oil\", \"3 small zucchini, thinly sliced\", \"2 tablespoons chopped fresh basil\", \"1 garlic clove, minced\"]', 'directions': '[\"Heat oil in heavy medium nonstick skillet.\", \"Add zucchini, 1 tablespoon basil and garlic and stir-fry until zucchini is just tender, about 5 minutes.\", \"Season to taste with salt and pepper.\", \"Remove from heat.\", \"Sprinkle with remaining basil.\"]', 'NER': '[\"olive oil\", \"zucchini\", \"fresh basil\", \"garlic\"]'}, page_content='{\\'Unnamed: 0\\': 2363, \\'title\\': \\'Basil Zucchini\\', \\'ingredients\\': \\'[\"1 teaspoon olive oil\", \"3 small zucchini, thinly sliced\", \"2 tablespoons chopped fresh basil\", \"1 garlic clove, minced\"]\\', \\'directions\\': \\'[\"Heat oil in heavy medium nonstick skillet.\", \"Add zucchini, 1 tablespoon basil and garlic and stir-fry until zucchini is just tender, about 5 minutes.\", \"Season to taste with salt and pepper.\", \"Remove from heat.\", \"Sprinkle with remaining basil.\"]\\', \\'NER\\': \\'[\"olive oil\", \"zucchini\", \"fresh')]\n",
            "Human: The kitchen smells warm and sweet already. I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. it’s turned into a thick, glossy batter, smooth and golden. I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. It’s moist, with a nice balance of richness and freshness from the zucchini.\n",
            "\n",
            "What is your best guess about what am I cooking?\n",
            "AI: \n",
            "Based on the context provided, it seems you are cooking a zucchini bread. The ingredients and directions you've described, such as the use of eggs, sugar, vegetable oil, vanilla, flour, salt, baking soda, baking powder, cinnamon, nutmeg, and optional nuts and raisins, are typical for a zucchini bread recipe. The process of mixing these ingredients and the description of the batter's texture and appearance also align with the preparation of zucchini bread.\n"
          ]
        }
      ],
      "source": [
        "# For this cell, everything is the same as the previous cell, except that we are using the semantic retriever instead of the sparse retriever.\n",
        "\n",
        "semantic_rag = (\n",
        "    # The same as previous cell, but using the semantic retriever instead of the sparse retriever\n",
        "    RunnableLambda(lambda x: {\"question\": x['question'], \"rag\": semantic_retriever.get_relevant_documents(x['question'])[:3], \"response\": \"\\n\"})\n",
        "    | prompt\n",
        "    | llm\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "# Now, let's test the semantic RAG pipeline with a sample query.\n",
        "response = semantic_rag.invoke({\"question\": Sample_query})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RvYodfcg2bBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7595141b-e15b-4e4f-a911-7e8a9e2bf883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The kitchen smells warm and sweet already. I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. it’s turned into a thick, glossy batter, smooth and golden. I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. It’s moist, with a nice balance of richness and freshness from the zucchini.\n",
            "\n",
            "What is your best guess about what am I cooking? I’m guessing a zucchini bread, but I’m not sure. I’ve never made it before, and I’m not sure if I have all the ingredients. I have flour, sugar, eggs, oil, and vanilla, but I don’t have any zucchini. I also don’t have any baking powder or baking soda, which I’ve read are necessary for a good rise. I’m not sure if I can just add some baking soda to the mix, or if I need to find a way to incorporate the zucchini without it. I’m also not sure if I need to preheat the oven, or if I can just put the batter in the oven and let it cook. I’m not sure if I need to grease the pan, or if the batter will stick to the pan. I’m also not sure if I need to add any other ingredients, like nuts or chocolate chips, to make it more interesting. I’m not sure if I should just go with the recipe I found, or if I should try to make it my own. I’m not sure if I should just go with the recipe I found, or if I should try to make it my own. I’m not sure if I should just go with the recipe I found, or if I should\n"
          ]
        }
      ],
      "source": [
        "# Finally, let's try the same query with the LLM directly, without any retrieval.\n",
        "# response = model.generate(tokenizer(Sample_query, return_tensors=\"pt\").input_ids.to(DEVICE))\n",
        "# response = tokenizer.decode(response[0]).replace(Sample_query, \"\")\n",
        "response = llm.invoke(Sample_query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mCOh9_i2bBd"
      },
      "source": [
        "### 3.5 Evaluate our pipelines (9 points)\n",
        "\n",
        "In this section, we are going to evaluate our RAG pipelines. First, we would design 5 queries to evaluate our RAG pipelines and our LLM alone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDgQ1uNE2bBd"
      },
      "source": [
        "#### Completion 9: (1 point)\n",
        "\n",
        "Add 4 more queries, similar to the example. The examples would be based on the first 100 recipes of our dataset.\n",
        "We would keep the title of the recipe that we used to create the query, for future reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "o2pV6lw_2bBd"
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    {\n",
        "        \"title\": \"Balsamic Chicken Pasta with Fresh Cheese\",\n",
        "        \"query\": \"I am cooking dinner. Here is what my kitchen looks like:\\nThe linguine is cooked and set aside. The red bell peppers are soft and slightly caramelized. The balsamic dressing is mixed with garlic, salt, pepper, and fresh basil. Each component is ready in its bowl, colorful and aromatic.\\n\\nWhat should I do as my next step?\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Frappuccino\",\n",
        "        \"query\": \"Its early in the morning and I am in the kitchen. I have prepared some coffee and I also have some leftover sugar and milk from earlier. Ah, I remembered now, I also have pectin and some water in the kitchen. What should I make with these?\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Hummus\",\n",
        "        \"query\": \"I love trying new recipes, but I usually make a mess and can't get everything right. I want to make some Hummus today, and I want your help. I have some chickpies, some lime juice and usual spices and salt in my kitchen. What else should I buy to make Hummus?\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Eggs Pesto\",\n",
        "        \"query\": \"I wanted to make Eggs Pesto for breakfast, but I think I did something wrong because it doesn't taste right. I poached the eggs and added salt and pepper, then I buttered the toast and put the eggs on top. I also added some pesto and parmesan to the dish. What did I do wrong?\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Monkey Bread\",\n",
        "        \"query\": \"Im so hungry. I enter the kitchen and I see my husband left some ingredients on the kitchen, but I have no idea what he was doing. I noticed that oven is preheat to 350 degrees. There is a bundt pan on the table and there is a mixture of sugar and cinnamon. I also saw some biscuits that were cut into smaller parts which was also in a bowl. How can I finish this recipe and enjoy my meal?\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4Gglt-XH2bBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b790d5-5078-4c41-e344-6bd42e1ebd7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balsamic Chicken Pasta with Fresh Cheese:\n",
            "  - Without  RAG:  To continue with your dinner preparation, you should combine the cooked linguine with the\n",
            "                  prepared balsamic dressing and the caramelized red bell peppers. Here's\n",
            "                  a step-by-step guide to help you:  1. **Drain the Linguine**: If the\n",
            "                  linguine is still in its cooking water, drain it to prevent the pasta\n",
            "                  from being too wet.  2. **Combine Ingredients**: In a large mixing bowl,\n",
            "                  combine the cooked linguine, the balsamic dressing, and the caramelized\n",
            "                  red bell peppers. Toss everything together until the pasta is evenly\n",
            "                  coated with the dressing and the peppers are well distributed.  3.\n",
            "                  **Season**: Taste the mixture and adjust the seasoning if necessary. You\n",
            "                  might want to add a bit more salt, pepper, or even a squeeze of lemon\n",
            "                  juice for extra flavor.  4. **Serve**: Transfer the combined ingredients\n",
            "                  to a serving dish. You can garnish with additional fresh basil leaves or\n",
            "                  a sprinkle of Parmesan cheese if desired.  5. **Enjoy**: Serve the dish\n",
            "                  hot and enjoy your delicious dinner!  Bon appétit!\n",
            "  - Sparse   RAG:   As your next step, you should combine the cooked linguine with the balsamic dressing and\n",
            "                  the sautéed shrimp, onion, garlic, parsley, walnuts, olives, oregano,\n",
            "                  and basil. Then, place about a cup to a cup and a half of this mixture\n",
            "                  over each plate of linguine. Finally, sprinkle some grated parmesan\n",
            "                  cheese on top for added flavor. Enjoy your delicious Shrimp Italiano\n",
            "                  over Linguine!\n",
            "  - Semantic RAG:   As your next step, you should combine the cooked linguine with the red bell pepper\n",
            "                  sauce. To do this, pour the red bell pepper sauce over the cooked\n",
            "                  linguine in the serving bowl. Mix gently to ensure the pasta is evenly\n",
            "                  coated with the sauce. If desired, you can also add some of the balsamic\n",
            "                  dressing to the pasta for extra flavor. Once mixed, your Grilled Fish\n",
            "                  Over Linguine With Roasted Pepper Sauce is ready to be served. Enjoy\n",
            "                  your dinner!\n",
            "\n",
            "Frappuccino:\n",
            "  - Without  RAG:  I have a few ideas, but I am not sure which one to choose. I could make a simple syrup, a\n",
            "                  fruit jam, or even a homemade jelly. I could also try my hand at making\n",
            "                  a custard or a pudding. I have a few fruits in the fridge, but I am not\n",
            "                  sure which one to use. I could also use the pectin and water to make a\n",
            "                  simple syrup, but I am not sure if I want to use it for that. I could\n",
            "                  also use the pectin and water to make a fruit jam, but I am not sure if\n",
            "                  I want to use it for that. I could also use the pectin and water to make\n",
            "                  a homemade jelly, but I am not sure if I want to use it for that. I\n",
            "                  could also use the pectin and water to make a custard or a pudding, but\n",
            "                  I am not sure if I want to use it for that. I am not sure which one to\n",
            "                  choose. I could also use the leftover sugar and milk to make a simple\n",
            "                  syrup, but I am not sure if I want to use it for that. I could also use\n",
            "                  the leftover sugar and milk to make a fruit jam, but I am not sure if I\n",
            "                  want to use it for that. I could also\n",
            "  - Sparse   RAG:   You can make a simple syrup with the leftover sugar, milk, pectin, and water. Here's a\n",
            "                  basic recipe:  1. In a small saucepan, combine 1 cup of sugar, 1 cup of\n",
            "                  water, and 1 teaspoon of pectin. Stir until the sugar is completely\n",
            "                  dissolved. 2. Bring the mixture to a boil over medium heat, stirring\n",
            "                  constantly. 3. Once boiling, reduce the heat to low and let it simmer\n",
            "                  for about 10 minutes, stirring occasionally. 4. Remove the saucepan from\n",
            "                  the heat and let the syrup cool slightly. 5. Once cooled, stir in 1 cup\n",
            "                  of milk until the syrup is smooth and well combined. 6. Allow the syrup\n",
            "                  to cool completely before using it in your coffee or other recipes.\n",
            "                  This simple syrup will add a touch of sweetness to your coffee and can\n",
            "                  also be used as a base for other recipes, such as sauces or dressings.\n",
            "                  Enjoy!\n",
            "  - Semantic RAG:   Based on the ingredients you have, you could make a simple syrup or a basic jam. Since\n",
            "                  you have pectin, which is a gelling agent used in jam making, you could\n",
            "                  make a jam. Here's a basic recipe you could try:  1. Combine the pectin,\n",
            "                  water, and sugar in a saucepan. Stir until the sugar is completely\n",
            "                  dissolved. 2. Bring the mixture to a boil over medium heat, stirring\n",
            "                  constantly. 3. Once it reaches a rolling boil, add the milk and continue\n",
            "                  to stir. 4. Add the fruit of your choice (you didn't mention any, but\n",
            "                  you could use any fruit you have on hand) and stir until the fruit is\n",
            "                  well coated with the syrup. 5. Return the mixture to the heat and cook,\n",
            "                  stirring frequently, until the mixture thickens to your desired\n",
            "                  consistency. This could take anywhere from 10 to 30 minutes, depending\n",
            "                  on the type of fruit and the amount of pectin. 6. Once the jam has\n",
            "                  reached the desired consistency, remove it from the heat and let it\n",
            "                  cool. 7. Once cooled, you can pour the jam into a jar and seal it. You\n",
            "                  can also add a layer of fruit on top for decoration.  Remember to\n",
            "                  sterilize your jars before canning jam, and follow proper can\n",
            "\n",
            "Hummus:\n",
            "  - Without  RAG:  To make a delicious and creamy hummus, you'll need a few additional ingredients. Here's a\n",
            "                  simple recipe to follow:  Ingredients: - 1 can of chickpeas (drained and\n",
            "                  rinsed) - 1/4 cup of fresh lemon juice - 1/4 cup of tahini (sesame\n",
            "                  paste) - 1 clove of garlic, minced - 1/4 cup of olive oil - 1/2 teaspoon\n",
            "                  of ground cumin - 1/2 teaspoon of ground paprika - Salt to taste -\n",
            "                  Optional: 1/4 cup of chopped fresh parsley or cilantro for garnish\n",
            "                  Instructions: 1. In a food processor or blender, combine the chickpeas,\n",
            "                  lemon juice, tahini, garlic, olive oil, cumin, paprika, and salt. Blend\n",
            "                  until smooth and creamy, scraping down the sides as needed. 2. Taste the\n",
            "                  hummus and adjust the seasoning if necessary. Add more salt or lemon\n",
            "                  juice if desired. 3. Transfer the hummus to a serving bowl or a piping\n",
            "                  bag, if you have one, and garnish with chopped parsley or cilantro, if\n",
            "                  using. 4. Serve with warm pita bread, vegetable sticks, or any other\n",
            "                  favorite accompaniments.  To make this recipe even more delicious, you\n",
            "                  can also\n",
            "  - Sparse   RAG:   To make hummus, you will need the following ingredients in addition to chickpeas, lime\n",
            "                  juice, and your usual spices and salt:  1. Tahini (sesame seed paste) 2.\n",
            "                  Garlic 3. Olive oil 4. Cumin 5. Paprika (optional) 6. Salt (if not\n",
            "                  already available) 7. Water (to adjust the consistency)  You can find\n",
            "                  these ingredients at most grocery stores or health food stores. Once you\n",
            "                  have all the ingredients, you can follow a basic hummus recipe:  1.\n",
            "                  Rinse and drain the chickpeas. 2. In a food processor, combine the\n",
            "                  chickpeas, tahini, garlic, olive oil, cumin, paprika, salt, and water.\n",
            "                  3. Blend until smooth and creamy. 4. Taste and adjust the seasoning if\n",
            "                  necessary. 5. Serve with warm pita bread, vegetables, or as a dip for\n",
            "                  your favorite snacks.  Enjoy your homemade hummus!\n",
            "  - Semantic RAG:   Based on the ingredients you have, you are already on the right track to make Hummus.\n",
            "                  However, to make a traditional Hummus, you would need the following\n",
            "                  additional ingredients:  1. Cumin: This spice is essential for the\n",
            "                  flavor of Hummus. You mentioned you have it, so you're good on that. 2.\n",
            "                  Paprika: This spice adds a bit of sweetness and color to the Hummus. You\n",
            "                  also have this, so you're set. 3. Ground Black Pepper: This spice adds a\n",
            "                  bit of heat and depth to the Hummus. You have this as well. 4. Salt:\n",
            "                  This is a basic seasoning that you have, so you're good on that. 5.\n",
            "                  Almonds: These are often used in Hummus for added crunch and flavor. You\n",
            "                  mentioned you have slivered almonds, but whole almonds are typically\n",
            "                  used in Hummus. You might want to consider buying some whole almonds. 6.\n",
            "                  Garlic: You mentioned you have minced garlic, which is great. However,\n",
            "                  for a more traditional Hummus, you might want to consider buying some\n",
            "                  whole garlic cloves to be crushed or minced yourself. 7. Olive Oil: This\n",
            "                  is used to cook the chickpeas and onions in the recipe. You mentioned\n",
            "                  you have olive\n",
            "\n",
            "Eggs Pesto:\n",
            "  - Without  RAG:  I think you might have added too much pesto. Pesto is very flavorful and can easily\n",
            "                  overpower the eggs if you use too much. Try using a smaller amount of\n",
            "                  pesto and see if that improves the taste. You can also try adding some\n",
            "                  fresh herbs like basil or parsley to balance out the flavors. Another\n",
            "                  thing to consider is the type of toast you used. If it was too thick or\n",
            "                  dense, it might have absorbed too much pesto and made the eggs taste\n",
            "                  bland. Try using a thinner slice of toast next time.\n",
            "  - Sparse   RAG:   Answer: You might have added too much cayenne pepper, which can make the dish taste too\n",
            "                  spicy. You could try reducing the amount of cayenne pepper or removing\n",
            "                  it altogether to see if that improves the taste. Also, make sure to use\n",
            "                  fresh eggs and not too old ones, as they can affect the texture and\n",
            "                  taste of the poached eggs. Additionally, you could try using a different\n",
            "                  type of bread, such as sourdough or whole grain, to see if that changes\n",
            "                  the flavor. Finally, make sure to use high-quality pesto and parmesan\n",
            "                  cheese for the best taste.\n",
            "  - Semantic RAG:   Based on the context provided, it seems like you followed the recipe for Eggs Pesto\n",
            "                  correctly. The ingredients and directions match the one in the document\n",
            "                  with id 'd4c0c1e7-b4b6-4b5c-8142-c43d0c53a4e0'. The ingredients you used\n",
            "                  are all listed in the recipe, and the steps you followed are also the\n",
            "                  same as the ones in the recipe.   However, the taste of food can be\n",
            "                  subjective and can be influenced by many factors such as the freshness\n",
            "                  of the ingredients, the quality of the ingredients, the cooking method,\n",
            "                  and personal taste preferences.   If you think the taste is not right,\n",
            "                  you might want to try adjusting the amount of cayenne pepper, as it is\n",
            "                  said to make a huge difference in the recipe. You could also try using\n",
            "                  different types of bread or different brands of pesto and parmesan.\n",
            "                  Remember, cooking is an art and it often takes a few tries to get the\n",
            "                  taste just right. Don't be discouraged, keep experimenting and you'll\n",
            "                  get there!\n",
            "\n",
            "Monkey Bread:\n",
            "  - Without  RAG:  I think I can make a dessert. I can use the oven to bake the biscuits and the bundt pan\n",
            "                  to make a cinnamon roll. I can mix the sugar and cinnamon with the\n",
            "                  biscuits and roll them in the bundt pan. I can then bake them in the\n",
            "                  oven at 350 degrees. I can enjoy the dessert with my husband.\n",
            "  - Sparse   RAG:   You can finish the recipe by following these steps:  1. Preheat the oven to 350 degrees\n",
            "                  as you have already done. 2. Grease the bundt pan with butter or oil to\n",
            "                  prevent the cake from sticking. 3. In a large bowl, combine the sugar\n",
            "                  and cinnamon. You can add more sugar or cinnamon if you like. 4. In\n",
            "                  another bowl, mix the biscuits with melted butter or oil. You can also\n",
            "                  add some milk or cream to make the mixture smoother. 5. Pour the biscuit\n",
            "                  mixture into the bundt pan and spread it evenly. 6. Bake the cake in the\n",
            "                  preheated oven for about 30-35 minutes, or until a toothpick inserted in\n",
            "                  the center comes out clean. 7. Let the cake cool in the pan for a few\n",
            "                  minutes, then remove it from the pan and let it cool completely on a\n",
            "                  wire rack. 8. Once the cake is completely cooled, you can frost it with\n",
            "                  your favorite icing or serve it as is.  Enjoy your meal!\n",
            "                  Document(metadata={'Unnamed: 0': 4084, 'title': 'Shrimp, Egg and Pea\n",
            "                  fried rice', 'ingredients': '[\"1/2 lb shrimp\", \"1/2 tsp cornstarch\",\n",
            "                  \"1/4 tsp salt\",\n",
            "  - Semantic RAG:   You can finish the recipe by following these steps:  1. Preheat the oven to 350 degrees\n",
            "                  Fahrenheit, as you have already done. 2. Take the bundt pan and pour the\n",
            "                  mixture of sugar and cinnamon into it. This will be the topping for the\n",
            "                  cake. 3. In a separate bowl, mix the ingredients for the cake batter.\n",
            "                  You can use the ingredients you have in the kitchen, such as eggs,\n",
            "                  vanilla pudding, sour cream, oil, butter Scotch morsels, cream cheese,\n",
            "                  and powdered sugar. You can also add some nuts or fruits if you have\n",
            "                  them. 4. Pour the batter into the bundt pan and spread it evenly. You\n",
            "                  can use a spatula or a spoon to do this. 5. Place the bundt pan in the\n",
            "                  oven and bake for about 30 to 40 minutes, or until a toothpick inserted\n",
            "                  in the center comes out clean. You can check the doneness by inserting a\n",
            "                  toothpick or a cake tester in the center of the cake. If it comes out\n",
            "                  clean, the cake is done. If it comes out with some batter, bake for a\n",
            "                  few more minutes and check again. 6. Once the cake is done, remove it\n",
            "                  from the oven and let it cool for a few minutes. You can use\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from textwrap import fill\n",
        "questions = [{\"question\": q[\"query\"]} for q in queries]\n",
        "\n",
        "llm_responses = llm.batch([q[\"query\"] for q in queries])\n",
        "sparse_rag_responses = sparse_rag.batch(questions)\n",
        "semantic_rag_responses = semantic_rag.batch(questions)\n",
        "\n",
        "for query, r1, r2, r3 in zip(queries, llm_responses, sparse_rag_responses, semantic_rag_responses):\n",
        "    # r1 = r1.split(query[\"query\"])[1].strip()\n",
        "    # r2 = r2[\"response\"].split(query[\"AI:\"])[-1].strip()\n",
        "    # r3 = r3[\"response\"].split(query[\"AI:\"])[-1].strip()\n",
        "    r1 = r1.replace(query[\"query\"], \"\")\n",
        "    r2 = r2.split(\"AI:\")[-1]\n",
        "    r3 = r3.split(\"AI:\")[-1]\n",
        "    print(f'{query[\"title\"]}:')\n",
        "    print(f'  - Without  RAG: {fill(r1, width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print(f'  - Sparse   RAG: {fill(r2, width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print(f'  - Semantic RAG: {fill(r3, width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOJLRpnD2bBd"
      },
      "source": [
        "#### Report 2: (2 points)\n",
        "\n",
        "Write a report about the experiments above. Your report should address the following:\n",
        "1. Compare the quality of the answers. In which cases did Sparse or Semantic RAG help improve the response? Was there any example where it hurt the performance?\n",
        "2. Discuss the differences between Sparse and Semantic RAG. Based on your examples, which one seems more effective and why?\n",
        "3. Any surprising findings or patterns? Did anything behave differently than you expected?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtIsiDjA6Zoc"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TJIR0sJ2bBd"
      },
      "source": [
        "#### Completion 10: (3 points)\n",
        "\n",
        "Now we want to automate the evaluation process. For this purpose, we are going to use the `ragas`. Follow the instructions of each cell to create the evaluation pipeline. To learn more about this framework, please refer to its [get started](https://docs.ragas.io/en/stable/getstarted/) or [how-to](https://docs.ragas.io/en/stable/howtos/) pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxjSZO7V2bBd"
      },
      "outputs": [],
      "source": [
        "!pip install -q ragas rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG_Q2JCf2bBd"
      },
      "outputs": [],
      "source": [
        "# Load the LLM as ragas llm. For this, we can use the provided wrapper for our existing LLM.\n",
        "\n",
        "ragas_llm ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuL-IP9L2bBd"
      },
      "outputs": [],
      "source": [
        "# Generate 10 test cases, using ragas, based on your documents. You can use a subset of your documents for faster runtime.\n",
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "# test_generator = TestsetGenerator(llm=ragas_llm,embedding_model=ragas_embedding)\n",
        "# test_set = test_generator.generate_with_langchain_docs(documents[:10], testset_size=10)\n",
        "\n",
        "test_set.test_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O-XwiVs2bBd"
      },
      "outputs": [],
      "source": [
        "test_df = test_set.to_pandas()\n",
        "test_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU3QIdVW2bBd"
      },
      "outputs": [],
      "source": [
        "test_questions = test_df[\"question\"].values.tolist()\n",
        "test_ground_truths = test_df[\"ground_truth\"].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAyWxf1u2bBd"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"sparse\": {\n",
        "        \"answers\": [],\n",
        "        \"contexts\": []\n",
        "    },\n",
        "    \"dense\": {\n",
        "        \"answers\": [],\n",
        "        \"contexts\": []\n",
        "    },\n",
        "}\n",
        "\n",
        "for question in test_questions:\n",
        "    q = {\"question\": question}\n",
        "    s_response = sparse_rag.invoke(q)\n",
        "    d_response = semantic_rag.invoke(q)\n",
        "\n",
        "    results[\"sparse\"][\"answers\"].append(s_response[\"response\"])\n",
        "    results[\"sparse\"][\"contexts\"].append([context.page_content for context in s_response[\"context\"]])\n",
        "    results[\"dense\"][\"answers\"].append(d_response[\"response\"])\n",
        "    results[\"dense\"][\"contexts\"].append([context.page_content for context in d_response[\"context\"]])\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "sparse_response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : results[\"sparse\"][\"answers\"],\n",
        "    \"contexts\" : results[\"sparse\"][\"contexts\"],\n",
        "    \"ground_truth\" : test_ground_truths\n",
        "})\n",
        "dense_response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : results[\"dense\"][\"answers\"],\n",
        "    \"contexts\" : results[\"dense\"][\"contexts\"],\n",
        "    \"ground_truth\" : test_ground_truths\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHHmIKiV2bBd"
      },
      "outputs": [],
      "source": [
        "# Load ragas evaluation metrics. We would use all possible metrics, including:\n",
        "# - Faithfulness\n",
        "# - Answer relevancy\n",
        "# - Answer correctness\n",
        "# - retrieved context related metrics\n",
        "\n",
        "metrics = [\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gn_stOM2bBd"
      },
      "outputs": [],
      "source": [
        "# Use ragas evaluator to report the score of each pipeline, using the metrics defined above.\n",
        "\n",
        "sparse_scores =\n",
        "dense_scores =\n",
        "\n",
        "print(f\"Sparse RAG Score: {sparse_scores}\")\n",
        "print(f\"Dense RAG Score: {dense_scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p1Zsbbv2bBe"
      },
      "outputs": [],
      "source": [
        "sparse_scores.to_pandas().head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkrM-3XC2bBe"
      },
      "outputs": [],
      "source": [
        "dense_scores.to_pandas().head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGTkCkXR2bBe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_sparse = pd.DataFrame(list(sparse_scores.items()), columns=['Metric', 'Sparse Retriever'])\n",
        "df_dense = pd.DataFrame(list(dense_scores.items()), columns=['Metric', 'Dense Retriever'])\n",
        "\n",
        "df_merged = pd.merge(df_sparse, df_dense, on='Metric')\n",
        "\n",
        "df_merged['Delta'] = df_merged['Dense Retriever'] - df_merged['Sparse Retriever']\n",
        "\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgCuLLiP2bBe"
      },
      "source": [
        "#### Report 3: (1 point)\n",
        "\n",
        "Compare the automated evaluation (using ragas) with your manual evaluation from the previous step. In your report, make sure to address the following:\n",
        "1. How are the two evaluation methods different? Briefly describe what makes the automated evaluation distinct from your manual judgment process (e.g., consistency, objectivity, criteria used).\n",
        "2.\tDo both evaluations show the same results? Were the rankings or judgments about the quality of responses consistent between your analysis and the automated scores?\n",
        "3.\tIf there were differences, why might that be? Reflect on what factors could lead to different results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOwTO_Xn6Zoe"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSIS7xwI2bBe"
      },
      "source": [
        "#### Question 8: (2 points)\n",
        "\n",
        "1. Explain the underlying mechanism and techniques used by `ragas` to evaluate the performance of RAG pipelines. How does it ensure that the evaluation is both comprehensive and relevant to the task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh_mRj8-6Zoe"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrgGWbOG2bBe"
      },
      "source": [
        "### 3.6 (Optional) Other strategy: (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12LXj99X2bBe"
      },
      "source": [
        "#### Question 9: (3 points)\n",
        "\n",
        "There are other retriever strategies you can use to improve the performance of your RAG pipeline. In this task:\n",
        "1. Explain what the `MultiQueryRetriever` does and how it can help improve retrieval quality in your pipeline.\n",
        "2. Implement the `MultiQueryRetriever` in your RAG pipeline and evaluate its performance using both manual and automated methods.\n",
        "3. You may also make additional improvements to your pipeline. If you do so, briefly explain what changes you made, how they affect the system, and why they might improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHg0DwLO6Zoe"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUy0-3vC6Zof"
      },
      "source": [
        "## 4. Read more: (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPUsHZh56Zof"
      },
      "source": [
        "#### Cache-Augmented Generation (CAG): (4 points)\n",
        "\n",
        "1. What is Cache-Augmented Generation (CAG)? How does it improve efficiency or performance during generation?\n",
        "2. What are the similarities and differences between Cache-Augmented Generation (CAG) and Retrieval-Augmented Generation (RAG)? In what scenarios might you prefer one over the other?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efeiIAsc6Zof"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuZxBibl6Zof"
      },
      "source": [
        "#### Multi-modal RAG: (6 points)\n",
        "\n",
        "1. How do models like CLIP enable the embedding of both text and images into a shared vector space? What are the advantages and disadvantages of using a unified embedding space for cross-modal retrieval in RAG systems.\n",
        "2. In systems like [Colpali](https://arxiv.org/pdf/2407.01449), how does dividing document images into patches enhance the retrieval process in multimodal RAG? Explore how patch-based processing preserves structural information and its impact on retrieval accuracy.\n",
        "3. What are the implications of converting non-text modalities (e.g., images) into textual representations for retrieval purposes? Discuss the benefits and drawbacks of grounding all modalities into a primary modality, such as text, in the context of RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiGRfYMs6Zof"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TuwfOPincl-1",
        "OiB9LEjb8r0f",
        "FN7dSqY-NNel",
        "QvmpQ-BtNNem"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c64ffd50c78429aa7da2ec51dd9797a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9697c587102c44cfbcc60ee59f223a6f",
              "IPY_MODEL_0b8ef95054cf4732bc2f2d1f1b659207",
              "IPY_MODEL_96c957caddb34d3e99ecf95ffcdac8e4"
            ],
            "layout": "IPY_MODEL_9e61680b7c194192a5b0b0ab793b6f25"
          }
        },
        "9697c587102c44cfbcc60ee59f223a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b2318a1b1444640969a4ef289d6fa5f",
            "placeholder": "​",
            "style": "IPY_MODEL_23c54afe53d54e8ab104db39611ee5b9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0b8ef95054cf4732bc2f2d1f1b659207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04e4c8a5fbfc484ab737365f57784c64",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa54c6c70d7d41148e86799e9bd833f1",
            "value": 2
          }
        },
        "96c957caddb34d3e99ecf95ffcdac8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_546ed3fa25314ee8aa5aaee532c57ab9",
            "placeholder": "​",
            "style": "IPY_MODEL_32c6e2932428430883580809138126e1",
            "value": " 2/2 [00:41&lt;00:00, 19.76s/it]"
          }
        },
        "9e61680b7c194192a5b0b0ab793b6f25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2318a1b1444640969a4ef289d6fa5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c54afe53d54e8ab104db39611ee5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04e4c8a5fbfc484ab737365f57784c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa54c6c70d7d41148e86799e9bd833f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "546ed3fa25314ee8aa5aaee532c57ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c6e2932428430883580809138126e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00e1a268d18d4720944e30acb39a30c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62352ebe50654885ad6a2b54bfe5d384",
              "IPY_MODEL_526a317f0ffd4e29b9c11d5ba86567de",
              "IPY_MODEL_61373864472a436bb919f1cd6dab74ba"
            ],
            "layout": "IPY_MODEL_d7a48e886e854d28a9e6ef832e6af7d1"
          }
        },
        "62352ebe50654885ad6a2b54bfe5d384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e801ad2c2ee54e07a22ac0e73bb5ff57",
            "placeholder": "​",
            "style": "IPY_MODEL_409bc8f7608349589dfb7930b5ddff16",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "526a317f0ffd4e29b9c11d5ba86567de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09c01e8b6e2249878b5f961552aa404b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d5bd78b7c6542748786ad3ec3fe2eb9",
            "value": 2
          }
        },
        "61373864472a436bb919f1cd6dab74ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_761a4452048041039ffcd1a5fd7c672e",
            "placeholder": "​",
            "style": "IPY_MODEL_280b9a3a032a40b9a6fccb238390570d",
            "value": " 2/2 [00:19&lt;00:00,  9.26s/it]"
          }
        },
        "d7a48e886e854d28a9e6ef832e6af7d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e801ad2c2ee54e07a22ac0e73bb5ff57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "409bc8f7608349589dfb7930b5ddff16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09c01e8b6e2249878b5f961552aa404b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d5bd78b7c6542748786ad3ec3fe2eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "761a4452048041039ffcd1a5fd7c672e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "280b9a3a032a40b9a6fccb238390570d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}