{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArianFiroozi/LLM-Course-Projects/blob/main/LLM_HW3_Part2_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFsETkP_6ZoL"
      },
      "source": [
        "# **CA3-Part2, LLMs Spring 2025**\n",
        "\n",
        "- **Name:**\n",
        "- **Student ID:**\n",
        "\n",
        "---\n",
        "#### Your submission should be named using the following format: `CA3-Part2_LASTNAME_STUDENTID.ipynb`.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks under each `Completion` section.\n",
        "\n",
        "- For text-based answers, you should replace the text that says `WRITE YOUR ANSWER HERE` with your actual answer, or you can look for `Report` and `Question` blocks.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---\n",
        "\n",
        "If you have any further questions or concerns, contact the TAs via email or Telegram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zivgc23P8r0f"
      },
      "source": [
        "# RAG (50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7koJfgPBRlNz"
      },
      "source": [
        "If you have any further questions or concerns, contact the TA via email (pouya.sadeghi@ut.ac.ir) or telegram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNHplrzc8r0f"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iX-Lm3M58r0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e63a6afe-3391-45c7-8a2d-f3a09e12bcb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_community langchain_huggingface huggingface_hub\n",
        "!pip install -q sentence_transformers tiktoken lark datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SalrEJJrutW",
        "outputId": "6f58ed09-0627-420e-f94c-9d9eba62aede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May  6 11:53:41 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3MhNsfsNVAVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a988473e-ece2-472a-f854-1c241c22b1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# To determine your system's CUDA version, run the following command:\n",
        "# !nvidia-smi\n",
        "\n",
        "# Based on your CUDA version, install the appropriate FAISS-GPU package:\n",
        "\n",
        "# For CUDA 12.x:\n",
        "!pip install -q faiss-gpu-cu12\n",
        "\n",
        "# For CUDA 11.x:\n",
        "# !pip install faiss-gpu-cu11\n",
        "\n",
        "# If you prefer the CPU-only version of FAISS:\n",
        "# !pip install -q faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuwfOPincl-1"
      },
      "source": [
        "## 1. An Overview of Information Retrieval (IR) and RAG (2 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHWLyGsKc59P"
      },
      "source": [
        "- **Information Retrieval (IR)**: The process of obtaining information system resources relevant to a specific information need from a collection of those resources. Each IR system consists of a collection of documents, a set of queries, and a retrieval function that ranks the documents based on their relevance to the query.\n",
        "- **Retrieval-Augmented Generation (RAG)**: A model that combines the strengths of retrieval-based and generation-based approaches. It retrieves relevant documents from a large corpus and uses them to generate a response to a query. RAG is particularly useful for tasks where the answer is not explicitly present in the training data but can be inferred from related documents.\n",
        "- **RAG Architecture**: The RAG architecture consists of two main components:\n",
        "  - **Retriever**: This component retrieves relevant documents from a large corpus based on the input query. It can be implemented using various retrieval methods, such as BM25 or dense retrieval.\n",
        "  - **Generator**: This component generates a response based on the retrieved documents and the input query. It can be implemented using transformer-based models.\n",
        "  \n",
        "In this computer assignment, you will implement a RAG pipeline using the LangChain framework. You will use two different retrievers: TF-IDF and dense retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0wV_2SCcl-2"
      },
      "source": [
        "#### Question 1: (2 points)\n",
        "1. Why do we need to use RAG?\n",
        "2. What is LangChain and how does it help in building RAG pipelines?\n",
        "<!-- 2. What are the advantages of RAG over generation methods? -->\n",
        "<!-- 3. What is the difference between dense and sparse retrievers? -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0zBJI6O6ZoN"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiB9LEjb8r0f"
      },
      "source": [
        "## 2. An Overview of LangChain (12 points + 2)\n",
        "\n",
        "In this overview, we will provide a step-by-step guide on how to construct a basic application using LangChain. To learn more about this framework, check its [tutorial](https://python.langchain.com/docs/tutorials/) which is available for different releases!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPaEdN5E8r0g"
      },
      "source": [
        "### 2.1 Lets load our model (4 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRahQhR_cl-2"
      },
      "source": [
        "#### Question 2: (2 points)\n",
        "\n",
        "1. Explain how different parameters, such as `temperature`, `max_length`, `top_p`, `top_k`, and `repetition_penalty`, affect the generation process in a language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8uW1zlt6ZoO"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKqpgVSmcl-2"
      },
      "source": [
        "#### Completion 1: (2 points)\n",
        "\n",
        "Load the `microsoft/Phi-4-mini-instruct` model and its tokenizer, and create a `text-generation` pipeline. Use the LangChain framework to integrate the model into your application. You should configure the pipeline with appropriate parameters, such as *max_new_tokens*, *temperature*, *top_p*, *top_k*, and *repetition_penalty*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64-XOX3M8r0g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3c64ffd50c78429aa7da2ec51dd9797a",
            "9697c587102c44cfbcc60ee59f223a6f",
            "0b8ef95054cf4732bc2f2d1f1b659207",
            "96c957caddb34d3e99ecf95ffcdac8e4",
            "9e61680b7c194192a5b0b0ab793b6f25",
            "6b2318a1b1444640969a4ef289d6fa5f",
            "23c54afe53d54e8ab104db39611ee5b9",
            "04e4c8a5fbfc484ab737365f57784c64",
            "fa54c6c70d7d41148e86799e9bd833f1",
            "546ed3fa25314ee8aa5aaee532c57ab9",
            "32c6e2932428430883580809138126e1"
          ]
        },
        "outputId": "57546c5d-f8b1-4573-ab0b-8586ee2f5be3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c64ffd50c78429aa7da2ec51dd9797a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
        "DEVICE = \"cuda:0\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=DEVICE,\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens= 50)\n",
        "\n",
        "# Load the pipeline into LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCphndr0uyDf",
        "outputId": "b6101874-d6be-4ddd-bfdc-862f1ac7f45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAxNYgBGcl-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87bd6e92-5017-4b1e-a27b-6865337113b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is the president of the United States? The president of the United States is Joe Biden. He assumed office on January 20, 2021, after winning the 2020 presidential election.\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Who is the president of the United States?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joCaX8tY8r0g"
      },
      "source": [
        "### 2.2 Simple Chain (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdkMm3Mz8r0h"
      },
      "source": [
        "#### Completion 2: (2 points)\n",
        "\n",
        "Complete the next cell to create a simple chain that takes the name of a football (soccer) player as input and outputs some information about that person. To do so:\n",
        "\n",
        "1. Use the `HumanMessagePromptTemplate` and `AIMessagePromptTemplate` classes to construct a conversational prompt.\n",
        "2. Use `ChatPromptTemplate` to organize the messages.\n",
        "3. Pass the prompt into the model you have loaded before.\n",
        "4. Use `StrOutputParser` to return a plain string.\n",
        "\n",
        "Your final chain should take a dictionary with a **person_name** key and return a brief description about that player."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMEB4D4z8r0h"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# Create a simple prompt template with a human message and an AI message\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "  HumanMessagePromptTemplate.from_template(\"Give me a brief explanation of {person_name} in soccer.\"),\n",
        "  AIMessagePromptTemplate.from_template(\"{person_name} is\"),\n",
        "])\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Create a simple chain with the prompt, LLM, and output parser. The goal is to generate a response to the prompt and parse the output as a string.\n",
        "simple_chain = (\n",
        "    prompt |\n",
        "    llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGGAq5Atcl-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33bb4114-fb1a-445c-cc24-e1f5cfef36aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Give me a brief explanation of Kylian Mbappé in soccer.\n",
            "AI: Kylian Mbappé is a professional French footballer who plays as a forward. He is known for his speed, agility, and goal-scoring ability. Mbappé has been a key player for Paris Saint-Germain (PSG) in France and has also represented the French\n"
          ]
        }
      ],
      "source": [
        "answer = simple_chain.invoke({\"person_name\": \"Kylian Mbappé\"})\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq8wDeKK8r0h"
      },
      "source": [
        "#### Question 3: (2 points)\n",
        "\n",
        "1. Write about the objectives behind the creation of `HumanMessagePromptTemplate` and `AIMessagePromptTemplate` classes. What they actually do and how should we use them? Write a brief description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2syj8rxJ6ZoP"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoN6LFaW8r0h"
      },
      "source": [
        "### 2.3 JSON Chain (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F56RIBhk8r0i"
      },
      "source": [
        "#### Completion 3: (1 point)\n",
        "\n",
        "Now we want to improve the chain to extract data from the model response. Modify the existing prompt to request information about a football player, such as:\n",
        "- full name\n",
        "- nationality\n",
        "- age\n",
        "- current club\n",
        "- position\n",
        "\n",
        "In this chain, you can use `SystemMessagePromptTemplate` as well.\n",
        "At the end, use `JsonOutputParser` to parse the model's output and return a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztIkrpmk8r0i"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Create the prompt template.\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"You are an expert in football, and you give statistics about players like full name, nationality, age, current club and position. format your answers as json.\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"Give me a brief explanation of {player_name} in soccer.\"),\n",
        "    AIMessagePromptTemplate.from_template(\"{{\")\n",
        "])\n",
        "\n",
        "# Use JsonOutputParser to parse the response as a dictionary\n",
        "output_parser = JsonOutputParser()\n",
        "\n",
        "get_ai_resp = lambda x: x.split(\"AI:\")[-1]\n",
        "\n",
        "# Define the chain. This time, we want output to be parsed as JSON, using the JsonOutputParser.\n",
        "json_chain =prompt|llm|get_ai_resp|output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVy5yaAJi3NU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25486c2d-6d54-40f8-ad71-b156c7d59c00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'full_name': 'Lionel Andrés Messi',\n",
              " 'nationality': 'Argentine',\n",
              " 'age': 35,\n",
              " 'current_club': 'Paris Saint-Germain',\n",
              " 'position': 'Forward'}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "json_chain.invoke({\"player_name\": \"Lionel Messi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xMWli_Ocl-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5da4d7-6271-4f34-e73d-74fc2a4697bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lionel Messi:\n",
            "  full_name: Lionel Andrés Messi\n",
            "  nationality: Argentine\n",
            "  age: 35\n",
            "  current_club: Paris Saint-Germain\n",
            "  position: Forward\n",
            "\n",
            "Cristiano Ronaldo:\n",
            "  full_name: Cristiano Ronaldo dos Santos Aveiro\n",
            "  nationality: Portuguese\n",
            "  age: 37\n",
            "  current_club: Al Nassr\n",
            "  position: Forward\n",
            "\n",
            "Kylian Mbappé:\n",
            "  full_name: Kylian Mbappé\n",
            "  nationality: French\n",
            "  age: 22\n",
            "  current_club: Paris Saint-Germain\n",
            "  position: Forward\n",
            "\n",
            "Neymar:\n",
            "  full_name: Neymar Jr.\n",
            "  nationality: Brazilian\n",
            "  age: 32\n",
            "  current_club: Paris Saint-Germain\n",
            "  position: Forward\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Batch the requests for multiple\n",
        "batch_questions = [\n",
        "  {\"player_name\": \"Lionel Messi\"},\n",
        "  {\"player_name\": \"Cristiano Ronaldo\"},\n",
        "  {\"player_name\": \"Kylian Mbappé\"},\n",
        "  {\"player_name\": \"Neymar\"}\n",
        "]\n",
        "answers = json_chain.batch(batch_questions)\n",
        "\n",
        "# Print the extracted information\n",
        "for (q, a) in zip(batch_questions, answers):\n",
        "  print(f\"{q['player_name']}:\")\n",
        "  for key, value in a.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ0ogQiMcl-4"
      },
      "source": [
        "#### Report 1: (1 point)\n",
        "\n",
        "Explain the challenges you faced in this step. How did you manage to solve them? How could the parameters you used in the text generation pipeline affect the model’s output?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uowWBQc6ZoP"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_3x5rCT6ZoP"
      },
      "source": [
        "#### Question 4: (2 points)\n",
        "\n",
        "1. How sampling parameters such as *temperature*, *top_p*, and *top_k* can affect our JSON pipeline? Answer the question with respect to the format and content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mQ6qjFu6ZoP"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RF_aXt6dC9g"
      },
      "source": [
        "### 2.4 (Optional) Tool use: Web search (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjkcoqElcl-4"
      },
      "source": [
        "#### Completion 4: (2 points)\n",
        "\n",
        "Add context about each player by using web search. For this purpose, you need to use a web search tool, and write a function to check the output of the search tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE-1-fzpcl-4"
      },
      "outputs": [],
      "source": [
        "# Code here. Note that you may need to install additional packages for web search tool.\n",
        "\n",
        "\n",
        "\n",
        "# Define the chain here\n",
        "footballer_chain = (\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UhjgFJTcl-4"
      },
      "outputs": [],
      "source": [
        "# Batch the requests for multiple\n",
        "batch_questions = [\n",
        "  {\"player_name\": \"Lionel Messi\"},\n",
        "  {\"player_name\": \"Cristiano Ronaldo\"},\n",
        "  {\"player_name\": \"Kylian Mbappé\"},\n",
        "  {\"player_name\": \"Neymar\"},\n",
        "  {\"player_name\": \"Declan Rice\"},\n",
        "  {\"player_name\": \"Trent Alexander-Arnold\"},\n",
        "  {\"player_name\": \"John Stones\"},\n",
        "  {\"player_name\": \"Alphonso Davies\"}\n",
        "]\n",
        "answers = footballer_chain.batch(batch_questions)\n",
        "\n",
        "# Print the extracted information\n",
        "for (q, a) in zip(batch_questions, answers):\n",
        "  print(f\"{q['player_name']}:\")\n",
        "  for key, value in a.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS8as30J8r0i"
      },
      "source": [
        "## 3. Build a RAG pipeline (26 points + 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aM-prY48r0i"
      },
      "source": [
        "In this section, We use a subset of [RecipeNLG](https://recipenlg.cs.put.poznan.pl) dataset to build our RAG pipelines. The dataset contains recipes and their corresponding instructions.\n",
        "\n",
        "You can download the subset from [this google drive link](https://drive.google.com/file/d/1mgPcQKc7-SaWVyxaJ404L6dGkQvODca5/view?usp=sharing) or from the course website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW2JuoTzNNek"
      },
      "source": [
        "### 3.1 Load and prepare the dataset (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGWVvgLqNNek"
      },
      "source": [
        "#### Completion 5: (4 point)\n",
        "\n",
        "First, you should load the dataset, which is stored in a CSV file. and converting it to a `datasets.Dataset` object.\n",
        "\n",
        "The dataset contains the following columns:\n",
        "- **title**: The name of the recipe\n",
        "- **ingredients**: A list of ingredients used in the recipe, including quantities and preparation methods\n",
        "- **directions**: The instructions for preparing the recipe, presented as a list of sequential steps\n",
        "- **NER**: A list of named entities representing the core food items and cooking components extracted from each recipe, without quantities or preparation instructions.\n",
        "\n",
        "**Attention**: You should carefully process list objects (ingredients, directions, and NER) and convert them to a string document.\n",
        "\n",
        "**Attention 2**: The provided dataset, has 5k recipes. You can use a smaller subset of the dataset for your experiments. For example, you can use the first 100 recipes for your experiments or more, based on your resource limitation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YIkZv62ZNNek"
      },
      "outputs": [],
      "source": [
        "!pip install -q gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6cVk_djLNNek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07fc914-c176-480c-c145-e1ba4f920f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mgPcQKc7-SaWVyxaJ404L6dGkQvODca5\n",
            "To: /content/data_5000.csv\n",
            "\r  0% 0.00/5.92M [00:00<?, ?B/s]\r 80% 4.72M/5.92M [00:00<00:00, 38.9MB/s]\r100% 5.92M/5.92M [00:00<00:00, 47.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1mgPcQKc7-SaWVyxaJ404L6dGkQvODca5 -O data_5000.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dmjlX1Go8r0i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "b60a0b9a79ee4762ad9e2957c391a288",
            "4a7bc525229b49a2af767cad439776a0",
            "d5fdb4c8ddc6435da34c3fc58a680dc6",
            "4dc94883b0064fbf99ba12edd572adac",
            "e78862f4cb424b748ec5688083ac168e",
            "664223d5ca98409cb8a68557d1824228",
            "44d248d6ac9f4e3db84c2b0ae3aa0423",
            "fcce634cab064dd084053256bf73fa5a",
            "1451b214af67413bad5cb5c2bf269b46",
            "7685b0831c304cf4a0592e21c70dd3e5",
            "5c424cd16e414ad2af1b48604e6d04d2"
          ]
        },
        "outputId": "31ad431d-6cbd-4355-e3af-5b4d9b9c5d49"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b60a0b9a79ee4762ad9e2957c391a288"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Unnamed: 0', 'title', 'ingredients', 'directions', 'NER'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Code here to load and process the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Store the datasets.Dataset object in the variable `dataset`\n",
        "dataset = load_dataset('csv', data_files=\"/content/data_5000.csv\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WnclCb3WZOA",
        "outputId": "2828b233-e3ee-4aa6-f1f1-d72e8bc4dd18"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Unnamed: 0': 0,\n",
              " 'title': 'Worlds Best Mac and Cheese',\n",
              " 'ingredients': '[\"6 ounces penne\", \"2 cups Beechers Flagship Cheese Sauce (recipe follows)\", \"1 ounce Cheddar, grated (1/4 cup)\", \"1 ounce Gruyere cheese, grated (1/4 cup)\", \"1/4 to 1/2 teaspoon chipotle chili powder (see Note)\", \"1/4 cup (1/2 stick) unsalted butter\", \"1/3 cup all-purpose flour\", \"3 cups milk\", \"14 ounces semihard cheese (page 23), grated (about 3 1/2 cups)\", \"2 ounces semisoft cheese (page 23), grated (1/2 cup)\", \"1/2 teaspoon kosher salt\", \"1/4 to 1/2 teaspoon chipotle chili powder\", \"1/8 teaspoon garlic powder\", \"(makes about 4 cups)\"]',\n",
              " 'directions': '[\"Preheat the oven to 350 F. Butter or oil an 8-inch baking dish.\", \"Cook the penne 2 minutes less than package directions.\", \"(It will finish cooking in the oven.)\", \"Rinse the pasta in cold water and set aside.\", \"Combine the cooked pasta and the sauce in a medium bowl and mix carefully but thoroughly.\", \"Scrape the pasta into the prepared baking dish.\", \"Sprinkle the top with the cheeses and then the chili powder.\", \"Bake, uncovered, for 20 minutes.\", \"Let the mac and cheese sit for 5 minutes before serving.\", \"Melt the butter in a heavy-bottomed saucepan over medium heat and whisk in the flour.\", \"Continue whisking and cooking for 2 minutes.\", \"Slowly add the milk, whisking constantly.\", \"Cook until the sauce thickens, about 10 minutes, stirring frequently.\", \"Remove from the heat.\", \"Add the cheeses, salt, chili powder, and garlic powder.\", \"Stir until the cheese is melted and all ingredients are incorporated, about 3 minutes.\", \"Use immediately, or refrigerate for up to 3 days.\", \"This sauce reheats nicely on the stove in a saucepan over low heat.\", \"Stir frequently so the sauce doesnt scorch.\", \"This recipe can be assembled before baking and frozen for up to 3 monthsjust be sure to use a freezer-to-oven pan and increase the baking time to 50 minutes.\", \"One-half teaspoon of chipotle chili powder makes a spicy mac, so make sure your family and friends can handle it!\", \"The proportion of pasta to cheese sauce is crucial to the success of the dish.\", \"It will look like a lot of sauce for the pasta, but some of the liquid will be absorbed.\"]',\n",
              " 'NER': '[\"penne\", \"Beechers Flagship Cheese Sauce\", \"Cheddar\", \"Gruyere cheese\", \"chipotle chili powder\", \"butter\", \"flour\", \"milk\", \"semihard cheese\", \"semisoft cheese\", \"kosher salt\", \"chipotle chili powder\", \"garlic\"]'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WF2MjTYelRfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3f2a3b-eaae-4a73-a597-4030bf8240a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 5000\n"
          ]
        }
      ],
      "source": [
        "# In this cell, you should store the dataset, as a list of `langchain_core.documents.Document` objects, which can simplify your future steps.\n",
        "# You should decide how to convert the dataset to documents\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents: list[Document] = [Document(\n",
        "        page_content=str(row),\n",
        "        metadata=dict(row)\n",
        "    )\n",
        "    for row in dataset['train']]\n",
        "\n",
        "\n",
        "print(f\"Number of documents: {len(documents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2bHNwp3sNNel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b040fcb4-62f8-4678-8770-78e174e6d00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 15194\n"
          ]
        }
      ],
      "source": [
        "# Now, you should use a splitter to divide long texts into smaller, manageable chunks so they can fit within the context window of language models or retrievers.\n",
        "# Use `RecursiveCharacterTextSplitter` to split the documents into smaller chunks, ans set the `chunk_size` and `chunk_overlap` parameters accordingly.\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,         # adjust based on your needs and model token limits\n",
        "    chunk_overlap=50        # overlap helps preserve context between chunks\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Number of chunks: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN7dSqY-NNel"
      },
      "source": [
        "### 3.2 Sparse Retriever (3 points)\n",
        "\n",
        "In this section, we would create a sparse retriever for our RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elzh6QSSNNem"
      },
      "source": [
        "#### Question 5: (2 points)\n",
        "\n",
        "1. Explain how a sparse retriever like TF-IDF represents documents and queries. How does this representation influence which documents are retrieved?\n",
        "2. Sparse retrievers rely on exact token matches between queries and documents. What are the strengths and weaknesses of this approach, especially compared to dense retrievers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrP8Kz1K6Zoa"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvmpQ-BtNNem"
      },
      "source": [
        "#### Completion 6: (1 point)\n",
        "\n",
        "Complete the code cells below to create a sparse retriever, which would be later used in our RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6Rq6DS9wNNem"
      },
      "outputs": [],
      "source": [
        "# Prepare your retriever. For this section, you should use a sparse retriever such as `TFIDF` or `BM25`.\n",
        "# We want our retriever to retrieve the first 3 chunks that are most relevant to the query.\n",
        "from langchain_community.retrievers import TFIDFRetriever\n",
        "\n",
        "sparse_retriever =TFIDFRetriever.from_documents(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "S7IoZQioNNem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced446c4-60af-4f65-d514-0eba141e82c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-16b24db22bc3>:11: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_chunks = sparse_retriever.get_relevant_documents(Sample_query)[:3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "Metadata: {'Unnamed: 0': 728, 'title': 'Pumpkin Cider Bread', 'ingredients': '[\"1 cup apple cider\", \"1 cup canned pumpkin puree\", \"2 large eggs\", \"1/4 cup vegetable oil\", \"3/4 cup firmly packed light brown sugar\", \"2 tablespoons freshly grated orange zest\", \"2 cups all-purpose flour\", \"2 teaspoons double-acting baking powder\", \"1/2 teaspoon salt\", \"1/4 teaspoon baking soda\", \"1/4 teaspoon ground mace\", \"1/4 teaspoon cinnamon\", \"1/8 teaspoon ground cloves\", \"1/2 cup chopped walnuts\"]', 'directions': '[\"In a saucepan boil the cider until it is reduced to about 1/4 cup and let it cool.\", \"In a bowl whisk together well the pumpkin puree, the eggs, the oil, the brown sugar, the zest, and the reduced cider.\", \"Into the bowl sift together the flour, the baking powder, the salt, the baking soda, the mace, the cinnamon, and the cloves, add the walnuts, and stir the batter until it is just combined.\", \"Transfer the batter to a well-buttered 8 1/2-by 4 1/2-inch loaf pan and bake the bread in the middle of a preheated 350F.\", \"oven for 1 hour, or until a tester comes out clean.\", \"Let the bread cool in the pan.\"]', 'NER': '[\"apple cider\", \"pumpkin puree\", \"eggs\", \"vegetable oil\", \"brown sugar\", \"orange zest\", \"flour\", \"double-acting baking powder\", \"salt\", \"baking soda\", \"ground mace\", \"cinnamon\", \"ground cloves\", \"walnuts\"]'}\n",
            "\n",
            "Chunk 2:\n",
            "Metadata: {'Unnamed: 0': 1339, 'title': 'Rich Flourless Chocolate Torte', 'ingredients': '[\"1 cup granulated sugar\", \"1 1/2 cups light corn syrup\", \"20 ounces goodquality semisweet chocolate, preferably French or Belgian cut into small pieces plus, 8 ounces additional\", \"chocolate\", \"2 sticks (16 tablespoons) unsalted butter\", \"10 whole eggs\", \"1/2 cup heavy cream\"]', 'directions': '[\"Preheat the oven to 350 degrees.\", \"In a heavy bottomed pot, combine the sugar and corn syrup and bring to a boil.\", \"Reduce the heat to low and cook for 5 minutes.\", \"Remove from the heat.\", \"Melt the 20 ounces chopped chocolate in the top of a double boiler over simmering water.\", \"Stir the chocolate occasionally as it melts.\", \"Remove the pan from the heat and stir in the butter.\", \"Set aside to cool slightly.\", \"With an electric mixer, beat the eggs at high speed until frothy.\", \"Lower the speed to medium and carefully pour the hot syrup into the beaten eggs.\", \"Add the melted chocolate and butter mixture and blend to a smooth consistency.\", \"Butter a 10 inch to 12inch round cake pan lightly and pour in the chocolate mixture.\", \"Set the cake pan into a larger pan (such as a roasting pan) and add hot water to rise halfway up the sides of the cake pan.\", \"Place in the preheated oven and bake about 45 to 50 minutes.\", \"Being very careful not to scald yourself or get water into the torte, remove the pans from the oven, and remove the cake pan from the water bath.\", \"Cool the torte completely to room temperature before unmolding.\", \"Run a sharp knife around the inner circumference of the pan.\", \"Place a plate over the pan, invert, and, if necessary, tap the bottom of the pan with the knife handle to encourage the torte to release.\", \"The torte should then be refrigerated for at least 3 hours before serving.\", \"While the torte is cooling, place the heavy cream in a thick bottomed pot and begin to heat gently.\", \"Add the remaining 8 ounces of chocolate, stir together while the chocolate begins to melt in the hot cream.\", \"Being careful not to burn, boil or scorch the chocolate, when the chocolate has melted almost entirely, remove from the heat and allow to cool for several minutes before pouring over the torte for a glossy \\\\\"finish\\\\\".\", \"Place the unmolded torte on a baking rack with a cookie sheet underneath, and pour slightly cooled chocolate ganache directly onto the center of the torte in one motion so that it will flow out in one smooth sheet over the top and sides of the torte.\", \"You can gently help this along with the aid of a spatula but is important to use still warmed ganache.\", \"Allow to dry a few moments at room temperature.\", \"Then place the torte, baking rack, and cookie sheet into the refrigerator to chill completely and set firm.\", \"If you haven\\'t overheated the chocolate or cream, this \\\\\"ganache\\\\\" will set and harden into a shiny \\\\\"couverture\\\\\" during the hour the torte is chilling in the refrigerator.\"]', 'NER': '[\"sugar\", \"light corn syrup\", \"goodquality semisweet chocolate\", \"chocolate\", \"butter\", \"eggs\", \"heavy cream\"]'}\n",
            "\n",
            "Chunk 3:\n",
            "Metadata: {'Unnamed: 0': 1592, 'title': \"Kunkhen's Torn Noodle Soup\", 'ingredients': '[\"2 cups all-purpose flour\", \"1/4 teaspoon sea salt\", \"3/4 cup water\", \"1 tablespoon peanut oil\", \"2 medium onions, thinly sliced\", \"2 cloves garlic, coarsely chopped\", \"1 2-inch length fresh ginger, peeled and coarsely chopped\", \"1 carrot, cut in half lengthwise, then cut in thin half-moon shapes\", \"2 ounces black radish, peeled and diced\", \"4 tomatoes, cored and coarsely chopped\", \"5 shiitake mushrooms, soaked in 1 cup hot water\", \"About 5 teaspoons fermented black beans, or to taste, (or 1 heaping tablespoon prepared black bean garlic sauce - see head note)\", \"8 cups water\", \"1 1/2 pounds fresh spinach, trimmed and rinsed\", \"2 heads butter lettuce, rinsed, leaves scissor cut in wide strips\", \"Sea salt to taste\", \"1 cup fresh cilantro leaves, firmly packed\", \"3 scallions, trimmed and minced\"]', 'directions': '[\"1.\", \"Place the flour in a medium bowl and make a well.\", \"Add the water and the salt and mix, then gradually mix in the flour.\", \"Turn out the dough onto a lightly floured surface and knead it until it is slightly elastic and smooth, about 5 minutes.\", \"Cover it with a bowl and let it sit.\", \"The noodle dough can be prepared up to 2 hours before making the soup.\", \"2.\", \"To make the soup, place the oil, onions and garlic in a large stockpot over medium heat and cook, stirring occasionally, until the onions are translucent, about 8 minutes.\", \"Add the ginger, the carrots, and the radish and stir, then add the tomatoes and stir.\", \"Cook, stirring, until the tomatoes have lost their shape, about 8 minutes.\", \"3.\", \"Drain the mushrooms, saving the liquid they soaked in, and cut them in quarters.\", \"Add the black beans, or the black bean puree to the vegetables in the stock pot, stir, then add the water and the mushroom soaking liquid.\", \"Stir, cover, and bring to a boil.\", \"Reduce the heat so the liquid is simmering and cook until the vegetables are tender, about 45 minutes.\", \"Add the spinach and the lettuces.\", \"You may need to add the greens gradually, pushing them down into the liquid as they wilt.\", \"Cook until all the greens are wilted.\", \"4.\", \"While the greens are cooking, divide the noodle dough in quarters.\", \"Roll the quarters into 1-inch thick ropes, then take one rope and holding it at the end between your thumbs and forefingers, gradually move along the rope of dough as you continue to pinch the dough with your fingers, so that you end up with a band of dough that is about 2-inches wide.\", \"Repeat the process on the same band of dough, then repeat with all the ropes of dough so that you have four, flat bands.\", \"They will be irregular, but that is fine.\", \"When the greens are thoroughly cooked, tear 1-inch pieces off the first band of dough and throw them into the soup.\", \"Stir, then continue with the remaining band of dough.\", \"The dough is fairly soft, but it tears quite well, resulting in noodles that are irregularly shaped, just as Kunkhen\\'s were.\", \"5.\", \"Cook the noodles just until they are tender, 2 to 3 minutes, stirring the soup so they cook evenly.\", \"Serve the soup immediately, with the garnishes alongside.\"]', 'NER': '[\"flour\", \"salt\", \"water\", \"peanut oil\", \"onions\", \"garlic\", \"ginger\", \"carrot\", \"black radish\", \"tomatoes\", \"shiitake mushrooms\", \"black beans\", \"water\", \"fresh spinach\", \"butter\", \"salt\", \"fresh cilantro\", \"scallions\"]'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query below is related to `Zucchini Nut Bread` recipe.\n",
        "\n",
        "Sample_query = \"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\"\n",
        "\n",
        "# Use the sparse retriever to get the most relevant chunks for the query\n",
        "retrieved_chunks = sparse_retriever.get_relevant_documents(Sample_query)[:3]\n",
        "\n",
        "# Now, see what chunks were retrieved\n",
        "for i, chunk in enumerate(retrieved_chunks, start=1):\n",
        "    print(f\"Chunk {i}:\")\n",
        "    # print(chunk.page_content)\n",
        "    print(\"Metadata:\", chunk.metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qnbeJnz8r0k"
      },
      "source": [
        "### 3.3 Semantic Retriever (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgEphnmn8r0k"
      },
      "source": [
        "#### Question 6: (2 point)\n",
        "\n",
        "1. How does a semantic retriever represent documents and queries differently from a sparse retriever? Explain why this helps in cases where there are no exact word overlaps.\n",
        "2. What role do embedding models (e.g., sentence-transformers) play in semantic retrieval? How does the choice of embedding model affect the retriever’s performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6u_rLg-6Zob"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1GCzoOXUf8c"
      },
      "source": [
        "#### Completion 7: (2 point)\n",
        "\n",
        "Let's create a semantic retriever. We would use `BAAI/bge-small-en` as our embedding model, and `FAISS` as our vector store. Complete the code cells below to create a semantic retriever, which would be later used in our RAG pipeline.\n",
        "\n",
        "As explained before, we want our retriever to retrieve the first 3 most relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "V6NzUEAs8r0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a533e3ba-f8b9-4901-99d0-ff57162cab06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-eb02adf2272e>:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n"
          ]
        }
      ],
      "source": [
        "# Code here\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
        "semantic_retriever =vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "l5Uc59HkUf8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff62cdc7-8546-42e5-f548-c10902b68513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "Metadata: {'Unnamed: 0': 2363, 'title': 'Basil Zucchini', 'ingredients': '[\"1 teaspoon olive oil\", \"3 small zucchini, thinly sliced\", \"2 tablespoons chopped fresh basil\", \"1 garlic clove, minced\"]', 'directions': '[\"Heat oil in heavy medium nonstick skillet.\", \"Add zucchini, 1 tablespoon basil and garlic and stir-fry until zucchini is just tender, about 5 minutes.\", \"Season to taste with salt and pepper.\", \"Remove from heat.\", \"Sprinkle with remaining basil.\"]', 'NER': '[\"olive oil\", \"zucchini\", \"fresh basil\", \"garlic\"]'}\n",
            "\n",
            "Chunk 2:\n",
            "Metadata: {'Unnamed: 0': 229, 'title': 'Baked Zucchini', 'ingredients': '[\"1 teaspoon olive oil\", \"1 pound zucchini, sliced 1/4-inch thick, at an angle\", \"Kosher salt and freshly ground black pepper\", \"Pinch of Hungarian paprika\", \"1/2 cup panko breadcrumbs (Japanese)\", \"1/4 cup grated Parmesan cheese\", \"8 sprigs fresh thyme, leaves stripped from the stem, lightly chopped\", \"1 tablespoon olive oil\"]', 'directions': '[\"Heat the oven to 350 degrees F.\", \"Brush 1 teaspoon olive oil on the bottom of an 8 by 8-inch baking dish.\", \"Arrange the slices of zucchini in the dish with an overlapping pattern in rows or a spiral in a pie dish.\", \"Sprinkle with salt, pepper and paprika.\", \"To make the topping: In a bowl, stir together the panko breadcrumbs, thyme, Parmesan cheese and season with a sprinkle of salt and a few grinds of pepper.\", \"Add 1 tablespoon olive oil and stir until all the breadcrumbs are soaked with the yellow tint of the oil.\", \"Sprinkle the topping evenly over the dish and bake until the top is golden brown, 30 to 35 minutes.\"]', 'NER': '[\"olive oil\", \"zucchini\", \"Kosher salt\", \"paprika\", \"breadcrumbs\", \"Parmesan cheese\", \"thyme\", \"olive oil\"]'}\n",
            "\n",
            "Chunk 3:\n",
            "Metadata: {'Unnamed: 0': 1891, 'title': 'Zucchini Bread', 'ingredients': '[\"4 eggs, beaten\", \"2 cups sugar\", \"1 cup oil\", \"3 12 cups flour\", \"34 teaspoon baking powder\", \"1 12 teaspoons baking soda\", \"1 12 teaspoons salt\", \"2 cups grated zucchini\", \"3 teaspoons vanilla\", \"2 teaspoons cinnamon\", \"1 cup raisins (optional)\", \"1 cup nuts (optional)\"]', 'directions': '[\"Grease and flour 2 loaf pans.\", \"Mix eggs, sugar and oil together.\", \"Add grated zucchini and vanilla to egg mixture.\", \"Sift dry ingredients and add to wet ingredients.\", \"Add raisins and nuts.\", \"Mix well.\", \"Bake at 350 degrees for 1 hour or until toothpick inserted in center comes out clean.\"]', 'NER': '[\"eggs\", \"sugar\", \"oil\", \"flour\", \"baking powder\", \"baking soda\", \"salt\", \"zucchini\", \"vanilla\", \"cinnamon\", \"raisins\", \"nuts\"]'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query below is related to `Zucchini Nut Bread` recipe.\n",
        "\n",
        "Sample_query = \"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\"\n",
        "\n",
        "# Use the semantic retriever to get the most relevant chunks for the query\n",
        "retrieved_chunks = semantic_retriever.get_relevant_documents(Sample_query)[:3]\n",
        "\n",
        "# Now, see what chunks were retrieved\n",
        "for i, chunk in enumerate(retrieved_chunks, start=1):\n",
        "    print(f\"Chunk {i}:\")\n",
        "    # print(chunk.page_content)\n",
        "    print(\"Metadata:\", chunk.metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYbF75HP2bBc"
      },
      "source": [
        "### 3.4 Create RAG pipelines (6 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_KnISF32bBc"
      },
      "source": [
        "#### Question 7: (2 points)\n",
        "\n",
        "1. What are the main components of a RAG system, and how do they interact during inference? Describe the flow from user input to model output.\n",
        "2. Describe two different strategies for integrating retrieved context into the prompt. What are the trade-offs between these approaches?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwNj8aph6Zob"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avtuGMuT2bBc"
      },
      "source": [
        "#### Completion 8: (4 points)\n",
        "\n",
        "Follow the instructions below to build a RAG pipeline using the retrievers you created in the previous sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqo5m1La2bBc"
      },
      "outputs": [],
      "source": [
        "Sample_query = \"\"\"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\n",
        "\n",
        "What is your best guess about what am I cooking?\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxybNSeT2bBc"
      },
      "outputs": [],
      "source": [
        "# We are going to use \"microsoft/Phi-4-mini-instruct\" as our LLM again. If you need, load it again here and as before.\n",
        "\n",
        "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
        "tokenizer =\n",
        "model =\n",
        "pipe =\n",
        "llm ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-_gpU7u2bBc"
      },
      "outputs": [],
      "source": [
        "# First, we need to define a new chat template, that provide the retrieved documents as context to the LLM.\n",
        "from langchain_core.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YU0sdZa2bBc"
      },
      "outputs": [],
      "source": [
        "# Now, let's create a simple RAG pipeline, using the sparse retriever. Note that we need the retrieved context as part of the output, so that we can later use it for evaluation.\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "sparse_rag = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : (input)  populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : (output) chunks retrieved by the sparse retriever, based on the \"question\" value\n",
        "    # \"response\" : (output) the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #                       into the LLM and stored in a key called \"response\"\n",
        "\n",
        ")\n",
        "\n",
        "# Now, let's test the sparse RAG pipeline with a sample query.\n",
        "response =\n",
        "print(response[\"response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRy88ZMi2bBc"
      },
      "outputs": [],
      "source": [
        "# For this cell, everything is the same as the previous cell, except that we are using the semantic retriever instead of the sparse retriever.\n",
        "\n",
        "semantic_rag = (\n",
        "    # The same as previous cell, but using the semantic retriever instead of the sparse retriever\n",
        ")\n",
        "\n",
        "# Now, let's test the semantic RAG pipeline with a sample query.\n",
        "response =\n",
        "print(response[\"response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvYodfcg2bBd"
      },
      "outputs": [],
      "source": [
        "# Finally, let's try the same query with the LLM directly, without any retrieval.\n",
        "response =\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mCOh9_i2bBd"
      },
      "source": [
        "### 3.5 Evaluate our pipelines (9 points)\n",
        "\n",
        "In this section, we are going to evaluate our RAG pipelines. First, we would design 5 queries to evaluate our RAG pipelines and our LLM alone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDgQ1uNE2bBd"
      },
      "source": [
        "#### Completion 9: (1 point)\n",
        "\n",
        "Add 4 more queries, similar to the example. The examples would be based on the first 100 recipes of our dataset.\n",
        "We would keep the title of the recipe that we used to create the query, for future reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2pV6lw_2bBd"
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    {\n",
        "        \"title\": \"Balsamic Chicken Pasta with Fresh Cheese\",\n",
        "        \"query\": \"I am cooking dinner. Here is what my kitchen looks like:\\nThe linguine is cooked and set aside. The red bell peppers are soft and slightly caramelized. The balsamic dressing is mixed with garlic, salt, pepper, and fresh basil. Each component is ready in its bowl, colorful and aromatic.\\n\\nWhat should I do as my next step?\"\n",
        "    },\n",
        "    # Add 4 more\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Gglt-XH2bBd"
      },
      "outputs": [],
      "source": [
        "from textwrap import fill\n",
        "questions = [{\"question\": q[\"query\"]} for q in queries]\n",
        "\n",
        "llm_responses = llm.batch([q[\"query\"] for q in queries])\n",
        "sparse_rag_responses = sparse_rag.batch(questions)\n",
        "semantic_rag_responses = semantic_rag.batch(questions)\n",
        "\n",
        "for query, r1, r2, r3 in zip(queries, llm_responses, sparse_rag_responses, semantic_rag_responses):\n",
        "    print(f'{query[\"title\"]}:')\n",
        "    print(f'  - Without  RAG: {fill(r1, width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print(f'  - Sparse   RAG: {fill(r2[\"response\"], width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print(f'  - Semantic RAG: {fill(r3[\"response\"], width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOJLRpnD2bBd"
      },
      "source": [
        "#### Report 2: (2 points)\n",
        "\n",
        "Write a report about the experiments above. Your report should address the following:\n",
        "1. Compare the quality of the answers. In which cases did Sparse or Semantic RAG help improve the response? Was there any example where it hurt the performance?\n",
        "2. Discuss the differences between Sparse and Semantic RAG. Based on your examples, which one seems more effective and why?\n",
        "3. Any surprising findings or patterns? Did anything behave differently than you expected?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtIsiDjA6Zoc"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TJIR0sJ2bBd"
      },
      "source": [
        "#### Completion 10: (3 points)\n",
        "\n",
        "Now we want to automate the evaluation process. For this purpose, we are going to use the `ragas`. Follow the instructions of each cell to create the evaluation pipeline. To learn more about this framework, please refer to its [get started](https://docs.ragas.io/en/stable/getstarted/) or [how-to](https://docs.ragas.io/en/stable/howtos/) pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxjSZO7V2bBd"
      },
      "outputs": [],
      "source": [
        "!pip install -q ragas rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG_Q2JCf2bBd"
      },
      "outputs": [],
      "source": [
        "# Load the LLM as ragas llm. For this, we can use the provided wrapper for our existing LLM.\n",
        "\n",
        "ragas_llm ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuL-IP9L2bBd"
      },
      "outputs": [],
      "source": [
        "# Generate 10 test cases, using ragas, based on your documents. You can use a subset of your documents for faster runtime.\n",
        "test_set =\n",
        "\n",
        "\n",
        "test_set.test_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O-XwiVs2bBd"
      },
      "outputs": [],
      "source": [
        "test_df = test_set.to_pandas()\n",
        "test_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU3QIdVW2bBd"
      },
      "outputs": [],
      "source": [
        "test_questions = test_df[\"question\"].values.tolist()\n",
        "test_ground_truths = test_df[\"ground_truth\"].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAyWxf1u2bBd"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"sparse\": {\n",
        "        \"answers\": [],\n",
        "        \"contexts\": []\n",
        "    },\n",
        "    \"dense\": {\n",
        "        \"answers\": [],\n",
        "        \"contexts\": []\n",
        "    },\n",
        "}\n",
        "\n",
        "for question in test_questions:\n",
        "    q = {\"question\": question}\n",
        "    s_response = sparse_rag.invoke(q)\n",
        "    d_response = semantic_rag.invoke(q)\n",
        "\n",
        "    results[\"sparse\"][\"answers\"].append(s_response[\"response\"])\n",
        "    results[\"sparse\"][\"contexts\"].append([context.page_content for context in s_response[\"context\"]])\n",
        "    results[\"dense\"][\"answers\"].append(d_response[\"response\"])\n",
        "    results[\"dense\"][\"contexts\"].append([context.page_content for context in d_response[\"context\"]])\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "sparse_response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : results[\"sparse\"][\"answers\"],\n",
        "    \"contexts\" : results[\"sparse\"][\"contexts\"],\n",
        "    \"ground_truth\" : test_ground_truths\n",
        "})\n",
        "dense_response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : results[\"dense\"][\"answers\"],\n",
        "    \"contexts\" : results[\"dense\"][\"contexts\"],\n",
        "    \"ground_truth\" : test_ground_truths\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHHmIKiV2bBd"
      },
      "outputs": [],
      "source": [
        "# Load ragas evaluation metrics. We would use all possible metrics, including:\n",
        "# - Faithfulness\n",
        "# - Answer relevancy\n",
        "# - Answer correctness\n",
        "# - retrieved context related metrics\n",
        "\n",
        "metrics = [\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gn_stOM2bBd"
      },
      "outputs": [],
      "source": [
        "# Use ragas evaluator to report the score of each pipeline, using the metrics defined above.\n",
        "\n",
        "sparse_scores =\n",
        "dense_scores =\n",
        "\n",
        "print(f\"Sparse RAG Score: {sparse_scores}\")\n",
        "print(f\"Dense RAG Score: {dense_scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p1Zsbbv2bBe"
      },
      "outputs": [],
      "source": [
        "sparse_scores.to_pandas().head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkrM-3XC2bBe"
      },
      "outputs": [],
      "source": [
        "dense_scores.to_pandas().head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGTkCkXR2bBe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_sparse = pd.DataFrame(list(sparse_scores.items()), columns=['Metric', 'Sparse Retriever'])\n",
        "df_dense = pd.DataFrame(list(dense_scores.items()), columns=['Metric', 'Dense Retriever'])\n",
        "\n",
        "df_merged = pd.merge(df_sparse, df_dense, on='Metric')\n",
        "\n",
        "df_merged['Delta'] = df_merged['Dense Retriever'] - df_merged['Sparse Retriever']\n",
        "\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgCuLLiP2bBe"
      },
      "source": [
        "#### Report 3: (1 point)\n",
        "\n",
        "Compare the automated evaluation (using ragas) with your manual evaluation from the previous step. In your report, make sure to address the following:\n",
        "1. How are the two evaluation methods different? Briefly describe what makes the automated evaluation distinct from your manual judgment process (e.g., consistency, objectivity, criteria used).\n",
        "2.\tDo both evaluations show the same results? Were the rankings or judgments about the quality of responses consistent between your analysis and the automated scores?\n",
        "3.\tIf there were differences, why might that be? Reflect on what factors could lead to different results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOwTO_Xn6Zoe"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSIS7xwI2bBe"
      },
      "source": [
        "#### Question 8: (2 points)\n",
        "\n",
        "1. Explain the underlying mechanism and techniques used by `ragas` to evaluate the performance of RAG pipelines. How does it ensure that the evaluation is both comprehensive and relevant to the task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh_mRj8-6Zoe"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrgGWbOG2bBe"
      },
      "source": [
        "### 3.6 (Optional) Other strategy: (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12LXj99X2bBe"
      },
      "source": [
        "#### Question 9: (3 points)\n",
        "\n",
        "There are other retriever strategies you can use to improve the performance of your RAG pipeline. In this task:\n",
        "1. Explain what the `MultiQueryRetriever` does and how it can help improve retrieval quality in your pipeline.\n",
        "2. Implement the `MultiQueryRetriever` in your RAG pipeline and evaluate its performance using both manual and automated methods.\n",
        "3. You may also make additional improvements to your pipeline. If you do so, briefly explain what changes you made, how they affect the system, and why they might improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHg0DwLO6Zoe"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUy0-3vC6Zof"
      },
      "source": [
        "## 4. Read more: (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPUsHZh56Zof"
      },
      "source": [
        "#### Cache-Augmented Generation (CAG): (4 points)\n",
        "\n",
        "1. What is Cache-Augmented Generation (CAG)? How does it improve efficiency or performance during generation?\n",
        "2. What are the similarities and differences between Cache-Augmented Generation (CAG) and Retrieval-Augmented Generation (RAG)? In what scenarios might you prefer one over the other?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efeiIAsc6Zof"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuZxBibl6Zof"
      },
      "source": [
        "#### Multi-modal RAG: (6 points)\n",
        "\n",
        "1. How do models like CLIP enable the embedding of both text and images into a shared vector space? What are the advantages and disadvantages of using a unified embedding space for cross-modal retrieval in RAG systems.\n",
        "2. In systems like [Colpali](https://arxiv.org/pdf/2407.01449), how does dividing document images into patches enhance the retrieval process in multimodal RAG? Explore how patch-based processing preserves structural information and its impact on retrieval accuracy.\n",
        "3. What are the implications of converting non-text modalities (e.g., images) into textual representations for retrieval purposes? Discuss the benefits and drawbacks of grounding all modalities into a primary modality, such as text, in the context of RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiGRfYMs6Zof"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TuwfOPincl-1",
        "OiB9LEjb8r0f",
        "FN7dSqY-NNel",
        "QvmpQ-BtNNem"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c64ffd50c78429aa7da2ec51dd9797a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9697c587102c44cfbcc60ee59f223a6f",
              "IPY_MODEL_0b8ef95054cf4732bc2f2d1f1b659207",
              "IPY_MODEL_96c957caddb34d3e99ecf95ffcdac8e4"
            ],
            "layout": "IPY_MODEL_9e61680b7c194192a5b0b0ab793b6f25"
          }
        },
        "9697c587102c44cfbcc60ee59f223a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b2318a1b1444640969a4ef289d6fa5f",
            "placeholder": "​",
            "style": "IPY_MODEL_23c54afe53d54e8ab104db39611ee5b9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0b8ef95054cf4732bc2f2d1f1b659207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04e4c8a5fbfc484ab737365f57784c64",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa54c6c70d7d41148e86799e9bd833f1",
            "value": 2
          }
        },
        "96c957caddb34d3e99ecf95ffcdac8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_546ed3fa25314ee8aa5aaee532c57ab9",
            "placeholder": "​",
            "style": "IPY_MODEL_32c6e2932428430883580809138126e1",
            "value": " 2/2 [00:41&lt;00:00, 19.76s/it]"
          }
        },
        "9e61680b7c194192a5b0b0ab793b6f25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2318a1b1444640969a4ef289d6fa5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c54afe53d54e8ab104db39611ee5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04e4c8a5fbfc484ab737365f57784c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa54c6c70d7d41148e86799e9bd833f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "546ed3fa25314ee8aa5aaee532c57ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c6e2932428430883580809138126e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b60a0b9a79ee4762ad9e2957c391a288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a7bc525229b49a2af767cad439776a0",
              "IPY_MODEL_d5fdb4c8ddc6435da34c3fc58a680dc6",
              "IPY_MODEL_4dc94883b0064fbf99ba12edd572adac"
            ],
            "layout": "IPY_MODEL_e78862f4cb424b748ec5688083ac168e"
          }
        },
        "4a7bc525229b49a2af767cad439776a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_664223d5ca98409cb8a68557d1824228",
            "placeholder": "​",
            "style": "IPY_MODEL_44d248d6ac9f4e3db84c2b0ae3aa0423",
            "value": "Generating train split: "
          }
        },
        "d5fdb4c8ddc6435da34c3fc58a680dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcce634cab064dd084053256bf73fa5a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1451b214af67413bad5cb5c2bf269b46",
            "value": 1
          }
        },
        "4dc94883b0064fbf99ba12edd572adac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7685b0831c304cf4a0592e21c70dd3e5",
            "placeholder": "​",
            "style": "IPY_MODEL_5c424cd16e414ad2af1b48604e6d04d2",
            "value": " 5000/0 [00:00&lt;00:00, 18085.60 examples/s]"
          }
        },
        "e78862f4cb424b748ec5688083ac168e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "664223d5ca98409cb8a68557d1824228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44d248d6ac9f4e3db84c2b0ae3aa0423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcce634cab064dd084053256bf73fa5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1451b214af67413bad5cb5c2bf269b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7685b0831c304cf4a0592e21c70dd3e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c424cd16e414ad2af1b48604e6d04d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}